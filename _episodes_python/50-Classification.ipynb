{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a classifier?\n",
    "\n",
    "A classifier is some kind of rule / black box / widget that you can feed a new observation/data/record and it will decide whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.\n",
    "\n",
    "![A classifier for cats and not cats.](../fig/50-CatNotCat.jpg)\n",
    "\n",
    "You can have classifiers for anything you can have a yes/no answer to, e.g.\n",
    "\n",
    "- Is this a cat? üê±\n",
    "- Do these test results indicate cancer? üöë\n",
    "- Is this email spam or not spam? üìß\n",
    "\n",
    "You can also have classifiers that categorise things into multiple (more than two) categories e.g.\n",
    "\n",
    "- Which animal is this, out of the 12 animals I have trained my model on? üê±\n",
    "- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? üöë\n",
    "- Is this email important, not important but not spam, or spam? üìß\n",
    "\n",
    "It is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.\n",
    "\n",
    "## Model evaluation (classification)\n",
    "\n",
    "For now, let's imagine we have a classifier already. How can we test it to see how good it is?\n",
    "A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.\n",
    "\n",
    "![An demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/50-CatConfusion.jpg)\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares \"the truth\" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and of course if we correctly label something which is not a cat, as not a cat, then this is a true negative.\n",
    "\n",
    "### Some common metrics\n",
    "\n",
    "#### Accuracy: \n",
    "How often does the classifier label examples correctly? Objective: maximize. Example: \n",
    "\n",
    "$$\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correctly labelled examples}}{\\text{All examples}}=\\frac{31+52}{31+52+10+7}=83\n",
    "\\%$$\n",
    "\n",
    "Accuracy is the opposite of the misclassification rate. So,\n",
    "\n",
    "$$\\text{Misclassification rate} = 1 - \\text{Accuracy} = \\frac{\\text{Incorrectly labelled examples}}{\\text{All examples}} $$\n",
    "\n",
    "#### Precision: \n",
    "What fraction of things labelled as a cat were actually cats? Objective: maximize. Example:\n",
    "\n",
    "$$\\frac{TP}{TP+FP} = \\frac{\\text{Correctly labelled cats}}{\\text{All things labelled as cats}}=\\frac{31}{31+10}=76\\%$$\n",
    "\n",
    "#### Sensitivity / Recall: \n",
    "How often does the classifier label a cat as a cat? Objective: maximize. Example: \n",
    "\n",
    "$$\\frac{TP}{TP+FN} = \\frac{\\text{Correctly labelled cats}}{\\text{All true cats}}=\\frac{31}{31+7}=81\\%$$\n",
    "\n",
    "#### Specificity: \n",
    "How often does it label a not-cat as a not-cat? Objective: maximize. Example: \n",
    "\n",
    "$$\\frac{TN}{TN+FP} = \\frac{\\text{Correctly labelled not-cats}}{\\text{All true not-cats}}=\\frac{52}{52+10}=84\\%$$\n",
    "\n",
    "#### F1-score:\n",
    "\n",
    "This is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;\n",
    "\n",
    "$$\\frac{1}{F_1} = \\frac{1}{2}\\left(\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Sensitivity}}\\right) $$\n",
    "So that\n",
    "$$F_1 = 2\\cdot\\left(\\frac{1}{\\frac{1}{81\\%}+\\frac{1}{83\\%}}\\right) = 82\\%$$\n",
    "\n",
    "#### Mean-square error (MSE)\n",
    "\n",
    "Define a loss function $L_i = 1$ if the $i$th example is classified incorrectly and $L_i = 0$ if it is classified correctly. If there are $N$ examples in total then the mean-square error is\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N}\\sum_i L_i = \\frac{\\text{Incorrectly labelled examples}}{\\text{All examples}}=\\text{Misclassification Rate}$$\n",
    "\n",
    "Which is actually just the misclassification rate above. \n",
    "\n",
    "#### AUC: Area under the curve\n",
    "\n",
    "A good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.\n",
    "\n",
    "To capture this balance, we often use a Receiver Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.\n",
    "\n",
    "![A Receiver Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/50-CatArea.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wisconsin Diagnostic Breast Cancer Dataset\n",
    "\n",
    "Today we're going to be classifying patient's biopsy to see whether their breast tumor is malignant or benign. First, a fine needle aspirate (FNA) of a breast mass is taken. Basically they stick a needle in you and take a chunk out then put it under the microscope. It looks like this:\n",
    "\n",
    "![An example of a Fine Needle Aspiration Biopsy](../fig/50-742_FNA1.jpg)\n",
    "\n",
    "Features are computed from the digitized image, which describe the characteristics of the cell nuclei present in the image.\n",
    "\n",
    "## Attribute Information:\n",
    "\n",
    "1) ID number\n",
    "2) Diagnosis (M = malignant, B = benign)\n",
    "3-32)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus, each has \n",
    "- a *mean* across cells \n",
    "- a *standard deviation* across cells and \n",
    "- the *worst* value across cells:\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)\n",
    "b) texture (standard deviation of gray-scale values)\n",
    "c) perimeter\n",
    "d) area\n",
    "e) smoothness (local variation in radius lengths)\n",
    "f) compactness (perimeter^2 / area - 1.0)\n",
    "g) concavity (severity of concave portions of the contour)\n",
    "h) concave points (number of concave portions of the contour)\n",
    "i) symmetry\n",
    "j) fractal dimension (\"coastline approximation\" - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Explore our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn import model_selection\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdbc_raw = pd.read_csv('../data/breast-cancer-wisconsin.csv') # read csv\n",
    "wdbc = wdbc_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the problem of whitespace in column names by replacing with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(wdbc.columns)\n",
    "#wdbc.rename(columns=lambda x: x.strip())\n",
    "wdbc.columns = wdbc.columns.str.replace(' ', '_', regex=True)\n",
    "#print(wdbc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recode diagnosis into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wdbc['diagnosis']\n",
    "#wdbc['diagnosis'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary info\n",
    "Shape of data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdbc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for non-NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdbc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdbc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore relationship between variables\n",
    "Plot mean radius of cell nucleii vs. mean concavity, coloured by diagnosis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sns.lmplot('radius_mean','concavity_mean',data=wdbc, hue='diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to look at all possible scatterplot pairs we would do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sns.pairplot(data=wdbc,\n",
    "             vars=wdbc.columns[2:10],\n",
    "             hue='diagnosis',\n",
    "             markers=[\"o\", \"s\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's easier to look at a correlation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catVars = wdbc.select_dtypes(include = ['object']).columns\n",
    "numericVars = wdbc.select_dtypes(exclude = ['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr = wdbc[numericVars[1:-1]].corr() # excluding id & Unnamed_32\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "_ = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0., square=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sm.qqplot(\n",
    "    wdbc['radius_mean'],\n",
    "    line = 's'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = sm.qqplot(\n",
    "    np.sqrt(wdbc['radius_mean']),\n",
    "    line = 's'\n",
    ")\n",
    "\n",
    "_ = sm.qqplot(\n",
    "    np.log(wdbc['radius_mean']),\n",
    "    line = 's'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "To create a classifier for predicting whether a breast cancer patient's tumor is malignant or benign.\n",
    "\n",
    "#### Train Test Split\n",
    "\n",
    "We're going to split our data into 70% training and 30% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove id, diagnosis, and Unnamed\n",
    "predictors = wdbc.columns.values.tolist()\n",
    "predictors.remove('id')\n",
    "predictors.remove('diagnosis')\n",
    "predictors.remove('Unnamed:_32')\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_train, features_test, outcome_train, outcome_test = model_selection.train_test_split(wdbc[predictors],wdbc['diagnosis'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many examples do we have in the training and testing sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's Classify!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours Classifier\n",
    "\n",
    "This takes the nearest k things and and says what is the majority vote? E.g. in the example below we look at the seven nearest neighbours, 4 of which are cats so we say that the new example is probably a cat as well.\n",
    "\n",
    "![A way to classify a new example as a cat or not...take the average of the nearest k=7 examples. It's a cat!](../fig/50-CatKNN.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit KNN classifier with a pre-determine number of neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=4)\n",
    "knn = neigh.fit(features_train, outcome_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome_pred_class = neigh.predict(features_test)\n",
    "#outcome_pred_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome_pred_prob = neigh.predict_proba(features_test)\n",
    "outcome_pred_prob1 = [p[1] for p in outcome_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_decision = knn.decision_function(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Diagnostics\n",
    "So how well did the classifier do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true_outcome = outcome_test.astype(\"category\").cat.codes.values\n",
    "print(y_true_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_true_outcome, outcome_pred_prob1)\n",
    "AUC = roc_auc_score(y_true_outcome, outcome_pred_prob1)\n",
    "\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC = %4.2f'%AUC)\n",
    "_ = plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
