---
title: "Introduction to machine learning using python"
author: "Darya Vanichkina"
questions: 
- What is machine learning? 
- How is it different from statistics? 
- What types of machine learning exist?
keypoints: 
- This course is an introduction to machine learning
- ML is distinct from statistical inference in that it focuses on prediction, NOT explanation. 
- ML is classified into supervised, unsupervised, semi-supervised and reinforcement learning.
- There is no free lunch - no magic approach that will suit all problems well.
objectives:
  - Introduce the course
  - Explain python proficiency required 
  - Contrast machine learning with statistical inference
  - Understand the key types of ML problems
source: Rmd
teaching: 30
exercises: 0
start: 0
---



**Instructor note: this lesson is usually taught with the help of the slide deck.**


## Overview of the course

This workshop is designed to provide and introduction to practical machine learning with python.

The attendees are expected to have some python background (at least at the level of the [Intersect python courses](https://intersect.org.au/training/course/python203/), especially numpy, pandas and the use of python as a data processing tool.

The workshop will include a lot of live coding and hands-on "experimenting" with python code, and not a lot of detail about the underlying mathematical and computational details of the algorithms used  - we suggest that when you want to try applying one of these algorithms to your own work, you review its underlying assumption in one of the recommended reference resources.

We will provide an introduction to some basic principles of machine learning experimentation, describing how one selects a model to use, the concepts of cross-validation. We will demonstrate how these apply to several classical machine learning approaches in python, including supervised (classification and regression, such as K-nearest neighbour and linear regression) and unsupervised (clustering, such as hierarchical and k-means clustering, and dimensionality reduction, such as principal component analysis) methods.


We hope that at the end of the course, learners are able to assess whether any of these approaches are applicable to their own analysis problems, to apply the demonstrated methods to their own datasets (if they're appropriate), and to evaluate whether the models are a good fit for their data. Furthermore, we hope learners become more confident in discovering and trying out new methods of interest that they come across in the literature. 


## What is machine learning? How is it different from statistical learning?

- **Machine learning** - also called predictive modelling or statistical learning - is the process of developing a mathematical tool or model that generates an active prediction (Kuhn 2013).

- This makes it distinct from **statistical inference**, where the model must be interpretable and possibly illuminate causal relationships in the data. In ML, we care less about that, and more about the accuracy of our prediction.

### Caveat:

> If a predictive signal exists in the data, many models will find some degree of 
> that signal regardless of the technique or care placed in developing the model <...> 
> But the best, most predictive models are fundamentally influenced by a modeller with 
> expert knowledge and context of the problem. (Kuhn and Johnson 2013).

## What are some case studies of the use of ML?

- Predicting whether a patient has a disease or not, based on a set of symptoms
- Predicting the likelihood a patient will return to hospital after treatment
- Segmenting viewers to suggest which films they should watch


## Types of Machine learning

1. **Supervised learning**:
  - The dataset contains a series of inputs, based on which we are trying to predict a predefined outcome, which we know for the original data in the dataset. The outcome can be numerical (in which case the problem is called regression) or categorical (classification).
2. **Unsupervised learning**:
  - No labels are provided, and the algorithm tries to find structure in unlabelled data. Finding groups of similar users (think Netflix) is a classic example of this. 
3. **Semi-supervised learning**:
  - A combination of the two approaches, where we try to use labels from the data to influence the unlabelled data to identify and annotate new classes in the dataset (aka novelty detection). Example: image clustering + manual annotation of clusters.
4. **Reinforcement learning**:
  - The learning algorithm performs a task and gets feedback in real time from operating in a real or simulated environment. Examples are elevator scheduling or learning to play a game.
  
## "No Free Lunch" theorem

> Without strong, potentially causal, information about the modeling problem, there is no single model 
> that will always do better than any other model. (Wolpert 1996)


## Key terms
1. The main goal of the machine learning process is to find an algorithm $$f(x)$$ that most accurately predicts 
   future values $$y$$ based on a set of inputs $$\mathbf{X}$$. 

2. The terms `sample`, `data point`, `observation`, and `instance` refer to a single independent unit of study, i.e. a customer, patient, compound, email, etc. The term `sample` can also be used (next slides) to describe a set of entries, when discussing a training and test sample. 

3. The `inputs` **X**, which can also be called `predictors`, `independent variables`, `attributes` or `descriptors` are used as inputs for the prediction equation. 

4. `Outcome`, `dependent variable`, `target`, `class`, or `response` - what you're trying to predict (usually more applicable in the context of supervised learning). 


The "predicts future values" phrase here is very important: it's more important for our function to predict the future outcome accurately, not to fit the existing data as well as possible! This ability to predict the future well is called **generalizability**, whereas a situation when the algorithm fits our existing data very well, but doesn't generalise is called **overfitting**. 


## Training and testing

To enable us to assess how generalizable our model is, we can split the data into a training and testing set:

- The training set is used to train the model, and to tune the (hyper)parameters
- The testing or validation set is withheld during training, and is used once we have chosen a final model to estimate the prediction error (also known as generalisation error).

We expect/hope/assume that the prediction error determined using the testing set is the same as what would be characteristic of completely new data that we would need to use the model to make a prediction for. 

- The relative proportions of the training and testing set depend on the total of number of observations, and the variability observed in the data. The tradeoff to be considered is:
  - if there is too much data in the training set, the assessment of the prediction error will be carried out on a very small test set - therefor we might overfit the model, finding a formula that fits the existing data very well, but generalizes very poorly
  - if there is too much data in the testing set, this means that we might not have enough data in the training set to accurately estimate the model parameters - so the model won't be very accurate

- Some commonly used cutoffs include:
  - 60% training / 40% testing
  - 70% training / 30% testing
  - 80% training / 20% testing
