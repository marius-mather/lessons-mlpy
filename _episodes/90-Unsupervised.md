---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 90-Unsupervised.md in _episodes_rmd/
title: "Unsupervised learning"
author: "Darya Vanichkina"
keypoints:
- Unsupervised learning is an approach where we use machine learning to search for patterns in the data
- Clustering can 
source: Rmd
start: 0
teaching: 30
exercises: 0
---






## Load the Iris dataset and libraries


~~~
library(tidyverse)  # data manipulation
~~~
{: .language-r}



~~~
── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ──
~~~
{: .output}



~~~
✔ ggplot2 3.1.0       ✔ purrr   0.3.1  
✔ tibble  2.0.1       ✔ dplyr   0.8.0.1
✔ tidyr   0.8.3       ✔ stringr 1.4.0  
✔ readr   1.3.1       ✔ forcats 0.4.0  
~~~
{: .output}



~~~
Warning: package 'tibble' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'tidyr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'purrr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'dplyr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'stringr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'forcats' was built under R version 3.5.2
~~~
{: .error}



~~~
── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
~~~
{: .output}



~~~
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
~~~
{: .language-r}



~~~
Welcome! Related Books: `Practical Guide To Cluster Analysis in R` at https://goo.gl/13EFCZ
~~~
{: .output}



~~~
library(dendextend) # nice dendrograms 
~~~
{: .language-r}



~~~

---------------------
Welcome to dendextend version 1.9.0
Type citation('dendextend') for how to cite the package.

Type browseVignettes(package = 'dendextend') for the package vignette.
The github page is: https://github.com/talgalili/dendextend/

Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues
Or contact: <tal.galili@gmail.com>

	To suppress this message use:  suppressPackageStartupMessages(library(dendextend))
---------------------
~~~
{: .output}



~~~

Attaching package: 'dendextend'
~~~
{: .output}



~~~
The following object is masked from 'package:stats':

    cutree
~~~
{: .output}



~~~
library(pheatmap) # nice heatmaps
~~~
{: .language-r}



~~~
Warning: package 'pheatmap' was built under R version 3.5.2
~~~
{: .error}



~~~
library(caret) #another option for pca
~~~
{: .language-r}



~~~
Loading required package: lattice
~~~
{: .output}



~~~

Attaching package: 'caret'
~~~
{: .output}



~~~
The following object is masked from 'package:purrr':

    lift
~~~
{: .output}



~~~
library(Rtsne) #t-SNE

data("iris")
str(iris)
~~~
{: .language-r}



~~~
'data.frame':	150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
~~~
{: .output}

## Explore the iris dataset



> ## Challenge
>
> Take a few moments to explore the Iris dataset. What can you learn? Which species do you think will be easier to separate?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> > 
> > dim(iris)
> > str(iris)
> > 
> > iris %>%
> >   ggplot(aes(x = Sepal.Length, y = Sepal.Width, col = Species)) + geom_point() + theme_minimal()
> > 
> > iris %>%
> >   ggplot(aes(x = Sepal.Length, y = Sepal.Width, col = Species)) + geom_point() + theme_minimal()
> > 
> > iris %>%
> >   ggplot(aes(x = Petal.Width, y = Petal.Length, col = Species)) + geom_point() + theme_minimal()
> > 
> > iris %>% 
> >   gather(key, value, -Species) %>%
> >   ggplot(aes(y = value, fill = Species)) + geom_boxplot() + facet_wrap(.~key) + theme_bw()
> > 
> > ~~~
> > 
> > {: .output}
> {: .solution}
{: .challenge}


What if we didn't know we had 3 species??? Could we use the morphological data to study this problem?

## Clustering

### k-means clustering



~~~
iris_scaled <- scale(iris[,1:4])
rownames(iris_scaled) <- paste(substr(iris$Species, 1, 2), seq(1:length(iris$Species)), sep = "_")
distance <- get_dist(iris_scaled)
fviz_dist(distance)
~~~
{: .language-r}

<img src="../fig/rmd-90-getdist-1.png" title="plot of chunk getdist" alt="plot of chunk getdist" width="612" style="display: block; margin: auto;" />


~~~
k2 <- kmeans(iris_scaled, centers = 2, nstart = 25)
str(k2)
~~~
{: .language-r}



~~~
List of 9
 $ cluster     : Named int [1:150] 1 1 1 1 1 1 1 1 1 1 ...
  ..- attr(*, "names")= chr [1:150] "se_1" "se_2" "se_3" "se_4" ...
 $ centers     : num [1:2, 1:4] -1.011 0.506 0.85 -0.425 -1.301 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:2] "1" "2"
  .. ..$ : chr [1:4] "Sepal.Length" "Sepal.Width" "Petal.Length" "Petal.Width"
 $ totss       : num 596
 $ withinss    : num [1:2] 47.4 173.5
 $ tot.withinss: num 221
 $ betweenss   : num 375
 $ size        : int [1:2] 50 100
 $ iter        : int 1
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"
~~~
{: .output}


~~~
fviz_cluster(k2, data = iris_scaled) + theme_minimal()
~~~
{: .language-r}

<img src="../fig/rmd-90-Visualisek2-1.png" title="plot of chunk Visualisek2" alt="plot of chunk Visualisek2" width="612" style="display: block; margin: auto;" />




~~~
set.seed(42)
fviz_nbclust(iris_scaled, kmeans, method = "wss")
~~~
{: .language-r}

<img src="../fig/rmd-90-OptimalNumberOfClusters-1.png" title="plot of chunk OptimalNumberOfClusters" alt="plot of chunk OptimalNumberOfClusters" width="612" style="display: block; margin: auto;" />

~~~
fviz_nbclust(iris_scaled, kmeans, method = "silhouette")
~~~
{: .language-r}

<img src="../fig/rmd-90-OptimalNumberOfClusters-2.png" title="plot of chunk OptimalNumberOfClusters" alt="plot of chunk OptimalNumberOfClusters" width="612" style="display: block; margin: auto;" />

~~~
fviz_nbclust(iris_scaled, kmeans, method = "gap_stat")
~~~
{: .language-r}

<img src="../fig/rmd-90-OptimalNumberOfClusters-3.png" title="plot of chunk OptimalNumberOfClusters" alt="plot of chunk OptimalNumberOfClusters" width="612" style="display: block; margin: auto;" />


~~~
k3 <- kmeans(iris_scaled, centers = 3, nstart = 25)
str(k3)
~~~
{: .language-r}



~~~
List of 9
 $ cluster     : Named int [1:150] 2 2 2 2 2 2 2 2 2 2 ...
  ..- attr(*, "names")= chr [1:150] "se_1" "se_2" "se_3" "se_4" ...
 $ centers     : num [1:3, 1:4] 1.1322 -1.0112 -0.0501 0.0881 0.8504 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "1" "2" "3"
  .. ..$ : chr [1:4] "Sepal.Length" "Sepal.Width" "Petal.Length" "Petal.Width"
 $ totss       : num 596
 $ withinss    : num [1:3] 47.5 47.4 44.1
 $ tot.withinss: num 139
 $ betweenss   : num 457
 $ size        : int [1:3] 47 50 53
 $ iter        : int 2
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"
~~~
{: .output}



~~~
fviz_cluster(k3, data = iris_scaled) + theme_bw()
~~~
{: .language-r}

<img src="../fig/rmd-90-k3-1.png" title="plot of chunk k3" alt="plot of chunk k3" width="612" style="display: block; margin: auto;" />

~~~
k7 <- kmeans(iris_scaled, centers = 7, nstart = 25)
str(k7)
~~~
{: .language-r}



~~~
List of 9
 $ cluster     : Named int [1:150] 2 1 1 1 2 2 1 1 1 1 ...
  ..- attr(*, "names")= chr [1:150] "se_1" "se_2" "se_3" "se_4" ...
 $ centers     : num [1:7, 1:4] -1.303 -0.719 0.289 0.954 0.36 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:7] "1" "2" "3" "4" ...
  .. ..$ : chr [1:4] "Sepal.Length" "Sepal.Width" "Petal.Length" "Petal.Width"
 $ totss       : num 596
 $ withinss    : num [1:7] 9.65 12.15 11.83 7.55 5.94 ...
 $ tot.withinss: num 71.1
 $ betweenss   : num 525
 $ size        : int [1:7] 25 25 29 21 17 21 12
 $ iter        : int 3
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"
~~~
{: .output}



~~~
fviz_cluster(k7, data = iris_scaled) + theme_bw()
~~~
{: .language-r}

<img src="../fig/rmd-90-k3-2.png" title="plot of chunk k3" alt="plot of chunk k3" width="612" style="display: block; margin: auto;" />






> ## Challenge
>
> Choose whichever clustering approach you think worked best among the above. If you partition the data this way, 
> which of the variables is most distinct in the clusters?
> 
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> > 
> > iris %>%
> >   mutate(Cluster = k3$cluster) %>%
> >   group_by(Cluster) %>%
> >   summarize (MostFreqSpecies =names(which.max(table(Species))),
> >              Sepal.Length = mean(Sepal.Length),
> >              Sepal.Width = mean(Sepal.Width),
> >              Petal.Width = mean(Petal.Width),
> >              Petal.Length = mean(Petal.Length))
> > 
> > ~~~
> > 
> > {: .output}
> {: .solution}
{: .challenge}



### Hierarchical clustering

The first step is to compute the distance between each sample, and by default, the complete linkage method is used. 


~~~
iris_hcl <- hclust(dist(iris_scaled))
plot(iris_hcl)
~~~
{: .language-r}

<img src="../fig/rmd-90-hclust-1.png" title="plot of chunk hclust" alt="plot of chunk hclust" width="612" style="display: block; margin: auto;" />

~~~
# cut dendrogram in 3 clusters
dendcut = cutree(iris_hcl, 3)
table(dendcut, iris$Species)
~~~
{: .language-r}



~~~
       
dendcut setosa versicolor virginica
      1     49          0         0
      2      1         21         2
      3      0         29        48
~~~
{: .output}



~~~
iris_hcl %>% as.dendrogram() %>% plot()
~~~
{: .language-r}

<img src="../fig/rmd-90-hclust-2.png" title="plot of chunk hclust" alt="plot of chunk hclust" width="612" style="display: block; margin: auto;" />



~~~
fviz_dend(hcut(iris_scaled, k = 3, hc_method = "complete"), 
          rect = TRUE, 
          cex = 0.5, 
          palette = "Set1"
          )
~~~
{: .language-r}

<img src="../fig/rmd-90-withFactorExtra-1.png" title="plot of chunk withFactorExtra" alt="plot of chunk withFactorExtra" width="612" style="display: block; margin: auto;" />




~~~
collabels <- data.frame(species = substr(rownames(iris_scaled), 1, 2))
row.names(collabels) <- rownames(iris_scaled)
pheatmap(iris_scaled, 
         cluster_rows = iris_hcl, 
         treeheight_row = 30, 
         treeheight_col = 30,
         annotation_row = collabels)
~~~
{: .language-r}

<img src="../fig/rmd-90-heatmap-1.png" title="plot of chunk heatmap" alt="plot of chunk heatmap" width="612" style="display: block; margin: auto;" />




> ## Challenge
>
> Try constructing a heatmap using another agglomeration method, and visualise the results. 
> 
> Do you think your approach is better or worse than the "default"? Compare with your group...
>
> 
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> > iris_hcl <- hclust(dist(iris_scaled), method = "ward.D")
> > pheatmap(iris_scaled, 
> >     cluster_rows = iris_hcl, 
> >     treeheight_row = 30, 
> >     treeheight_col = 30,
> >     annotation_row = collabels)
> > 
> > ~~~
> > 
> > {: .output}
> {: .solution}
{: .challenge}








## PCA


~~~
iris_pca <- prcomp(iris_scaled)
iris_scores = as.data.frame(iris_pca$x)
iris_scores$species <- substr(row.names(iris_scores), 1, 2)



# plot of observations
ggplot(data = iris_scores, aes(x = PC1, y = PC2, color = species, label = species)) +
  geom_text(alpha = 0.8, size = 4) +
  ggtitle("First two PC of Iris data") +
  theme_minimal()
~~~
{: .language-r}

<img src="../fig/rmd-90-PCA-1.png" title="plot of chunk PCA" alt="plot of chunk PCA" width="612" style="display: block; margin: auto;" />


~~~
fviz_eig(iris_pca, addlabels = TRUE)
~~~
{: .language-r}

<img src="../fig/rmd-90-VarianceExplained-1.png" title="plot of chunk VarianceExplained" alt="plot of chunk VarianceExplained" width="612" style="display: block; margin: auto;" />



~~~
# plot of observations
ggplot(data = iris_scores, aes(x = PC2, y = PC3, color = species, label = species)) +
  geom_text(alpha = 0.8, size = 4) +
  ggtitle("PC2/3 of Iris data") +
  theme_minimal()
~~~
{: .language-r}

<img src="../fig/rmd-90-PC23-1.png" title="plot of chunk PC23" alt="plot of chunk PC23" width="612" style="display: block; margin: auto;" />

Let's look at the rotation matrix:


~~~
iris_pca$rotation
~~~
{: .language-r}



~~~
                    PC1         PC2        PC3        PC4
Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971
~~~
{: .output}



~~~
fviz_pca_var(iris_pca,  
             col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
~~~
{: .language-r}

<img src="../fig/rmd-90-RotMatrix-1.png" title="plot of chunk RotMatrix" alt="plot of chunk RotMatrix" width="612" style="display: block; margin: auto;" />



~~~
# Contributions of variables to PC1
fviz_contrib(iris_pca, choice = "var", axes = 1, top = 10)
~~~
{: .language-r}

<img src="../fig/rmd-90-ContribVars-1.png" title="plot of chunk ContribVars" alt="plot of chunk ContribVars" width="612" style="display: block; margin: auto;" />

~~~
# Contributions of variables to PC2
fviz_contrib(iris_pca, choice = "var", axes = 2, top = 10)
~~~
{: .language-r}

<img src="../fig/rmd-90-ContribVars-2.png" title="plot of chunk ContribVars" alt="plot of chunk ContribVars" width="612" style="display: block; margin: auto;" />




~~~
# Visualize
# Use habillage to specify groups for coloring
fviz_pca_ind(iris_pca,
             label = "none", 
             habillage = iris$Species, 
             palette = "Set1",
             addEllipses = TRUE 
             )
~~~
{: .language-r}

<img src="../fig/rmd-90-CoolVisualisation-1.png" title="plot of chunk CoolVisualisation" alt="plot of chunk CoolVisualisation" width="612" style="display: block; margin: auto;" />





~~~
iris_trans <- preProcess(iris[,1:4], method=c("BoxCox", "center",  "scale", "pca"))
iris_PCaret <- predict(iris_trans, iris[,1:4])
dim(iris_PCaret) # kept only the PCs that are necessary >= 95% of variability in the data
~~~
{: .language-r}



~~~
[1] 150   2
~~~
{: .output}



~~~
head(iris_PCaret)
~~~
{: .language-r}



~~~
        PC1        PC2
1 -2.303540 -0.4748260
2 -2.151310  0.6482903
3 -2.461341  0.3463921
4 -2.413968  0.6066839
5 -2.432777 -0.6110057
6 -1.979172 -1.3950594
~~~
{: .output}



~~~
head(iris_pca$x)
~~~
{: .language-r}



~~~
           PC1        PC2         PC3          PC4
se_1 -2.257141 -0.4784238  0.12727962  0.024087508
se_2 -2.074013  0.6718827  0.23382552  0.102662845
se_3 -2.356335  0.3407664 -0.04405390  0.028282305
se_4 -2.291707  0.5953999 -0.09098530 -0.065735340
se_5 -2.381863 -0.6446757 -0.01568565 -0.035802870
se_6 -2.068701 -1.4842053 -0.02687825  0.006586116
~~~
{: .output}



~~~
ggplot(data = iris_PCaret, aes(x = PC1, y = PC2, color = iris$Species, label = iris$Species)) +
  geom_text(alpha = 0.8, size = 4) +
  ggtitle("First two PC of Iris data") +
  theme_minimal()
~~~
{: .language-r}

<img src="../fig/rmd-90-PCAwCaret-1.png" title="plot of chunk PCAwCaret" alt="plot of chunk PCAwCaret" width="612" style="display: block; margin: auto;" />

~~~
# We can see the results are the same as above
~~~
{: .language-r}





> ## Challenge
>
> Perform a PCA on the Ames housing filtered and unfiltered datasets. 
> How much variance is explained by the top components?
> What is the difference between including and not including the outlier points?
> 
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> > 
> > numericVarsWoutSale <- numericVars[1:length(numericVars) - 1]
> > table(complete.cases(ameshousingFilt[,numericVarsWoutSale]))
> > 
> > ames_pca <- prcomp(ameshousingFilt[complete.cases(ameshousingFilt[,numericVarsWoutSale]), numericVarsWoutSale],
> >                    center = TRUE,
> >                    scale = TRUE) 
> > 
> > fviz_screeplot(ames_pca, addlabels = TRUE, ylim = c(0, 25))
> > fviz_pca_var(ames_pca, col.var="contrib", gradient.cols = c("red", "blue", "black"), repel = TRUE)
> > 
> > # Contributions of variables to PC1
> > fviz_contrib(ames_pca, choice = "var", axes = 1, top = 10)
> > # Contributions of variables to PC2
> > fviz_contrib(ames_pca, choice = "var", axes = 2, top = 10)
> > fviz_pca_ind(ames_pca,label = "none")
> > 
> > 
> > 
> > 
> > 
> > table(complete.cases(ameshousing[,numericVarsWoutSale]))
> > ames_pca2 <- prcomp(ameshousing[complete.cases(ameshousing[,numericVarsWoutSale]), numericVarsWoutSale],
> >                    center = TRUE,
> >                    scale = TRUE) 
> > fviz_screeplot(ames_pca2, addlabels = TRUE, ylim = c(0, 25))
> > fviz_pca_var(ames_pca2, col.var="contrib", gradient.cols = c("red", "blue", "black"), repel = TRUE)
> > 
> > # Contributions of variables to PC1
> > fviz_contrib(ames_pca2, choice = "var", axes = 1, top = 10)
> > # Contributions of variables to PC2
> > fviz_contrib(ames_pca2, choice = "var", axes = 2, top = 10)
> > fviz_pca_ind(ames_pca2,label = "none")
> > 
> > ~~~
> > 
> > {: .output}
> {: .solution}
{: .challenge}



### t-SNE


Don't forget to remove non-unique observations!


~~~
set.seed(42)
iris_uniq <- unique(iris)
iris_tsne <-  Rtsne(iris_uniq[1:4], dims = 2,  perplexity = 25, max_iter = 5000)
iris_tsne_df = data.frame(iris_tsne$Y)  
ggplot(iris_tsne_df, aes(x=X1, y=X2, color=iris_uniq$Species)) + geom_point(size=2) + theme_bw()
~~~
{: .language-r}

<img src="../fig/rmd-90-tsne-1.png" title="plot of chunk tsne" alt="plot of chunk tsne" width="612" style="display: block; margin: auto;" />

