---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 11-RidgeLassoElasticNet.md in _episodes_rmd/
title: "Regularised Regression. Principle components Regression. Partial Least Squares Regression."
author: "Darya Vanichkina"
keypoints:
- Regularisation helps us improve the performance of regression
- 
objectives:
- someobjective
questions:
- How do we prevent all variables from being incorporated into a regression model?
source: Rmd
start: 0
teaching: 30
exercises: 0
---




```r
library(tidyverse)
```

```
## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ──
```

```
## ✔ ggplot2 3.1.0       ✔ purrr   0.3.1  
## ✔ tibble  2.0.1       ✔ dplyr   0.8.0.1
## ✔ tidyr   0.8.3       ✔ stringr 1.4.0  
## ✔ readr   1.3.1       ✔ forcats 0.4.0
```

```
## Warning: package 'tibble' was built under R version 3.5.2
```

```
## Warning: package 'tidyr' was built under R version 3.5.2
```

```
## Warning: package 'purrr' was built under R version 3.5.2
```

```
## Warning: package 'dplyr' was built under R version 3.5.2
```

```
## Warning: package 'stringr' was built under R version 3.5.2
```

```
## Warning: package 'forcats' was built under R version 3.5.2
```

```
## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
```

```r
library(caret)
```

```
## Loading required package: lattice
```

```
## 
## Attaching package: 'caret'
```

```
## The following object is masked from 'package:purrr':
## 
##     lift
```

```r
library(tidymodels)
```

```
## ── Attaching packages ───────────────────────────────── tidymodels 0.0.2 ──
```

```
## ✔ broom     0.5.1     ✔ recipes   0.1.4
## ✔ dials     0.0.2     ✔ rsample   0.0.4
## ✔ infer     0.4.0     ✔ yardstick 0.0.3
## ✔ parsnip   0.0.1
```

```
## Warning: package 'rsample' was built under R version 3.5.2
```

```
## Warning: package 'yardstick' was built under R version 3.5.2
```

```
## ── Conflicts ──────────────────────────────────── tidymodels_conflicts() ──
## ✖ scales::discard()      masks purrr::discard()
## ✖ dplyr::filter()        masks stats::filter()
## ✖ recipes::fixed()       masks stringr::fixed()
## ✖ dplyr::lag()           masks stats::lag()
## ✖ caret::lift()          masks purrr::lift()
## ✖ yardstick::precision() masks caret::precision()
## ✖ yardstick::recall()    masks caret::recall()
## ✖ yardstick::spec()      masks readr::spec()
## ✖ recipes::step()        masks stats::step()
```

```r
library(AmesHousing)
```


```
## Error in strata %in% vars: object 'ameshousingFilt' not found
```

```
## Error in analysis(x): object 'ames_split' not found
```

```
## Error in assessment(x): object 'ames_split' not found
```


```r
param_search <- expand.grid(ncomp = seq(2, 219, length.out = 20))
```



## Regularised Regression. 

### Ridge (alpha = 0 )


```r
lambda_search  <- expand.grid( alpha = 0, lambda = c(0.1,1, 10, 100, 1000, 10000))
set.seed(42)
ames_ridge <- train(
  Sale_Price ~ ., 
  data = ameshousingFiltTrain,
  preProcess = c("zv", "center", "scale"),
  trControl = cv,
  method = "glmnet", 
  tuneGrid = lambda_search,
  metric = "RMSE"
  )
```

```
## Error in eval(expr, p): object 'ameshousingFiltTrain' not found
```

```r
# best model
ames_ridge$results %>%
  filter(
    alpha == ames_ridge$bestTune$alpha,
    lambda == ames_ridge$bestTune$lambda
    )
```

```
## Error in eval(lhs, parent, parent): object 'ames_ridge' not found
```

```r
# plot results
plot(ames_ridge)
```

```
## Error in plot(ames_ridge): object 'ames_ridge' not found
```

### Lasso (alpha = 1)



```r
lambda_search  <- expand.grid( alpha = 1, lambda = c(0.1,1, 10, 100, 1000, 10000))
set.seed(42)
ames_lasso <- train(
  Sale_Price ~ ., 
  data = ameshousingFiltTrain,
  preProcess = c("zv", "center", "scale"),
  trControl = cv,
  method = "glmnet", 
  tuneGrid = lambda_search,
  metric = "RMSE"
  )
```

```
## Error in eval(expr, p): object 'ameshousingFiltTrain' not found
```

```r
# best model
ames_lasso$results %>%
  filter(
    alpha == ames_lasso$bestTune$alpha,
    lambda == ames_lasso$bestTune$lambda
    )
```

```
## Error in eval(lhs, parent, parent): object 'ames_lasso' not found
```

```r
# plot results
plot(ames_lasso)
```

```
## Error in plot(ames_lasso): object 'ames_lasso' not found
```


### Elastic net - mix of the two



```r
lambda_search  <- expand.grid( alpha = seq(0,1,0.2), lambda = c(0.1,1, 10, 100, 1000, 10000))
set.seed(42)
ames_en <- train(
  Sale_Price ~ ., 
  data = ameshousingFiltTrain,
  preProcess = c("zv", "center", "scale"),
  trControl = cv,
  method = "glmnet", 
  tuneGrid = lambda_search,
  metric = "RMSE"
  )
```

```
## Error in eval(expr, p): object 'ameshousingFiltTrain' not found
```

```r
# best model
ames_en$results %>%
  filter(
    alpha == ames_en$bestTune$alpha,
    lambda == ames_en$bestTune$lambda
    )
```

```
## Error in eval(lhs, parent, parent): object 'ames_en' not found
```

```r
# plot results
plot(ames_en)
```

```
## Error in plot(ames_en): object 'ames_en' not found
```



> ## Challenge
>
> Work with the grid search for the model above to identify which value of ncomp is optimal for this model?
> Do you think the performance of this model will be very good?
> 
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> > 175 +/- 5
> > 
> > ~~~
> > 
> > {: .output}
> {: .solution}
{: .challenge}




## Partial Least Squares Regression.


```r
set.seed(42)
ames_plsr <- train(
  Sale_Price ~ ., 
  data = ameshousingFiltTrain_engineered, 
  trControl = cv,
  method = "pls",
  tuneGrid = param_search,
  metric = "RMSE"
  )
```

```
## Error in eval(expr, p): object 'ameshousingFiltTrain_engineered' not found
```

```r
# model with lowest RMSE
ames_plsr$bestTune
```

```
## Error in eval(expr, envir, enclos): object 'ames_plsr' not found
```

```r
ames_plsr$results %>%
  filter(ncomp == as.numeric(ames_plsr$bestTune))
```

```
## Error in eval(lhs, parent, parent): object 'ames_plsr' not found
```

```r
# coef(ames_plsr$finalModel, ames_plsr$bestTune$ncomp)
```





```r
allResamples <- resamples(list(
                               "Ridge" = ames_ridge,
                               "Lasso" = ames_lasso,
                               "EN" = ames_en,
                               "PLSR" = ames_plsr, 
                               "PCR" = ames_pcr
                               ))
```

```
## Error in resamples(list(Ridge = ames_ridge, Lasso = ames_lasso, EN = ames_en, : object 'ames_ridge' not found
```

```r
bwplot(allResamples)
```

```
## Error in bwplot(allResamples): object 'allResamples' not found
```

```r
parallelplot(allResamples)
```

```
## Error in parallelplot(allResamples): object 'allResamples' not found
```

```r
parallelplot(allResamples , metric = "Rsquared")
```

```
## Error in parallelplot(allResamples, metric = "Rsquared"): object 'allResamples' not found
```

```r
parallelplot(allResamples , metric = "RMSE")
```

```
## Error in parallelplot(allResamples, metric = "RMSE"): object 'allResamples' not found
```






```r
# 1. stratified sampling with the rsample package
set.seed(123)
split  <- initial_split(ames, prop = 0.7, strata = "Sale_Price")
```

```
## Error in strata %in% vars: object 'ames' not found
```

```r
ames_train  <- training(split)
```

```
## Error: `x` should be an `rsplit` object
```

```r
ames_test   <- testing(split)
```

```
## Error: `x` should be an `rsplit` object
```

```r
# 2. create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )
# 3. create a hyperparameter grid search
hyper_grid <- expand.grid(k = seq(2, 26, by = 2))
# 4. execute grid search with knn model
#    use RMSE as preferred metric
knn_fit <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )
```

```
## Error in eval(expr, p): object 'ames_train' not found
```

```r
# 5. evaluate results
# print model results
knn_fit
```

```
## Error in eval(expr, envir, enclos): object 'knn_fit' not found
```

```r
## k-Nearest Neighbors 
## 
## 2054 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 1848, 1850, 1848, 1848, 1848, 1848, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE     
##    2  46100.84  0.6618945  30205.06
```

