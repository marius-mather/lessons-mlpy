---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 45-Xgboost.md in _episodes_rmd/
title: "Gradient boosting."
author: "Darya Vanichkina"
keypoints:
- FIXME
objectives:
- FIXME
questions:
- FIXME
source: Rmd
start: 0
teaching: 30
exercises: 0
---




~~~
library(tidyverse)
~~~
{: .language-r}



~~~
── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ──
~~~
{: .output}



~~~
✔ ggplot2 3.1.0       ✔ purrr   0.3.1  
✔ tibble  2.0.1       ✔ dplyr   0.8.0.1
✔ tidyr   0.8.3       ✔ stringr 1.4.0  
✔ readr   1.3.1       ✔ forcats 0.4.0  
~~~
{: .output}



~~~
Warning: package 'tibble' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'tidyr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'purrr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'dplyr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'stringr' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'forcats' was built under R version 3.5.2
~~~
{: .error}



~~~
── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
~~~
{: .output}



~~~
library(caret)
~~~
{: .language-r}



~~~
Loading required package: lattice
~~~
{: .output}



~~~

Attaching package: 'caret'
~~~
{: .output}



~~~
The following object is masked from 'package:purrr':

    lift
~~~
{: .output}



~~~
library(tidymodels)
~~~
{: .language-r}



~~~
── Attaching packages ───────────────────────────────── tidymodels 0.0.2 ──
~~~
{: .output}



~~~
✔ broom     0.5.1     ✔ recipes   0.1.4
✔ dials     0.0.2     ✔ rsample   0.0.4
✔ infer     0.4.0     ✔ yardstick 0.0.3
✔ parsnip   0.0.1     
~~~
{: .output}



~~~
Warning: package 'rsample' was built under R version 3.5.2
~~~
{: .error}



~~~
Warning: package 'yardstick' was built under R version 3.5.2
~~~
{: .error}



~~~
── Conflicts ──────────────────────────────────── tidymodels_conflicts() ──
✖ scales::discard()      masks purrr::discard()
✖ dplyr::filter()        masks stats::filter()
✖ recipes::fixed()       masks stringr::fixed()
✖ dplyr::lag()           masks stats::lag()
✖ caret::lift()          masks purrr::lift()
✖ yardstick::precision() masks caret::precision()
✖ yardstick::recall()    masks caret::recall()
✖ yardstick::spec()      masks readr::spec()
✖ recipes::step()        masks stats::step()
~~~
{: .output}



~~~
library(AmesHousing)
~~~
{: .language-r}


~~~
ameshousingFiltTrain <- readRDS("models/ameshousingFiltTrain.Rds")
ameshousingFiltTest <- readRDS("models/ameshousingFiltTest.Rds")

ameshousingFiltTrain_engineered <- readRDS("models/ameshousingFiltTrain_engineered.Rds")
ameshousingFiltTest_engineered <-readRDS("models/ameshousingFiltTest_engineered.Rds")

ames_resamplingCV <- readRDS("models/ames_resamplingCV.Rds")

# ames_lm_all <- readRDS("models/ames_lm_all.Rds")
ames_mars <- readRDS("models/ames_mars.Rds")
# predict(ames_mars, newdata = ameshousingFiltTest_engineered)
~~~
{: .language-r}




## Gradient boosting

~~~
library(gbm)
~~~
{: .language-r}



~~~
Warning: package 'gbm' was built under R version 3.5.2
~~~
{: .error}



~~~
Loaded gbm 2.1.5
~~~
{: .output}



~~~
# hyper_grid <- expand.grid(
#   shrinkage = c(.01, .1, .3),
#   interaction.depth = c(1, 3, 5),
#   n.minobsinnode = c(5, 10, 15),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,              
#   min_RMSE = 0                   
# )

# randomize data
random_index <- sample(1:nrow(ameshousingFiltTrain), nrow(ameshousingFiltTrain))
random_ames_train <- ameshousingFiltTrain[random_index, ]
random_ames_train <- random_ames_train %>% select(-Utilities)



# gbm_optimise <- function(hyper_grid){
# for(i in 1:nrow(hyper_grid)) {
#   
#   # reproducibility
#   set.seed(42)
#   
#   # train model
#   gbm.tune <- gbm(
#     formula = Sale_Price ~ .,
#     distribution = "gaussian",
#     data = random_ames_train,
#     n.trees = 5000,
#     interaction.depth = hyper_grid$interaction.depth[i],
#     shrinkage = hyper_grid$shrinkage[i],
#     n.minobsinnode = hyper_grid$n.minobsinnode[i],
#     bag.fraction = hyper_grid$bag.fraction[i],
#     train.fraction = .75,
#     n.cores = NULL, # will use all cores by default
#     verbose = FALSE
#   )
#   
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
#   hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
# }
# return(hyper_grid)
# }
# 
# # saveRDS(hyper_grid, "models/gbm_hyper_grid1.Rds")
# hyper_grid1 <- read_rds("models/gbm_hyper_grid1.Rds")
# 
# hyper_grid2 <- expand.grid(
#   shrinkage = c(.01, .05, .1),
#   interaction.depth = c(3, 5, 7),
#   n.minobsinnode = c(5, 7, 10),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,             
#   min_RMSE = 0                  
# )

#hyper_grid2 <- gbm_optimise(hyper_grid2)
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")

hyper_grid2 %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
~~~
{: .language-r}



~~~
   shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees
1       0.01                 3              5         0.65          4755
2       0.01                 3              7         0.80          4945
3       0.01                 3              7         0.65          4460
4       0.01                 3              5         0.80          4983
5       0.05                 3              7         0.65           950
6       0.05                 3              7         0.80          1347
7       0.01                 3             10         0.80          4987
8       0.01                 5              5         0.80          3941
9       0.01                 5              7         0.80          3368
10      0.01                 5              5         0.65          3532
   min_RMSE
1  19763.76
2  19836.14
3  19948.46
4  20035.70
5  20087.31
6  20113.89
7  20162.16
8  20206.30
9  20244.55
10 20276.48
~~~
{: .output}


## Train final model


~~~
# set.seed(42)
# # train GBM model
# ames_gbm <- gbm(
#   formula = Sale_Price ~ .,
#   distribution = "gaussian",
#   data = ameshousingFiltTrain,
#   n.trees = 483,
#   interaction.depth = 5,
#   shrinkage = 0.1,
#   n.minobsinnode = 5,
#   bag.fraction = .65, 
#   train.fraction = 1,
#   n.cores = NULL, # will use all cores by default
#   verbose = FALSE
#   )  
# 
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
# hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")
~~~
{: .language-r}




~~~
library(xgboost)
~~~
{: .language-r}



~~~
Warning: package 'xgboost' was built under R version 3.5.2
~~~
{: .error}



~~~

Attaching package: 'xgboost'
~~~
{: .output}



~~~
The following object is masked from 'package:dplyr':

    slice
~~~
{: .output}



~~~
# Prepare the training data
ames_features_train <- ameshousingFiltTrain_engineered %>% select(-Sale_Price) %>% as.matrix()
ames_price_train <- ameshousingFiltTrain_engineered %>% select(Sale_Price) %>% as.matrix()


# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(0.005, .01,0.05),
  max_depth = c(3, 5, 7),
  min_child_weight = c(3, 5, 7),
  subsample = c(.5, .65, .8), 
  colsample_bytree = c(.9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)


# # grid search 
# for(i in 1:nrow(hyper_grid)) {
#   
#   # create parameter list
#   params <- list(
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i]
#   )
#   
#   # reproducibility
#   set.seed(42)
#   
#   # train model
#   xgb.tune <- xgb.cv(
#     params = params,
#     data = ames_features_train,
#     label = ames_price_train,
#     nrounds = 5000,
#     nfold = 5,
#     objective = "reg:linear",  # for regression models
#     verbose = 0,               # silent,
#     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
#   )
#   
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
#   hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
# }
~~~
{: .language-r}


