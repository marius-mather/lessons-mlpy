---
title: "Regularised Regression. Principle components Regression. Partial Least Squares Regression."
author: "Darya Vanichkina"
keypoints:
- Regularisation helps us improve the performance of regression
- 
objectives:
- someobjective
questions:
- How do we prevent all variables from being incorporated into a regression model?
source: Rmd
start: 0
teaching: 30
exercises: 0
---



```{r loadLibraries}
library(tidyverse)
library(caret)
library(tidymodels)
library(AmesHousing)
old <- theme_set(theme_minimal())
rm(old)
```

```{r getData}

ameshousingFiltTrain <- readRDS("models/ameshousingFiltTrain.Rds")
ameshousingFiltTest <- readRDS("models/ameshousingFiltTest.Rds")
ameshousingFiltTrain_engineered <- readRDS("models/ameshousingFiltTrain_engineered.Rds")
ameshousingFiltTest_engineered <-readRDS("models/ameshousingFiltTest_engineered.Rds")
ames_resamplingCV <- readRDS("models/ames_resamplingCV.Rds")


```





```{r gridDefine}
param_search <- expand.grid(ncomp = seq(2, 
                                        dim(ameshousingFiltTest_engineered)[2] - 1,
                                        length.out = 20))
```



## Regularised Regression. 
### Ridge (alpha = 0 )

```{r Ridge, warning=FALSE}
# lambda_search  <- expand.grid( alpha = 0, lambda = c(0.1,1, 10, 100, 1000, 10000))
# set.seed(42)
# ames_ridge <- train(
#   Sale_Price ~ ., 
#   data = ameshousingFiltTrain_engineered,
#   preProcess = c("zv", "center", "scale"),
#   trControl = ames_resamplingCV,
#   method = "glmnet", 
#   tuneGrid = lambda_search,
#   metric = "RMSE"
#   )
# saveRDS(ames_ridge, "models/ames_ridge.Rds")

ames_ridge <- readRDS("models/ames_ridge.Rds")

# best model
ames_ridge$results %>%
  filter(
    alpha == ames_ridge$bestTune$alpha,
    lambda == ames_ridge$bestTune$lambda
    )

# plot results
plot(ames_ridge)
```

### Lasso (alpha = 1)


```{r Lasso, warning=FALSE}
# lambda_search  <- expand.grid( alpha = 1, lambda = c(0.1,1, 10, 100, 1000, 10000))
# set.seed(42)
# ames_lasso <- train(
#   Sale_Price ~ ., 
#   data = ameshousingFiltTrain_engineered,
#   preProcess = c("zv", "center", "scale"),
#   trControl = ames_resamplingCV,
#   method = "glmnet", 
#   tuneGrid = lambda_search,
#   metric = "RMSE"
#   )
# saveRDS(ames_lasso, "models/ames_lasso.Rds")

ames_lasso <- readRDS("models/ames_lasso.Rds")

# best model
ames_lasso$results %>%
  filter(
    alpha == ames_lasso$bestTune$alpha,
    lambda == ames_lasso$bestTune$lambda
    )

# plot results
plot(ames_lasso)
```


### Elastic net - mix of the two


```{r EN, warning=F}
# lambda_search  <- expand.grid( alpha = seq(0,1,0.2), lambda = c(0.1,1, 10, 100, 1000, 10000))
# set.seed(42)
# ames_en <- train(
#   Sale_Price ~ ., 
#   data = ameshousingFiltTrain_engineered,
#   preProcess = c("zv", "center", "scale"),
#   trControl = ames_resamplingCV,
#   method = "glmnet", 
#   tuneGrid = lambda_search,
#   metric = "RMSE"
#   )
# saveRDS(ames_en, "models/ames_en.Rds")

ames_en <- readRDS("models/ames_en.Rds")

# best model
ames_en$results %>%
  filter(
    alpha == ames_en$bestTune$alpha,
    lambda == ames_en$bestTune$lambda
    )

# plot results
plot(ames_en)
```



> ## Challenge
>
> Work with the grid search for the model above to identify which value of ncomp is optimal for this model?
> Do you think the performance of this model will be very good?
> 
> {: .source}
>
> > ## Solution
> > 
> >
> 
{: .challenge}



## Principle components regression

```{r PCR, cache = TRUE}
set.seed(42)
ames_pcr <- train(
  Sale_Price ~ .,
  data = ameshousingFiltTrain_engineered,
  trControl = ames_resamplingCV,
  method = "pcr",
  tuneGrid = param_search,
  metric = "RMSE"
  )
saveRDS(ames_pcr, "models/ames_pcr.Rds")

ames_pcr <- readRDS("models/ames_pcr.Rds")

# model with lowest RMSE
ames_pcr$bestTune
ames_pcr$results %>%
  filter(ncomp == as.numeric(ames_pcr$bestTune))

```




## Partial Least Squares Regression.

```{r PLS, cache = TRUE}
# set.seed(42)
# ames_plsr <- train(
#   Sale_Price ~ ., 
#   data = ameshousingFiltTrain_engineered, 
#   trControl = ames_resamplingCV,
#   method = "pls",
#   tuneGrid = param_search,
#   metric = "RMSE"
#   )
# saveRDS(ames_en, "models/ames_en.Rds")

ames_en <- readRDS("models/ames_en.Rds")

# model with lowest RMSE
ames_plsr$bestTune
ames_plsr$results %>%
  filter(ncomp == as.numeric(ames_plsr$bestTune))

# coef(ames_plsr$finalModel, ames_plsr$bestTune$ncomp)
```



## MARS
```{r mars, cache=TRUE}
# grid_search <- expand.grid(
#   nprune = seq(2, 80, length.out = 10) %>% floor(),
#   degree = 1:3
# )
# # perform resampling
# set.seed(42)
# ames_mars <- train(
#   Sale_Price ~ ., 
#   data = ameshousingFiltTrain_engineered, 
#   trControl = ames_resamplingCV,
#   method = "earth",
#   tuneGrid = grid_search,
#   metric = "RMSE"
#   )
#saveRDS(ames_mars, "data/ames_mars.Rds")

ames_mars <- readRDS("models/ames_mars.Rds")

# best model
ames_mars$results %>%
  filter(
    nprune == ames_mars$bestTune$nprune,
    degree == ames_mars$bestTune$degree
    )
```




```{r allresampes}
ames_lm_all <- readRDS("models/ames_lm_all.Rds")

allResamples <- resamples(list(
                               "Ridge" = ames_ridge,
                               "Lasso" = ames_lasso,
                               "EN" = ames_en,
                               "PLSR" = ames_plsr, 
                               "PCR" = ames_pcr,
                               "MARS" = ames_mars,
                               "LM all" = ames_lm_all
                               ))
bwplot(allResamples)
parallelplot(allResamples)

parallelplot(allResamples , metric = "Rsquared")
parallelplot(allResamples , metric = "RMSE")

summary(allResamples)$statistics$RMSE %>% as.data.frame() %>% rownames_to_column()  %>% arrange(Median)

```




