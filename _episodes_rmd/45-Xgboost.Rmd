---
title: "Gradient boosting."
author: "Darya Vanichkina"
keypoints:
- FIXME
objectives:
- FIXME
questions:
- FIXME
source: Rmd
start: 0
teaching: 30
exercises: 0
---



```{r loadLibraries}
library(tidyverse)
library(caret)
library(tidymodels)
library(AmesHousing)
```

```{r getData}
ameshousingFiltTrain <- readRDS("models/ameshousingFiltTrain.Rds")
ameshousingFiltTest <- readRDS("models/ameshousingFiltTest.Rds")

ameshousingFiltTrain_engineered <- readRDS("models/ameshousingFiltTrain_engineered.Rds")
ameshousingFiltTest_engineered <-readRDS("models/ameshousingFiltTest_engineered.Rds")

ames_resamplingCV <- readRDS("models/ames_resamplingCV.Rds")

# ames_lm_all <- readRDS("models/ames_lm_all.Rds")
ames_mars <- readRDS("models/ames_mars.Rds")
# predict(ames_mars, newdata = ameshousingFiltTest_engineered)

```




## Gradient boosting
```{r optimisation}
library(gbm)

# hyper_grid <- expand.grid(
#   shrinkage = c(.01, .1, .3),
#   interaction.depth = c(1, 3, 5),
#   n.minobsinnode = c(5, 10, 15),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,              
#   min_RMSE = 0                   
# )

# randomize data
random_index <- sample(1:nrow(ameshousingFiltTrain), nrow(ameshousingFiltTrain))
random_ames_train <- ameshousingFiltTrain[random_index, ]
random_ames_train <- random_ames_train %>% select(-Utilities)



# gbm_optimise <- function(hyper_grid){
# for(i in 1:nrow(hyper_grid)) {
#   
#   # reproducibility
#   set.seed(42)
#   
#   # train model
#   gbm.tune <- gbm(
#     formula = Sale_Price ~ .,
#     distribution = "gaussian",
#     data = random_ames_train,
#     n.trees = 5000,
#     interaction.depth = hyper_grid$interaction.depth[i],
#     shrinkage = hyper_grid$shrinkage[i],
#     n.minobsinnode = hyper_grid$n.minobsinnode[i],
#     bag.fraction = hyper_grid$bag.fraction[i],
#     train.fraction = .75,
#     n.cores = NULL, # will use all cores by default
#     verbose = FALSE
#   )
#   
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
#   hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
# }
# return(hyper_grid)
# }
# 
# # saveRDS(hyper_grid, "models/gbm_hyper_grid1.Rds")
# hyper_grid1 <- read_rds("models/gbm_hyper_grid1.Rds")
# 
# hyper_grid2 <- expand.grid(
#   shrinkage = c(.01, .05, .1),
#   interaction.depth = c(3, 5, 7),
#   n.minobsinnode = c(5, 7, 10),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,             
#   min_RMSE = 0                  
# )

#hyper_grid2 <- gbm_optimise(hyper_grid2)
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")

hyper_grid2 %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```


## Train final model

```{r optimalGBM}
# set.seed(42)
# # train GBM model
# ames_gbm <- gbm(
#   formula = Sale_Price ~ .,
#   distribution = "gaussian",
#   data = ameshousingFiltTrain,
#   n.trees = 483,
#   interaction.depth = 5,
#   shrinkage = 0.1,
#   n.minobsinnode = 5,
#   bag.fraction = .65, 
#   train.fraction = 1,
#   n.cores = NULL, # will use all cores by default
#   verbose = FALSE
#   )  
# 
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
# hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")
```



```{r}
library(xgboost)
# Prepare the training data
ames_features_train <- ameshousingFiltTrain_engineered %>% select(-Sale_Price) %>% as.matrix()
ames_price_train <- ameshousingFiltTrain_engineered %>% select(Sale_Price) %>% as.matrix()


# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(0.005, .01,0.05),
  max_depth = c(3, 5, 7),
  min_child_weight = c(3, 5, 7),
  subsample = c(.5, .65, .8), 
  colsample_bytree = c(.9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)


# # grid search 
# for(i in 1:nrow(hyper_grid)) {
#   
#   # create parameter list
#   params <- list(
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i]
#   )
#   
#   # reproducibility
#   set.seed(42)
#   
#   # train model
#   xgb.tune <- xgb.cv(
#     params = params,
#     data = ames_features_train,
#     label = ames_price_train,
#     nrounds = 5000,
#     nfold = 5,
#     objective = "reg:linear",  # for regression models
#     verbose = 0,               # silent,
#     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
#   )
#   
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
#   hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
# }


```


