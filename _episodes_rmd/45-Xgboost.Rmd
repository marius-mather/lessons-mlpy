---
title: "Gradient boosting. XGBoost."
author: "Darya Vanichkina"
source: Rmd
start: 0
teaching: 30
exercises: 0
---
```{r echo=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("45-")
```


```{r loadLibraries}
library(tidyverse)
library(caret)
library(tidymodels)
library(AmesHousing)
```

```{r getData}
ameshousingFiltTrain <- readRDS("models/ameshousingFiltTrain.Rds")
ameshousingFiltTest <- readRDS("models/ameshousingFiltTest.Rds")

ameshousingFiltTrain_engineered <- readRDS("models/ameshousingFiltTrain_engineered.Rds")
ameshousingFiltTest_engineered <-readRDS("models/ameshousingFiltTest_engineered.Rds")

ames_resamplingCV <- readRDS("models/ames_resamplingCV.Rds")

# predict(ames_mars, newdata = ameshousingFiltTest_engineered)


ames_lm_all <- readRDS("models/ames_lm_all.Rds")
ames_mars <- readRDS("models/ames_mars.Rds")
ames_ridge <- readRDS("models/ames_ridge.Rds")
ames_lasso <- readRDS("models/ames_lasso.Rds")
ames_en <- readRDS("models/ames_en.Rds")
ames_plsr <- readRDS("models/ames_plsr.Rds")
ames_pcr <- readRDS("models/ames_pcr.Rds")
ames_knn <- readRDS("models/ames_knn.Rds")
ames_rf <- readRDS("models/ames_rf.Rds")
```




## Gradient boosting
```{r optimisation}
library(gbm)

# hyper_grid <- expand.grid(
#   shrinkage = c(.01, .1, .3),
#   interaction.depth = c(1, 3, 5),
#   n.minobsinnode = c(5, 10, 15),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,              
#   min_RMSE = 0                   
# )

# randomize data
random_index <- sample(1:nrow(ameshousingFiltTrain), nrow(ameshousingFiltTrain))
random_ames_train <- ameshousingFiltTrain[random_index, ]
random_ames_train <- random_ames_train %>% select(-Utilities)



# gbm_optimise <- function(hyper_grid){
# for(i in 1:nrow(hyper_grid)) {
#   
#   # reproducibility
#   set.seed(42)
#   
#   # train model
#   gbm.tune <- gbm(
#     formula = Sale_Price ~ .,
#     distribution = "gaussian",
#     data = random_ames_train,
#     n.trees = 5000,
#     interaction.depth = hyper_grid$interaction.depth[i],
#     shrinkage = hyper_grid$shrinkage[i],
#     n.minobsinnode = hyper_grid$n.minobsinnode[i],
#     bag.fraction = hyper_grid$bag.fraction[i],
#     train.fraction = .75,
#     n.cores = NULL, # will use all cores by default
#     verbose = FALSE
#   )
#   
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
#   hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
# }
# return(hyper_grid)
# }
# 
# # saveRDS(hyper_grid, "models/gbm_hyper_grid1.Rds")
# hyper_grid1 <- read_rds("models/gbm_hyper_grid1.Rds")
# 
# hyper_grid2 <- expand.grid(
#   shrinkage = c(.01, .05, .1),
#   interaction.depth = c(3, 5, 7),
#   n.minobsinnode = c(5, 7, 10),
#   bag.fraction = c(.65, .8, 1), 
#   optimal_trees = 0,             
#   min_RMSE = 0                  
# )

#hyper_grid2 <- gbm_optimise(hyper_grid2)
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")

hyper_grid2 %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```


## Train final model

```{r optimalGBM}
# set.seed(42)
# # train GBM model
# ames_gbm <- gbm(
#   formula = Sale_Price ~ .,
#   distribution = "gaussian",
#   data = ameshousingFiltTrain,
#   n.trees = 483,
#   interaction.depth = 5,
#   shrinkage = 0.1,
#   n.minobsinnode = 5,
#   bag.fraction = .65, 
#   train.fraction = 1,
#   n.cores = NULL, # will use all cores by default
#   verbose = FALSE
#   )  
# 
# saveRDS(hyper_grid2, "models/gbm_hyper_grid2.Rds")
# hyper_grid2 <- read_rds("models/gbm_hyper_grid2.Rds")
```



```{r}
library(xgboost)
# Prepare the training data
ames_features_train <- ameshousingFiltTrain_engineered %>% select(-Sale_Price) %>% as.matrix()
ames_price_train <- ameshousingFiltTrain_engineered %>% select(Sale_Price) %>% as.matrix()


# ran this on HPC
# # create hyperparameter grid
# 
# hyper_grid <- expand.grid(
#   eta = c(0.005, .01, 0.05, .1, .3),
#   max_depth = c(1, 3, 5, 7),
#   min_child_weight = c(3, 5, 7),
#   subsample = c(.5, .65, .8, 1),
#   colsample_bytree = c(.8, .9, 1),
#   optimal_trees = 0,               # a place to dump results
#   min_RMSE = 0                     # a place to dump results
# )
# 
# 
# # grid search
# for(i in 1:nrow(hyper_grid)) {
# 
#   # create parameter list
#   params <- list(
#     eta = hyper_grid$eta[i],
#     max_depth = hyper_grid$max_depth[i],
#     min_child_weight = hyper_grid$min_child_weight[i],
#     subsample = hyper_grid$subsample[i],
#     colsample_bytree = hyper_grid$colsample_bytree[i]
#   )
# 
#   # reproducibility
#   set.seed(42)
# 
#   # train model
#   xgb.tune <- xgb.cv(
#     params = params,
#     data = ames_features_train,
#     label = ames_price_train,
#     nrounds = 5000,
#     nfold = 5,
#     objective = "reg:linear",  # for regression models
#     verbose = 0,               # silent,
#     early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
#   )
# 
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
#   hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
# }

# optimal model:
# eta	max_depth	min_child_weight	subsample	colsample_bytree	optimal_trees	min_RMSE
# grid_0.005_5.csv:0.005	5	7	0.65	0.9	2989	0.1197378

ames_xgb <- readRDS("models/grid_0.005_5.Rds")

# #
# params <- list(
#   eta = 0.005,
#   max_depth = 5,
#   min_child_weight = 7,
#   subsample = 0.65,
#   colsample_bytree = 0.9,
#   nrounds = 2989
# )
# 
# # 
# # # train final model
# # ames_xgb2 <- xgboost(
# #   params = params,
# #   data = ames_features_train,
# #   label = ames_price_train,
# #   nrounds = 2989,
# #   objective = "reg:linear",
# #   verbose = 0
# # )
# 
# ames_xgb2 <- readRDS("models/ames_xgb2.Rds")
# 
# 
# ames_xgb_4caret <- caret::train(
#   x = ames_features_train, # tr_x is data frame, xgbTree needs matrix
#   y = as.vector(ames_price_train),
#   trControl = ames_resamplingCV,
#   tuneGrid = expand.grid(nrounds = 2989, eta = 0.005,  max_depth = 5,  min_child_weight = 7,  subsample = 0.65,  colsample_bytree = 0.9 , gamma = 0),
#   method = "xgbTree",
#   verbose = TRUE
# )
# saveRDS(ames_xgb_4caret, "models/ames_xgb_4caret.Rds")

ames_xgb_4caret <- readRDS("models/ames_xgb_4caret.Rds")


```


```{r Compare}
allResamples <- resamples(list(
                               "Ridge" = ames_ridge,
                               "Lasso" = ames_lasso,
                               "EN" = ames_en,
                               "PLSR" = ames_plsr, 
                               "PCR" = ames_pcr,
                               "MARS" = ames_mars,
                               "LM all" = ames_lm_all,
                               "RF" = ames_rf, 
                               "KNN" = ames_knn, 
                               "XGB" = ames_xgb_4caret
                               ))

bwplot(allResamples)
parallelplot(allResamples)

parallelplot(allResamples , metric = "Rsquared")
parallelplot(allResamples , metric = "RMSE")

summary(allResamples)$statistics$RMSE %>% as.data.frame() %>% rownames_to_column()  %>% arrange(Median)
```


