{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "---\n",
    "title: \"Gradient boosting. XGBoost\"\n",
    "author: \"Darya Vanichkina\"\n",
    "exercises: 30\n",
    "questions:\n",
    "- What are some newer approaches to ML?\n",
    "- What are their pros and cons?\n",
    "objectives:\n",
    "- To appreciate that more advanced methods can have a much larger number of parameters to optimise\n",
    "- This optimisation is best done iteratively on the HPC\n",
    "keypoints: \n",
    "- Modern approaches such as GBM and XGBoost can drastically improve prediction performance\n",
    "- Tuning hyperparameters is, however, computationally expensive \n",
    "source: Rmd\n",
    "teaching: 60\n",
    "exercises: 30\n",
    "bibliography: references.bib\n",
    "---\n",
    "-->\n",
    "\n",
    "### GBM. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'DeprecationDict' on <module 'sklearn.utils.deprecation' from '/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ce674bb95fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mobjectname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjectname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" = pickle.load(f)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'DeprecationDict' on <module 'sklearn.utils.deprecation' from '/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py'>"
     ]
    }
   ],
   "source": [
    "# when delivering live coding, these libraries and all code in this cell have already been loaded\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'py-earth')\n",
    "from pyearth import Earth\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Set up plotting options for seaborn and matplotlib\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle','models/ames_ols_all.pickle',\n",
    "                'models/ames_ridge.pickle','models/ames_lasso.pickle', \n",
    "                'models/ames_enet.pickle','models/ames_mars.pickle',\n",
    "               'models/ames_pcr.pickle', 'models/ames_plsr.pickle',\n",
    "               'models/ames_RF.pickle', 'models/ames_knn.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "## \n",
    "def assess_model_fit(listOfModels,\n",
    "                     listOfMethodNamesAsStrings, \n",
    "                     datasetX, \n",
    "                     datasetY):\n",
    "    columns= ['RMSE', 'R2', 'MAE']\n",
    "    rows=listOfMethodNamesAsStrings\n",
    "    results=pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "    for i, method in enumerate(listOfModels):\n",
    "        tmp_dataset_X=datasetX\n",
    "        # while we build the model and predict on the log10Transformed sale price, we display the error in dollars\n",
    "        # as that makes more sense\n",
    "        y_pred=10**(method.predict(tmp_dataset_X))\n",
    "        results.iloc[i,0] = np.sqrt(mean_squared_error(10**(datasetY), y_pred))\n",
    "        results.iloc[i,1] = r2_score(10**(datasetY), y_pred)\n",
    "        results.iloc[i,2] = mean_absolute_error(10**(datasetY), y_pred)\n",
    "    return(results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these need to be loaded for this section\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Unlike algorithms we've worked with up to now, GBM has a substantially larger number of parameters. While we could optimise them all together using a GridSearchCV(), it makes more sense to do so iteratively, honing down on the best parameter sets one by one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a `learning_rate` of 0.1, and identify how many `n_estimators` we'd use to fit the model. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 280}\n",
      "0.9068897670500741\n"
     ]
    }
   ],
   "source": [
    "# param_test1 = {'n_estimators': list(range(20, len(predictors)+1, 10))}\n",
    "\n",
    "# gbm1 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "#        loss='ls',\n",
    "#        learning_rate=0.1, \n",
    "#        min_samples_split=20, \n",
    "#        min_samples_leaf=10,\n",
    "#        max_depth=7, \n",
    "#        max_features='sqrt', \n",
    "#        subsample=0.8,   \n",
    "#        random_state=42),\n",
    "#    param_grid=param_test1, \n",
    "#    n_jobs=4, \n",
    "#    iid=False,\n",
    "#    cv=10)\n",
    "# gbm1.fit(ames_train_X, ames_train_y)\n",
    "# pickle.dump(gbm1, open('models/gbm1.pickle', 'wb'))\n",
    "\n",
    "with open('models/gbm1.pickle', 'rb') as f:\n",
    "    gbm1 = pickle.load(f)\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(gbm1.best_params_)\n",
    "print(gbm1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many estimators did we test?\n",
    "#print(param_test1)\n",
    "#print(len(predictors))\n",
    "\n",
    "\n",
    "# \n",
    "list(np.arange(0.001, 0.31, 0.05).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the optimum number was found to be 280 (almost all of them!) - but we only tested 270 and 286. Perhaps the number is between these two? :et's see if we can find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 280}\n",
      "0.9068897670500741\n",
      "{'n_estimators': [270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286]}\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {'n_estimators': list(range(270, len(predictors)+1, 1))}\n",
    "\n",
    "# gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "#        loss='ls',\n",
    "#        learning_rate=0.1, \n",
    "#        min_samples_split=20, \n",
    "#        min_samples_leaf=10,\n",
    "#        max_depth=7, \n",
    "#        max_features='sqrt', \n",
    "#        subsample=0.8,   \n",
    "#        random_state=42),\n",
    "#    param_grid=param_test1, \n",
    "#    n_jobs=4, \n",
    "#    iid=False, \n",
    "#    cv=10)\n",
    "# gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "\n",
    "# print(gbm2.best_params_)\n",
    "# print(gbm2.best_score_)\n",
    "print(param_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that with a `learning_rate` of 0.1, the optimal number of `n_estimators` is 280. (From this point, we start running things on the HPC, to achieve faster optimisation rates. However, this also means that we may not be able to match our pickles with those from our desktop, due to incompatibility of python package versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to optimise tree-specific parameters, starting with `max_depth` and `min_samples_split`. These correspond to:\n",
    "\n",
    "- `min_samples_split` : The minimum number of samples required to split an internal node. Values around 0.5-1% of the dataset can be feasible\n",
    "    - We will test values of 5 to 30, in increments of 5\n",
    "- `max_depth` : maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. \n",
    "    - We will test values of 3 to 15, in increments of 2\n",
    "    \n",
    "This ends up being quite an expansive grid search, as we need to fit 6 * 7 = 42 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'min_samples_split': list(range(5, 31, 5)),\n",
    "              'max_depth': list(range(3,16, 2))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we can use Artemis, the university's HPC cluster. A simple way to do this is to fit each of the `min_samples_split` independently, using a python and bash script. The python script we'd use would look like this:\n",
    "It would write out the best model to a tsv file, with the corresponding paramter, after which we could use Unix to look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Import libraries:\n",
    "import os\n",
    "os.chdir('/scratch/RDS-LALA/darya/gbm')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GBM model fitting')\n",
    "parser.add_argument('min_samples_split', type=int, help='min_samples_split value to test')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_test = {'min_samples_split': [args.min_samples_split],\n",
    "              'max_depth': list(range(3,16, 2))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       learning_rate=0.1,\n",
    "       n_estimators = 279,\n",
    "       min_samples_leaf=10,\n",
    "       max_features='sqrt',\n",
    "       subsample=0.8,\n",
    "       random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "filename = 'fit1/' + str(args.min_samples_split) + 'fit.tsv'\n",
    "with open(filename, 'w') as f:\n",
    "    print(gbm2.best_score_,\"\\t\",gbm2.best_params_, file = f)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shell script we could use is very basic:\n",
    "\n",
    "```\n",
    "python /home/darya/gbm/optimise1.py $PARAM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we could execute it using the following qsub commands:\n",
    "\n",
    "```\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this are as follows. \n",
    "```\n",
    "0.9107460935801089 \t {'max_depth': 7, 'min_samples_split': 30}\n",
    "```\n",
    "\n",
    "It's clear that we need to test higher values of the `min_samples_split`, so we can run another round of the optimisation with:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=50 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=55 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then arrive at the following optimum:\n",
    "\n",
    "```\n",
    "0.9109466273291733 \t {'max_depth': 7, 'min_samples_split': 40}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the fit of our model, let's try `max_depth` of 6, 7, or 8 and `min_samples_split` of 35 to 45 (increments of 1).\n",
    "```\n",
    "param_test = {'min_samples_split': list(range(35,46,1)),\n",
    "              'max_depth': list(range(6,9, 1))}\n",
    "```\n",
    "\n",
    "We finally arrive at the optimum of:\n",
    "```\n",
    "0.9118552463171431 \t {'max_depth': 7, 'min_samples_split': 41}\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "Let's leave `max_depth` at 7, and test a range of values for:\n",
    "- `min_samples_split` (35 to 45, increments of 1)\n",
    "- `min_samples_leaf` (5 to 45, increments of 5)\n",
    "\n",
    "We can use the HPC, as above, to run the code, or run it locally on our machine (takes ~20 mins):\n",
    "\n",
    "```\n",
    "param_test = {'min_samples_split': list(range(35,46,1)),\n",
    "              'min_samples_leaf': list(range(5,36, 5))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       learning_rate=0.1, \n",
    "       n_estimators = 279,\n",
    "       max_depth=7, \n",
    "       max_features='sqrt', \n",
    "       subsample=0.8,   \n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "```\n",
    "\n",
    "\n",
    "The outcome of this is\n",
    "\n",
    "```\n",
    "0.9151989049335256 \t {'min_samples_leaf': 5, 'min_samples_split': 36}\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "Next, let's tune the `max_features` parameter, trying values from 'auto' to 'log2' to 'sqrt' to only 20 parameters. \n",
    "\n",
    "```\n",
    "param_test = {'max_features': ['auto','log2','sqrt'],\n",
    "              'min_samples_leaf': list(range(5,21, 5))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       min_samples_split = 36,\n",
    "       learning_rate=0.1, \n",
    "       n_estimators = 279,\n",
    "       max_depth=7,  \n",
    "       subsample=0.8,   \n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "```\n",
    "\n",
    "The resulf of this is:\n",
    "\n",
    "`0.9151989049335256 \t {'min_samples_leaf': 5, 'max_features': 'sqrt'}`\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's go back to optimising the number of trees and learning rate, this time going with a lower learning rate (because the lower the learning rate, the slower the CV will be):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "param_test = {'n_estimators': list(range(275, len(predictors)+1, 1)),\n",
    "             'learning_rate': list(np.arange(0.001, 1.0, 0.1)),\n",
    "             'subsample': list(np.arange(0.6, 1.0))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       max_depth=7, \n",
    "       max_features='sqrt', \n",
    "       min_samples_leaf=5,\n",
    "       min_samples_split = 36,\n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.9117722351540429 \t {'learning_rate': 0.101, 'n_estimators': 280, 'subsample': 0.6}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also have \"brute forced\" the estimation using HPC, combining 3 scripts:\n",
    "\n",
    "```\n",
    "# optimise2.py ----------------\n",
    "#\n",
    "#\n",
    "# Import libraries:\n",
    "import os\n",
    "os.chdir('/scratch/RDS-CORE-SIH4HPC-RW/darya/gbm')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GBM model fitting')\n",
    "parser.add_argument('min_samples_split', type=int, help='min_samples_split value to test')\n",
    "parser.add_argument('n_estimators', type=int, help='n_estimators value to test')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "# [args.min_samples_split]\n",
    "\n",
    "param_test = {'n_estimators': [args.n_estimators], \n",
    "            'min_samples_split' : [args.min_samples_split], \n",
    "             'learning_rate': list(np.arange(0.001, 0.31, 0.05).round(2)), \n",
    "             'min_samples_leaf':  list(range(1,22, 4)),\n",
    "             'max_depth': [1,3,5,6,7,8,9,10,15],\n",
    "             'subsample': [0.65, 0.75, 0.8, 0.85, 0.9]\n",
    "             }\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       max_features='sqrt',\n",
    "       random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "\n",
    "\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "filename = 'fit1/' + str(args.min_samples_split) + \"_\" + str(args.n_estimators) + 'fit.tsv'\n",
    "with open(filename, 'w') as f:\n",
    "    print(gbm2.best_score_,\"\\t\",gbm2.best_params_, file = f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "####----\n",
    "optimise2.pbs\n",
    "#PBS -P PROJECTNAME\n",
    "#PBS -N gbmOpt3\n",
    "#PBS -l select=1:ncpus=3:mem=2gb\n",
    "#PBS -l walltime=02:0:00\n",
    "#PBS -J 100-286:5\n",
    "\n",
    "# unload all modules\n",
    "module purge\n",
    "\n",
    "cd /home/dvanichkina/scratch_sih/darya/gbm/\n",
    "python /home/dvanichkina/scratch_sih/darya/gbm/optimise2.py $PARAM $PBS_ARRAY_INDEX\n",
    "\n",
    "\n",
    "\n",
    "#### --------------\n",
    "# optimise2.sh\n",
    "qsub -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this finishes running, the optimal parameter values we obtain are:\n",
    "```\n",
    "0.918410561940631 \t {'learning_rate': 0.051, 'n_estimators': 280, 'min_samples_split': 35, 'max_depth': 7, 'subsample': 0.8, 'min_samples_leaf': 1}\n",
    "```\n",
    "\n",
    "Let's fit this model locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 280}\n",
      "0.9182771422480472\n"
     ]
    }
   ],
   "source": [
    "param_test = {'n_estimators': [280]}\n",
    "\n",
    "ames_gbm = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "        loss='ls',\n",
    "        learning_rate=0.051, \n",
    "        min_samples_split=35, \n",
    "        min_samples_leaf=1,\n",
    "        max_depth=7, \n",
    "        max_features='sqrt', \n",
    "        subsample=0.8,   \n",
    "        random_state=42),\n",
    "    param_grid=param_test, \n",
    "    n_jobs=4, \n",
    "    iid=False,\n",
    "    cv=10)\n",
    "\n",
    "#ames_gbm.fit(ames_train_X, ames_train_y)\n",
    "#pickle.dump(ames_gbm, open('models/ames_gbm.pickle', 'wb'))\n",
    "\n",
    "with open('models/ames_gbm.pickle', 'rb') as f:\n",
    "    ames_gbm = pickle.load(f)\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(ames_gbm.best_params_)\n",
    "print(ames_gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## XGBoost\n",
    "\n",
    "\n",
    "1. First, optimise `max_depth` and `min_child_weight`, with a varying number of `n_estimators` and a fixed, large `learning_rate` 0.1.\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_test = {\n",
    " 'max_depth':list(range(3,10,2)),\n",
    " 'min_child_weight':list(range(1,7,2)),\n",
    " 'n_estimators': [180, 230, 280]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "xgbmodel = GridSearchCV(estimator=xgb.XGBRegressor(\n",
    "  objective='reg:linear',\n",
    "  learning_rate =0.1,\n",
    "  n_estimators=280,\n",
    "  gamma=0,\n",
    "  subsample=0.8,\n",
    "  colsample_bytree=0.8,\n",
    "  random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "   \n",
    "xgbmodel.fit(ames_train_X, ames_train_y)\n",
    "print(xgbmodel.best_score_,\"\\t\",xgbmodel.best_params_)\n",
    "\n",
    "# 0.9159701741062035 \t {'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 280}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, it turns out the best parameter values are:\n",
    "- 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 280\n",
    "\n",
    "Now, let's see if we can improve this even more:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[0.5,1,1.5,2],\n",
    " 'n_estimators': [275, 280, 285]\n",
    "}\n",
    "\n",
    "# 0.9162060722720288 \t {'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 285}\n",
    "\n",
    "```\n",
    "\n",
    "2. Next, optimise gamma, the minimum split loss. This is the minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285],\n",
    " 'gamma': [i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "# 0.9162060722720288 \t {'gamma': 0.0, 'min_child_weight': 0.5, 'max_depth': 3, 'n_estimators': 285}\n",
    "# Default zero seems best\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "3. Next, optimise `subsample` and `colsample_bytree`, which represent the proportion of data used to make each tree and how many features can go into a tree at each branch\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# 0.916463171211643 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.6, 'subsample': 0.8, 'n_estimators': 284}\n",
    "\n",
    "```\n",
    "\n",
    "Let's zero in on that, and subsample values plus/minus 0.05 around the idenfied optima:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75, 0.775,  0.8, 0.825, 0.85],\n",
    " 'colsample_bytree':[0.5, 0.55, 0.6]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Perhaps need to reduce those two even more, as their values are the lowest that we're testing!\n",
    "\n",
    "```\n",
    "\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.65, 0.7, 0.75, 0.8],\n",
    " 'colsample_bytree':[0.4,0.45, 0.5, 0.55]\n",
    "\n",
    "}\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "```\n",
    "\n",
    "\n",
    "4. Next, let's tune the regularisation parameters:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [285],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75],\n",
    " 'colsample_bytree':[0.5],\n",
    " 'reg_alpha':[0, 0.1, 0.2, 0.5, 1],\n",
    " 'reg_lambda': [0, 0.1, 0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'reg_lambda': 1, 'reg_alpha': 0, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "\n",
    "# defaults we've been using so far: alpha = 0 , lambda = 1, are optimal for us (i.e. ridge regression-like)\n",
    "```\n",
    "\n",
    "\n",
    "Let's fit the final model, and assess its performance on the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'gamma': 0, 'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 285, 'subsample': 0.75}\n",
      "0.9138998720966125\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [285],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75],\n",
    " 'colsample_bytree':[0.5]\n",
    "}\n",
    "\n",
    "ames_xgb = GridSearchCV(estimator=xgb.XGBRegressor(\n",
    "  objective='reg:linear',\n",
    "  learning_rate =0.1,\n",
    "  random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "   \n",
    "#ames_xgb.fit(ames_train_X, ames_train_y)\n",
    "#pickle.dump(ames_xgb, open('models/ames_xgb.pickle', 'wb'))\n",
    "\n",
    "with open('models/ames_xgb.pickle', 'rb') as f:\n",
    "    ames_xgb = pickle.load(f)\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(ames_xgb.best_params_)\n",
    "print(ames_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, even the code above runs out of RAM on my normal machine, so I optimised it all on Artemis HPC. I then ran code very similar to what we've used before to get the training and testing RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>8375.174</td>\n",
       "      <td>0.989</td>\n",
       "      <td>6203.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>12322.502</td>\n",
       "      <td>0.977</td>\n",
       "      <td>8832.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>17099.214</td>\n",
       "      <td>0.955</td>\n",
       "      <td>11062.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>18810.886</td>\n",
       "      <td>0.946</td>\n",
       "      <td>11761.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>19106.227</td>\n",
       "      <td>0.944</td>\n",
       "      <td>11995.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLSR</th>\n",
       "      <td>20072.891</td>\n",
       "      <td>0.938</td>\n",
       "      <td>12450.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>20365.796</td>\n",
       "      <td>0.936</td>\n",
       "      <td>12266.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARS</th>\n",
       "      <td>20980.471</td>\n",
       "      <td>0.933</td>\n",
       "      <td>14240.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENet</th>\n",
       "      <td>21292.160</td>\n",
       "      <td>0.931</td>\n",
       "      <td>12706.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>21330.847</td>\n",
       "      <td>0.930</td>\n",
       "      <td>12951.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>32778.022</td>\n",
       "      <td>0.836</td>\n",
       "      <td>20021.304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE     R2        MAE\n",
       "GB      8375.174  0.989   6203.898\n",
       "XGB    12322.502  0.977   8832.373\n",
       "RF     17099.214  0.955  11062.209\n",
       "OLS    18810.886  0.946  11761.208\n",
       "PCR    19106.227  0.944  11995.822\n",
       "PLSR   20072.891  0.938  12450.140\n",
       "Lasso  20365.796  0.936  12266.462\n",
       "MARS   20980.471  0.933  14240.862\n",
       "ENet   21292.160  0.931  12706.575\n",
       "Ridge  21330.847  0.930  12951.372\n",
       "kNN    32778.022  0.836  20021.304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the RMSE on the training data?\n",
    "\n",
    "assess_model_fit(listOfModels=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],\n",
    "                listOfMethodNamesAsStrings=['OLS','Ridge', 'Lasso', 'ENet','PCR','PLSR','MARS', 'RF', 'kNN', 'GB', \"XGB\"],\n",
    "                datasetX=ames_train_X,\n",
    "                datasetY=ames_train_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>22383.748</td>\n",
       "      <td>0.917</td>\n",
       "      <td>14238.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>23317.975</td>\n",
       "      <td>0.910</td>\n",
       "      <td>14104.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARS</th>\n",
       "      <td>24262.447</td>\n",
       "      <td>0.902</td>\n",
       "      <td>15162.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>27185.647</td>\n",
       "      <td>0.877</td>\n",
       "      <td>16619.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>36781.781</td>\n",
       "      <td>0.775</td>\n",
       "      <td>23913.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>47670.165</td>\n",
       "      <td>0.623</td>\n",
       "      <td>15758.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLSR</th>\n",
       "      <td>51509.933</td>\n",
       "      <td>0.560</td>\n",
       "      <td>15751.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENet</th>\n",
       "      <td>52482.808</td>\n",
       "      <td>0.543</td>\n",
       "      <td>15590.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>59592.173</td>\n",
       "      <td>0.411</td>\n",
       "      <td>15672.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>64658.779</td>\n",
       "      <td>0.306</td>\n",
       "      <td>16238.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>64792.914</td>\n",
       "      <td>0.303</td>\n",
       "      <td>16436.269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE     R2        MAE\n",
       "XGB    22383.748  0.917  14238.189\n",
       "GB     23317.975  0.910  14104.315\n",
       "MARS   24262.447  0.902  15162.366\n",
       "RF     27185.647  0.877  16619.358\n",
       "kNN    36781.781  0.775  23913.011\n",
       "Ridge  47670.165  0.623  15758.453\n",
       "PLSR   51509.933  0.560  15751.082\n",
       "ENet   52482.808  0.543  15590.977\n",
       "Lasso  59592.173  0.411  15672.588\n",
       "PCR    64658.779  0.306  16238.705\n",
       "OLS    64792.914  0.303  16436.269"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the RMSE on the test data?\n",
    "\n",
    "assess_model_fit(listOfModels=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],\n",
    "                listOfMethodNamesAsStrings=['OLS','Ridge', 'Lasso', 'ENet','PCR','PLSR','MARS', 'RF', 'kNN', 'GB', \"XGB\"],\n",
    "                datasetX=ames_test_X,\n",
    "                datasetY=ames_test_y).sort_values(\"RMSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
