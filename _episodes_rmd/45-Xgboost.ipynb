{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "---\n",
    "title: \"Gradient boosting. XGBoost\"\n",
    "author: \"Darya Vanichkina\"\n",
    "exercises: 30\n",
    "questions:\n",
    "- What are some newer approaches to ML?\n",
    "- What are their pros and cons?\n",
    "objectives:\n",
    "- To appreciate that more advanced methods can have a much larger number of parameters to optimise\n",
    "- This optimisation is best done iteratively on the HPC\n",
    "keypoints: \n",
    "- Modern approaches such as GBM and XGBoost can drastically improve prediction performance\n",
    "- Tuning hyperparameters is, however, computationally expensive \n",
    "source: Rmd\n",
    "teaching: 60\n",
    "exercises: 30\n",
    "bibliography: references.bib\n",
    "---\n",
    "-->\n",
    "\n",
    "### GBM. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when delivering live coding, these libraries and all code in this cell have already been loaded\n",
    "websiterendering = True\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'py-earth')\n",
    "from pyearth import Earth\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Set up plotting options for seaborn and matplotlib\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks') \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle','models/ames_ols_all.pickle',\n",
    "                'models/ames_ridge.pickle','models/ames_lasso.pickle', \n",
    "                'models/ames_enet.pickle','models/ames_mars.pickle',\n",
    "               'models/ames_pcr.pickle', 'models/ames_plsr.pickle',\n",
    "               'models/ames_RF.pickle', 'models/ames_knn.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "## \n",
    "def assess_model_fit(listOfModels,\n",
    "                     listOfMethodNamesAsStrings, \n",
    "                     datasetX, \n",
    "                     datasetY):\n",
    "    columns= ['RMSE', 'R2', 'MAE']\n",
    "    rows=listOfMethodNamesAsStrings\n",
    "    results=pd.DataFrame(0.0, columns=columns, index=rows)\n",
    "    for i, method in enumerate(listOfModels):\n",
    "        tmp_dataset_X=datasetX\n",
    "        # while we build the model and predict on the log10Transformed sale price, we display the error in dollars\n",
    "        # as that makes more sense\n",
    "        y_pred=10**(method.predict(tmp_dataset_X))\n",
    "        results.iloc[i,0] = np.sqrt(mean_squared_error(10**(datasetY), y_pred))\n",
    "        results.iloc[i,1] = r2_score(10**(datasetY), y_pred)\n",
    "        results.iloc[i,2] = mean_absolute_error(10**(datasetY), y_pred)\n",
    "    return(results.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these need to be loaded for this section\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Unlike algorithms we've worked with up to now, GBM has a substantially larger number of parameters. While we could optimise them all together using a GridSearchCV(), it makes more sense to do so iteratively, honing down on the best parameter sets one by one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a `learning_rate` of 0.1, and identify how many `n_estimators` we'd use to fit the model. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 280}\n",
      "0.9068897670500741\n"
     ]
    }
   ],
   "source": [
    "# param_test1 = {'n_estimators': list(range(20, len(predictors)+1, 10))}\n",
    "\n",
    "# gbm1 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "#        loss='ls',\n",
    "#        learning_rate=0.1, \n",
    "#        min_samples_split=20, \n",
    "#        min_samples_leaf=10,\n",
    "#        max_depth=7, \n",
    "#        max_features='sqrt', \n",
    "#        subsample=0.8,   \n",
    "#        random_state=42),\n",
    "#    param_grid=param_test1, \n",
    "#    n_jobs=4, \n",
    "#    iid=False,\n",
    "#    cv=10)\n",
    "# gbm1.fit(ames_train_X, ames_train_y)\n",
    "# pickle.dump(gbm1, open('models/gbm1.pickle', 'wb'))\n",
    "\n",
    "with open('models/gbm1.pickle', 'rb') as f:\n",
    "    gbm1 = pickle.load(f)\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(gbm1.best_params_)\n",
    "print(gbm1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many estimators did we test?\n",
    "#print(param_test1)\n",
    "#print(len(predictors))\n",
    "\n",
    "\n",
    "# \n",
    "list(np.arange(0.001, 0.31, 0.05).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the optimum number was found to be 280 (almost all of them!) - but we only tested 270 and 286. Perhaps the number is between these two? :et's see if we can find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286]}\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {'n_estimators': list(range(270, len(predictors)+1, 1))}\n",
    "\n",
    "# gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "#        loss='ls',\n",
    "#        learning_rate=0.1, \n",
    "#        min_samples_split=20, \n",
    "#        min_samples_leaf=10,\n",
    "#        max_depth=7, \n",
    "#        max_features='sqrt', \n",
    "#        subsample=0.8,   \n",
    "#        random_state=42),\n",
    "#    param_grid=param_test1, \n",
    "#    n_jobs=4, \n",
    "#    iid=False, \n",
    "#    cv=10)\n",
    "# gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "\n",
    "# print(gbm2.best_params_)\n",
    "# print(gbm2.best_score_)\n",
    "print(param_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that with a `learning_rate` of 0.1, the optimal number of `n_estimators` is 280. (From this point, we start running things on the HPC, to achieve faster optimisation rates. However, this also means that we may not be able to match our pickles with those from our desktop, due to incompatibility of python package versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to optimise tree-specific parameters, starting with `max_depth` and `min_samples_split`. These correspond to:\n",
    "\n",
    "- `min_samples_split` : The minimum number of samples required to split an internal node. Values around 0.5-1% of the dataset can be feasible\n",
    "    - We will test values of 5 to 30, in increments of 5\n",
    "- `max_depth` : maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. \n",
    "    - We will test values of 3 to 15, in increments of 2\n",
    "    \n",
    "This ends up being quite an expansive grid search, as we need to fit 6 * 7 = 42 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {'min_samples_split': list(range(5, 31, 5)),\n",
    "              'max_depth': list(range(3,16, 2))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we can use Artemis, the university's HPC cluster. A simple way to do this is to fit each of the `min_samples_split` independently, using a python and bash script. The python script we'd use would look like this:\n",
    "It would write out the best model to a tsv file, with the corresponding paramter, after which we could use Unix to look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Import libraries:\n",
    "import os\n",
    "os.chdir('/scratch/RDS-LALA/darya/gbm')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GBM model fitting')\n",
    "parser.add_argument('min_samples_split', type=int, help='min_samples_split value to test')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_test = {'min_samples_split': [args.min_samples_split],\n",
    "              'max_depth': list(range(3,16, 2))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       learning_rate=0.1,\n",
    "       n_estimators = 279,\n",
    "       min_samples_leaf=10,\n",
    "       max_features='sqrt',\n",
    "       subsample=0.8,\n",
    "       random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "filename = 'fit1/' + str(args.min_samples_split) + 'fit.tsv'\n",
    "with open(filename, 'w') as f:\n",
    "    print(gbm2.best_score_,\"\\t\",gbm2.best_params_, file = f)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shell script we could use is very basic:\n",
    "\n",
    "```\n",
    "python /home/darya/gbm/optimise1.py $PARAM\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we could execute it using the following qsub commands:\n",
    "\n",
    "```\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this are as follows. \n",
    "```\n",
    "0.9107460935801089 \t {'max_depth': 7, 'min_samples_split': 30}\n",
    "```\n",
    "\n",
    "It's clear that we need to test higher values of the `min_samples_split`, so we can run another round of the optimisation with:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=50 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "qsub -P RDS-CORE-SIH4HPC-RW -l select=1:ncpus=4:mem=4gb -l walltime=2:0:0 -N gbm1 -v PARAM=55 /home/dvanichkina/scratch_sih/darya/gbm/optimise1.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then arrive at the following optimum:\n",
    "\n",
    "```\n",
    "0.9109466273291733 \t {'max_depth': 7, 'min_samples_split': 40}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the fit of our model, let's try `max_depth` of 6, 7, or 8 and `min_samples_split` of 35 to 45 (increments of 1).\n",
    "```\n",
    "param_test = {'min_samples_split': list(range(35,46,1)),\n",
    "              'max_depth': list(range(6,9, 1))}\n",
    "```\n",
    "\n",
    "We finally arrive at the optimum of:\n",
    "```\n",
    "0.9118552463171431 \t {'max_depth': 7, 'min_samples_split': 41}\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "Let's leave `max_depth` at 7, and test a range of values for:\n",
    "- `min_samples_split` (35 to 45, increments of 1)\n",
    "- `min_samples_leaf` (5 to 45, increments of 5)\n",
    "\n",
    "We can use the HPC, as above, to run the code, or run it locally on our machine (takes ~20 mins):\n",
    "\n",
    "```\n",
    "param_test = {'min_samples_split': list(range(35,46,1)),\n",
    "              'min_samples_leaf': list(range(5,36, 5))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       learning_rate=0.1, \n",
    "       n_estimators = 279,\n",
    "       max_depth=7, \n",
    "       max_features='sqrt', \n",
    "       subsample=0.8,   \n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "```\n",
    "\n",
    "\n",
    "The outcome of this is\n",
    "\n",
    "```\n",
    "0.9151989049335256 \t {'min_samples_leaf': 5, 'min_samples_split': 36}\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "Next, let's tune the `max_features` parameter, trying values from 'auto' to 'log2' to 'sqrt' to only 20 parameters. \n",
    "\n",
    "```\n",
    "param_test = {'max_features': ['auto','log2','sqrt'],\n",
    "              'min_samples_leaf': list(range(5,21, 5))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       min_samples_split = 36,\n",
    "       learning_rate=0.1, \n",
    "       n_estimators = 279,\n",
    "       max_depth=7,  \n",
    "       subsample=0.8,   \n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "```\n",
    "\n",
    "The resulf of this is:\n",
    "\n",
    "`0.9151989049335256 \t {'min_samples_leaf': 5, 'max_features': 'sqrt'}`\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's go back to optimising the number of trees and learning rate, this time going with a lower learning rate (because the lower the learning rate, the slower the CV will be):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "param_test = {'n_estimators': list(range(275, len(predictors)+1, 1)),\n",
    "             'learning_rate': list(np.arange(0.001, 1.0, 0.1)),\n",
    "             'subsample': list(np.arange(0.6, 1.0))}\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       max_depth=7, \n",
    "       max_features='sqrt', \n",
    "       min_samples_leaf=5,\n",
    "       min_samples_split = 36,\n",
    "       random_state=42),\n",
    "   param_grid=param_test, \n",
    "   n_jobs=4, \n",
    "   iid=False, \n",
    "   cv=10)\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "print(gbm2.best_score_,\"\\t\",gbm2.best_params_)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0.9117722351540429 \t {'learning_rate': 0.101, 'n_estimators': 280, 'subsample': 0.6}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also have \"brute forced\" the estimation using HPC, combining 3 scripts:\n",
    "\n",
    "```\n",
    "# optimise2.py ----------------\n",
    "#\n",
    "#\n",
    "# Import libraries:\n",
    "import os\n",
    "os.chdir('/scratch/RDS-CORE-SIH4HPC-RW/darya/gbm')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # GBM algorithm\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GBM model fitting')\n",
    "parser.add_argument('min_samples_split', type=int, help='min_samples_split value to test')\n",
    "parser.add_argument('n_estimators', type=int, help='n_estimators value to test')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "# [args.min_samples_split]\n",
    "\n",
    "param_test = {'n_estimators': [args.n_estimators], \n",
    "            'min_samples_split' : [args.min_samples_split], \n",
    "             'learning_rate': list(np.arange(0.001, 0.31, 0.05).round(2)), \n",
    "             'min_samples_leaf':  list(range(1,22, 4)),\n",
    "             'max_depth': [1,3,5,6,7,8,9,10,15],\n",
    "             'subsample': [0.65, 0.75, 0.8, 0.85, 0.9]\n",
    "             }\n",
    "\n",
    "gbm2 = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "       loss='ls',\n",
    "       max_features='sqrt',\n",
    "       random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "\n",
    "\n",
    "gbm2.fit(ames_train_X, ames_train_y)\n",
    "\n",
    "filename = 'fit1/' + str(args.min_samples_split) + \"_\" + str(args.n_estimators) + 'fit.tsv'\n",
    "with open(filename, 'w') as f:\n",
    "    print(gbm2.best_score_,\"\\t\",gbm2.best_params_, file = f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "####----\n",
    "optimise2.pbs\n",
    "#PBS -P PROJECTNAME\n",
    "#PBS -N gbmOpt3\n",
    "#PBS -l select=1:ncpus=3:mem=2gb\n",
    "#PBS -l walltime=02:0:00\n",
    "#PBS -J 100-286:5\n",
    "\n",
    "# unload all modules\n",
    "module purge\n",
    "\n",
    "cd /home/dvanichkina/scratch_sih/darya/gbm/\n",
    "python /home/dvanichkina/scratch_sih/darya/gbm/optimise2.py $PARAM $PBS_ARRAY_INDEX\n",
    "\n",
    "\n",
    "\n",
    "#### --------------\n",
    "# optimise2.sh\n",
    "qsub -v PARAM=5 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=10 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=15 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=20 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=25 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=30 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=35 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=40 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "qsub -v PARAM=45 /home/dvanichkina/scratch_sih/darya/gbm/optimise2.pbs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this finishes running, the optimal parameter values we obtain are:\n",
    "```\n",
    "0.918410561940631 \t {'learning_rate': 0.051, 'n_estimators': 280, 'min_samples_split': 35, 'max_depth': 7, 'subsample': 0.8, 'min_samples_leaf': 1}\n",
    "```\n",
    "\n",
    "Let's fit this model locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 280}\n",
      "0.9182771422480472\n"
     ]
    }
   ],
   "source": [
    "param_test = {'n_estimators': [280]}\n",
    "\n",
    "ames_gbm = GridSearchCV(estimator=GradientBoostingRegressor(\n",
    "        loss='ls',\n",
    "        learning_rate=0.051, \n",
    "        min_samples_split=35, \n",
    "        min_samples_leaf=1,\n",
    "        max_depth=7, \n",
    "        max_features='sqrt', \n",
    "        subsample=0.8,   \n",
    "        random_state=42),\n",
    "    param_grid=param_test, \n",
    "    n_jobs=4, \n",
    "    iid=False,\n",
    "    cv=10)\n",
    "\n",
    "#ames_gbm.fit(ames_train_X, ames_train_y)\n",
    "#pickle.dump(ames_gbm, open('models/ames_gbm.pickle', 'wb'))\n",
    "\n",
    "with open('models/ames_gbm.pickle', 'rb') as f:\n",
    "    ames_gbm = pickle.load(f)\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(ames_gbm.best_params_)\n",
    "print(ames_gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## XGBoost\n",
    "\n",
    "\n",
    "1. First, optimise `max_depth` and `min_child_weight`, with a varying number of `n_estimators` and a fixed, large `learning_rate` 0.1.\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation, metrics  # Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "# load from previous lessons\n",
    "cached_files = ['models/ames_train_y.pickle','models/ames_test_y.pickle',\n",
    "                'models/ames_train_X.pickle','models/ames_test_X.pickle',\n",
    "                'models/predictors.pickle']\n",
    "\n",
    "for file in cached_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        objectname = file.replace('models/', '').replace('.pickle', '')\n",
    "        exec(objectname + \" = pickle.load(f)\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param_test = {\n",
    " 'max_depth':list(range(3,10,2)),\n",
    " 'min_child_weight':list(range(1,7,2)),\n",
    " 'n_estimators': [180, 230, 280]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "xgbmodel = GridSearchCV(estimator=xgb.XGBRegressor(\n",
    "  objective='reg:linear',\n",
    "  learning_rate =0.1,\n",
    "  n_estimators=280,\n",
    "  gamma=0,\n",
    "  subsample=0.8,\n",
    "  colsample_bytree=0.8,\n",
    "  random_state=42),\n",
    "   param_grid=param_test,\n",
    "   n_jobs=4,\n",
    "   iid=False,\n",
    "   cv=10)\n",
    "   \n",
    "xgbmodel.fit(ames_train_X, ames_train_y)\n",
    "print(xgbmodel.best_score_,\"\\t\",xgbmodel.best_params_)\n",
    "\n",
    "# 0.9159701741062035 \t {'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 280}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, it turns out the best parameter values are:\n",
    "- 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 280\n",
    "\n",
    "Now, let's see if we can improve this even more:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[0.5,1,1.5,2],\n",
    " 'n_estimators': [275, 280, 285]\n",
    "}\n",
    "\n",
    "# 0.9162060722720288 \t {'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 285}\n",
    "\n",
    "```\n",
    "\n",
    "2. Next, optimise gamma, the minimum split loss. This is the minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285],\n",
    " 'gamma': [i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "# 0.9162060722720288 \t {'gamma': 0.0, 'min_child_weight': 0.5, 'max_depth': 3, 'n_estimators': 285}\n",
    "# Default zero seems best\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "3. Next, optimise `subsample` and `colsample_bytree`, which represent the proportion of data used to make each tree and how many features can go into a tree at each branch\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# 0.916463171211643 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.6, 'subsample': 0.8, 'n_estimators': 284}\n",
    "\n",
    "```\n",
    "\n",
    "Let's zero in on that, and subsample values plus/minus 0.05 around the idenfied optima:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75, 0.775,  0.8, 0.825, 0.85],\n",
    " 'colsample_bytree':[0.5, 0.55, 0.6]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Perhaps need to reduce those two even more, as their values are the lowest that we're testing!\n",
    "\n",
    "```\n",
    "\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [283, 284, 285, 286],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.65, 0.7, 0.75, 0.8],\n",
    " 'colsample_bytree':[0.4,0.45, 0.5, 0.55]\n",
    "\n",
    "}\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "```\n",
    "\n",
    "\n",
    "4. Next, let's tune the regularisation parameters:\n",
    "\n",
    "```\n",
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [285],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75],\n",
    " 'colsample_bytree':[0.5],\n",
    " 'reg_alpha':[0, 0.1, 0.2, 0.5, 1],\n",
    " 'reg_lambda': [0, 0.1, 0.2, 0.5, 1]\n",
    "}\n",
    "\n",
    "# 0.918366305109824 \t {'max_depth': 3, 'reg_lambda': 1, 'reg_alpha': 0, 'gamma': 0, 'min_child_weight': 0.5, 'colsample_bytree': 0.5, 'subsample': 0.75, 'n_estimators': 285}\n",
    "\n",
    "# defaults we've been using so far: alpha = 0 , lambda = 1, are optimal for us (i.e. ridge regression-like)\n",
    "```\n",
    "\n",
    "\n",
    "Let's fit the final model, and assess its performance on the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.5, 'gamma': 0, 'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 285, 'subsample': 0.75}\n",
      "0.9138998720966125\n"
     ]
    }
   ],
   "source": [
    "param_test = {\n",
    " 'max_depth':[3],\n",
    " 'min_child_weight':[0.5],\n",
    " 'n_estimators': [285],\n",
    " 'gamma': [0],\n",
    " 'subsample':[0.75],\n",
    " 'colsample_bytree':[0.5]\n",
    "}\n",
    "\n",
    "if websiterendering:\n",
    "    with open('models/ames_xgb.pickle', 'rb') as f:\n",
    "        ames_xgb = pickle.load(f)\n",
    "else:\n",
    "    # STUDENTS: RUN THE LINE BELOW ONLY:\n",
    "    ames_xgb = GridSearchCV(estimator=xgb.XGBRegressor(\n",
    "    objective='reg:linear',\n",
    "    learning_rate =0.1,\n",
    "    random_state=42),\n",
    "    param_grid=param_test,\n",
    "    n_jobs=4,\n",
    "    iid=False,\n",
    "    cv=10)\n",
    "    ames_xgb.fit(ames_train_X, ames_train_y)\n",
    "    pickle.dump(ames_xgb, open('models/ames_xgb.pickle', 'wb'))\n",
    "    \n",
    "\n",
    "\n",
    "# print(gsearch1.grid_scores_)\n",
    "print(ames_xgb.best_params_)\n",
    "print(ames_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, even the code above runs out of RAM on my normal machine, so I optimised it all on Artemis HPC. I then ran code very similar to what we've used before to get the training and testing RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>11528.128</td>\n",
       "      <td>0.979</td>\n",
       "      <td>7274.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>13901.512</td>\n",
       "      <td>0.969</td>\n",
       "      <td>9546.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>15757.714</td>\n",
       "      <td>0.961</td>\n",
       "      <td>10931.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>16038.217</td>\n",
       "      <td>0.959</td>\n",
       "      <td>10884.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>16480.809</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11448.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>16497.181</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11462.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLSR</th>\n",
       "      <td>16524.567</td>\n",
       "      <td>0.957</td>\n",
       "      <td>11602.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>16752.591</td>\n",
       "      <td>0.955</td>\n",
       "      <td>11765.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENet</th>\n",
       "      <td>17041.271</td>\n",
       "      <td>0.954</td>\n",
       "      <td>11799.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARS</th>\n",
       "      <td>19172.923</td>\n",
       "      <td>0.942</td>\n",
       "      <td>13498.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>31651.974</td>\n",
       "      <td>0.841</td>\n",
       "      <td>20155.938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE     R2        MAE\n",
       "GB     11528.128  0.979   7274.574\n",
       "XGB    13901.512  0.969   9546.663\n",
       "OLS    15757.714  0.961  10931.668\n",
       "RF     16038.217  0.959  10884.483\n",
       "Lasso  16480.809  0.957  11448.829\n",
       "Ridge  16497.181  0.957  11462.724\n",
       "PLSR   16524.567  0.957  11602.496\n",
       "PCR    16752.591  0.955  11765.495\n",
       "ENet   17041.271  0.954  11799.798\n",
       "MARS   19172.923  0.942  13498.476\n",
       "kNN    31651.974  0.841  20155.938"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the RMSE on the training data?\n",
    "\n",
    "assess_model_fit(listOfModels=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],\n",
    "                listOfMethodNamesAsStrings=['OLS','Ridge', 'Lasso', 'ENet','PCR','PLSR','MARS', 'RF', 'kNN', 'GB', \"XGB\"],\n",
    "                datasetX=ames_train_X,\n",
    "                datasetY=ames_train_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>19131.898</td>\n",
       "      <td>0.938</td>\n",
       "      <td>11399.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>19699.189</td>\n",
       "      <td>0.934</td>\n",
       "      <td>12359.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENet</th>\n",
       "      <td>19801.125</td>\n",
       "      <td>0.933</td>\n",
       "      <td>13317.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>19864.493</td>\n",
       "      <td>0.933</td>\n",
       "      <td>13120.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>20024.975</td>\n",
       "      <td>0.932</td>\n",
       "      <td>13270.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLSR</th>\n",
       "      <td>20113.237</td>\n",
       "      <td>0.931</td>\n",
       "      <td>13372.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>20541.485</td>\n",
       "      <td>0.928</td>\n",
       "      <td>13346.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>20988.955</td>\n",
       "      <td>0.925</td>\n",
       "      <td>13957.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARS</th>\n",
       "      <td>23226.020</td>\n",
       "      <td>0.908</td>\n",
       "      <td>15355.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>27382.699</td>\n",
       "      <td>0.872</td>\n",
       "      <td>16858.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kNN</th>\n",
       "      <td>34498.941</td>\n",
       "      <td>0.797</td>\n",
       "      <td>22983.686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            RMSE     R2        MAE\n",
       "GB     19131.898  0.938  11399.676\n",
       "XGB    19699.189  0.934  12359.483\n",
       "ENet   19801.125  0.933  13317.465\n",
       "Lasso  19864.493  0.933  13120.146\n",
       "Ridge  20024.975  0.932  13270.709\n",
       "PLSR   20113.237  0.931  13372.746\n",
       "OLS    20541.485  0.928  13346.733\n",
       "PCR    20988.955  0.925  13957.143\n",
       "MARS   23226.020  0.908  15355.804\n",
       "RF     27382.699  0.872  16858.700\n",
       "kNN    34498.941  0.797  22983.686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the RMSE on the test data?\n",
    "\n",
    "assess_model_fit(listOfModels=[ames_ols_all, ames_ridge, ames_lasso, ames_enet, ames_pcr, ames_plsr, ames_mars, ames_RF, ames_knn, ames_gbm, ames_xgb],\n",
    "                listOfMethodNamesAsStrings=['OLS','Ridge', 'Lasso', 'ENet','PCR','PLSR','MARS', 'RF', 'kNN', 'GB', \"XGB\"],\n",
    "                datasetX=ames_test_X,\n",
    "                datasetY=ames_test_y).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
