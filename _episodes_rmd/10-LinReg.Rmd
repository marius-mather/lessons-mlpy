---
title: "Linear Regression. "
author: "Darya Vanichkina"
exercises: 0
keypoints: 
- Regression is the prediction of the value of a continuous variable based on one or more other continuous or categorical variables.
- Multiple types of regression can be implemented to fit the data
objectives: 
- Introduce several approaches to carrying out linear regression in R
- Introduce and explore the Ames Housing dataset
questions: 
- How do we predict one continuous variable based on others?
- What is the first step of any ML project (and often the most time consuming)?
source: Rmd
start: 0
teaching: 30
bibliography: references.bib
---


```{r loadLibraries}
# set knitr options
# opts_knit$set(warning = FALSE, message = FALSE)
  
library(tidyverse)
library(caret)
library(tidymodels)
library(AmesHousing)
old <- theme_set(theme_minimal())
rm(old)
```

```{r getData, echo = F}
ameshousing <- 
  AmesHousing::make_ames() 
# remove 5 observations
ameshousingFilt <- 
  ameshousing %>% 
  filter(Gr_Liv_Area <= 4000)

set.seed(42) # so we all get the same results
ames_split <- initial_split(ameshousingFilt, prop = 0.7, strata = "Sale_Price")
ameshousingFiltTrain <- training(ames_split)
ameshousingFiltTest <- testing(ames_split)
```


## Use the (new, tidy) recipes package for feature engineering

```{r featureEngineering}
engineer_features_allames <-
  recipe(Sale_Price ~ ., data = ameshousingFiltTrain) %>%
  step_log(Sale_Price) %>% # log(Sale_Price)
  step_other(all_nominal(), threshold = 0.1) %>%
  step_nzv(all_nominal()) %>% # removes variables that are highly sparse or unbalanced
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)


# Train a Data Recipe
engineer_features_allames_prep <- prep(engineer_features_allames, training = ameshousingFiltTrain, verbose = TRUE)
# get the transformed training dataset
ameshousingFiltTrain_engineered <- juice(engineer_features_allames_prep)
# apply the same transformation onto the test set (with the TRAINING parameters)
ameshousingFiltTest_engineered <- bake(engineer_features_allames_prep, new_data = ameshousingFiltTest)

# use caret to specify cross-validation
ames_resamplingCV <- trainControl(
  method = "repeatedcv", 
  number = 10, #  k = 10
  repeats = 5, # repeat 5 times,
  savePredictions = "final"
  )

saveRDS(ames_resamplingCV, "models/ames_resamplingCV.Rds")
saveRDS(ameshousingFiltTrain_engineered, "models/ameshousingFiltTrain_engineered.Rds")
saveRDS(ameshousingFiltTest_engineered, "models/ameshousingFiltTest_engineered.Rds")
saveRDS(ameshousingFiltTrain, "models/ameshousingFiltTrain.Rds")
saveRDS(ameshousingFiltTest, "models/ameshousingFiltTest.Rds")
```


> ## Challenge
>
> 1. Look at (and explore) the code for the recipe above. What are each of the steps doing?
> Use the ?help to assist you with this.
> 
> 2. 
> {: .source}
>
> > ## Solution
> > 
> > We will discuss this as a group.
> > 
> > {: .output}
> {: .solution}
{: .challenge}


## Fitting a model

There are currently 237 types of ML models than can be fit using the `caret` package. To see a list of these, and to explore what tuning parameters they might need, [see here](https://topepo.github.io/caret/available-models.html). You can also [define your own model](https://topepo.github.io/caret/available-models.html).




### Simple linear regression

```{r ames1Cv}
# predict sale price based on Gr_Liv_Area
set.seed(42)
ames_lm1 <- train(
  Sale_Price ~ Gr_Liv_Area,
  data = ameshousingFiltTrain_engineered,
  method = "lm",
  trControl = ames_resamplingCV)
summary(ames_lm1)
ames_lm1


basicLm <- lm(Sale_Price ~ Gr_Liv_Area, data = ameshousingFiltTrain_engineered)

# implement as function

myLinearRegression <- function(formula, df){
set.seed(42)
tmp <-  train(
  formula,
  data = df,
  method = "lm",
  trControl = ames_resamplingCV)
print(summary(tmp))
return(tmp)
}
ames_lm1 <- myLinearRegression(Sale_Price ~ Gr_Liv_Area, df = ameshousingFiltTrain_engineered)



```





Use Second_Flr_SF and Gr_Liv_Area to predict:

```{r ames2vars}
ames_lm2 <- myLinearRegression(Sale_Price ~ Gr_Liv_Area + Second_Flr_SF,  df = ameshousingFiltTrain_engineered)

```

```{r ames2varsinteraction}
ames_lm2i <-  myLinearRegression(Sale_Price ~ Gr_Liv_Area + Second_Flr_SF +Gr_Liv_Area:Second_Flr_SF, df = ameshousingFiltTrain_engineered)

```



```{r Withyear}
ames_lm2year <-  myLinearRegression(Sale_Price ~ Gr_Liv_Area + Year_Built, df = ameshousingFiltTrain_engineered)

```



All predictors

```{r amesallvars, warnings=F}
ames_lm_all <- myLinearRegression(Sale_Price ~. , df = ameshousingFiltTrain_engineered)
saveRDS(ames_lm_all, "models/ames_lm_all.Rds")
```

```{r amesallvarsF, warnings=F}
ameshousingFiltTrain_engineeredF <- ameshousingFiltTrain_engineered %>% select(-ends_with("other"))
ames_lm_all2 <- myLinearRegression(Sale_Price ~. , df = ameshousingFiltTrain_engineeredF)
rm(ameshousingFiltTrain_engineeredF)
```


Compare performance on training set:

```{r Resamples}
allResamples <- resamples(list("Gr_Liv_Area" = ames_lm1, 
                               "Gr_Liv_Area + Year_Built" = ames_lm2year,
                               "Gr_Liv_Area + Second_Flr_SF" = ames_lm2,
                               "Gr_Liv_Area:Second_Flr_SF" = ames_lm2i,
                               "All vars" = ames_lm_all,
                               "All vars f" = ames_lm_all2
                               ))
bwplot(allResamples)
parallelplot(allResamples)

parallelplot(allResamples , metric = "Rsquared")
parallelplot(allResamples , metric = "RMSE")

summary(allResamples)$statistics$RMSE %>% 
  as.data.frame() %>% 
  rownames_to_column()  %>%
  arrange(Median)
```









## References


  
  