---
title: "Classification"
author: "Madhura Killedar, Darya Vanichkina"
keypoints:
- Classification attempts to predict the class to which a particular observation belongs
- Scikit-learn has tons of classifier options
- There are many different metrics for assessing performance for a classification problem
- Which metric you choose and optimise for should be considered carefully, and will be different depending on the problem
- Exporatory data analysis is a time consuming but critical process that needs to be carried out prior to any modeling
- Support vector machines are a class of ML algorithms that construct a boundary in n-dimensional space to separate different classes
- They represent flexible methods that can handle a wide range of problems, including those impossible to address using conventional linear approaches
-  Different models had different best performance depending on the error metric we chose to evaluate


objectives: 
- Explain what a confusion matrix is, and how it relates to common classification error metrics
- Understand how missing data can be hidden within normal-looking datasets
- Use the scikit-learn library to work with data
- Learn how to build and evaluate classifiers
- Use the online documentation to figure out what the hyperparameters are for a specific method
- Build classification models using KNN, naive bayes, regularised and boosted logistic regression, decision trees and a random forest


questions: 
- What metrics are used to for evaluation in a classification problem?
- What dataset will we be working with today?
- What exploratory data analysis do we need to carry out on datasets we plan to work with for classification?
- How do we split our data into training and testing sets?
- How can we use scikit-learn to classify data in python?
- How can we find out which hyperparameters are best for classification?

source: ipynb
teaching: 150
start: 1
exercises: 120
---


## Classification

## What is a classifier?

A classifier is some kind of rule / black box / widget that you can feed a new observation/data/record and it will decide whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.

![A classifier for cats and not cats](../fig/50-CatNotCat.jpg)

You can have classifiers for anything you can have a yes/no answer to, e.g.

- Is this a cat? üê±
- Do these test results indicate cancer? üöë
- Is this email spam or not spam? üìß

You can also have classifiers that categorise things into multiple (more than two) categories e.g.

- Which animal is this, out of the 12 animals I have trained my model on? üê±
- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? üöë
- Is this email important, not important but not spam, or spam? üìß

It is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.

## Model evaluation (classification)

For now, let's imagine we have a classifier already. How can we test it to see how good it is?
A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.

![An demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/50-CatConfusion.jpg)

### Confusion Matrix

When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares "the truth" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and of course if we correctly label something which is not a cat, as not a cat, then this is a true negative.

### Some common metrics

![Error metrics](../fig/50-ErrorMetrics.png)

#### AUC: Area under the curve

A good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.

To capture this balance, we often use a Receiver Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.

![A Receiver Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/50-CatArea.jpg)


# Pima Indians Diabetes
Today, we are going to be working with the [Pima Indians Diabetes Dataset](). This is a classic dataset from the UCI machine learning repository, which is now hosted on kaggle. We have downloaded the .csv file of this dataset from kaggle. It contains the following variables:



1) Pregnancies - Number of times pregnant
2) Glucose - Plasma glucose concentration a 2 hours in an [oral glucose tolerance test](https://en.wikipedia.org/wiki/Glucose_tolerance_test)
3) BloodPressure - Diastolic blood pressure (mm Hg)
4) SkinThickness - Triceps skin fold thickness (mm) - [a measure correlated with body fat](https://en.wikipedia.org/wiki/Anthropometry_of_the_upper_arm)
5) Insulin - 2-Hour serum insulin (mu U/ml)
6) BMI - Body mass index (weight in kg/(height in m)^2)
7) DiabetesPedigreeFunction - Diabetes pedigree function
8) Age - Age (years)
9) Outcome - diabetes status (1 - diabetic; 0 - non-diabetic)


The diabetes pedigree function was developed by [Smith 1988](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/) to provide a synthesis ofthe diabetes mellitus history in relatives and the genetic relationship of those relatives to the subject. It uses information from parents, grandparents, siblings, aunts and uncles, and first cousin to provide a measure of the expected genetic influence of affected and unaffected relatives on the subject‚Äôs eventual diabetes risk:

![DiabetesPedigree](dpf.png)


- i - ranges across all relatives who HAD developed diabetes by subject's examination date
- j - ranges across all relatives who HAD NOT developed diabetes by subject's examination date
- Kx - percentage of genes shared by relative and subject. Equal to:
  - 0.5 when relative is parent or full sibling
  - 0.25 when relative is half-sibling, grandparent, aunt or uncle
  - 0.125 when relative is half aunt, half uncle or first cousin
- ADMi - age when diabetes was diagnosed
- ALC - age of relative when last "non-diabetic" assessment was made
- 88 and 14 - constants - maximum and minimum age at which patients in study were diagnosed with diabetes
- Constants 20 and 50 chosen so that:
  - A subject with no relatives has DPF slighly lower than average
  - DPF value decreases as young relatives free of diabetes join the database
  - DPF increases quickly as known relatives develop diabetes
  
DPF increases as:

- the number of relatives with diabetes increases
- the age at which those relatives develop diabetes decreases
- percentage of genes these relatives share with subject increase

DPF decreases as:

- the number of relatives who never develop diabetes increases
- their ages at last examination increase
- percentage of genes these relatives share with subject increase


## Let's Explore our data


```python
websiterendering = True
import numpy as np
import pandas as pd

import statsmodels.api as sm

# plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# sklearn libraries
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, accuracy_score, recall_score, precision_score 
from sklearn.model_selection import GridSearchCV
from sklearn.utils.multiclass import unique_labels
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier

%matplotlib inline
sns.set(font_scale = 1.5)
```

### Load Data


```python
diabetes = pd.read_csv('../data/diabetes.csv') # read csv
```

Explore the variables:


```python
print(diabetes.columns)
```

    Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
          dtype='object')


Recode Outcome into integers


```python
print(diabetes.info())
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 768 entries, 0 to 767
    Data columns (total 9 columns):
     #   Column                    Non-Null Count  Dtype  
    ---  ------                    --------------  -----  
     0   Pregnancies               768 non-null    int64  
     1   Glucose                   768 non-null    int64  
     2   BloodPressure             768 non-null    int64  
     3   SkinThickness             768 non-null    int64  
     4   Insulin                   768 non-null    int64  
     5   BMI                       768 non-null    float64
     6   DiabetesPedigreeFunction  768 non-null    float64
     7   Age                       768 non-null    int64  
     8   Outcome                   768 non-null    int64  
    dtypes: float64(2), int64(7)
    memory usage: 54.1 KB
    None


### Summary info
Shape of data frame


```python
diabetes.shape
```




    (768, 9)



Look for missing data:


```python
diabetes.count()
```




    Pregnancies                 768
    Glucose                     768
    BloodPressure               768
    SkinThickness               768
    Insulin                     768
    BMI                         768
    DiabetesPedigreeFunction    768
    Age                         768
    Outcome                     768
    dtype: int64



It seems like there is no missing data.
Get a summary of the data frame:


```python
diabetes.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.845052</td>
      <td>120.894531</td>
      <td>69.105469</td>
      <td>20.536458</td>
      <td>79.799479</td>
      <td>31.992578</td>
      <td>0.471876</td>
      <td>33.240885</td>
      <td>0.348958</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.369578</td>
      <td>31.972618</td>
      <td>19.355807</td>
      <td>15.952218</td>
      <td>115.244002</td>
      <td>7.884160</td>
      <td>0.331329</td>
      <td>11.760232</td>
      <td>0.476951</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.078000</td>
      <td>21.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>99.000000</td>
      <td>62.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>27.300000</td>
      <td>0.243750</td>
      <td>24.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>117.000000</td>
      <td>72.000000</td>
      <td>23.000000</td>
      <td>30.500000</td>
      <td>32.000000</td>
      <td>0.372500</td>
      <td>29.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.000000</td>
      <td>140.250000</td>
      <td>80.000000</td>
      <td>32.000000</td>
      <td>127.250000</td>
      <td>36.600000</td>
      <td>0.626250</td>
      <td>41.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>17.000000</td>
      <td>199.000000</td>
      <td>122.000000</td>
      <td>99.000000</td>
      <td>846.000000</td>
      <td>67.100000</td>
      <td>2.420000</td>
      <td>81.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



> ## Challenge
>
> Look at the output of summary above and the table that explains what each of the variables are. Do the 
> values for all 
> - (a) Pregnancies and Glucose
> - (b) Blood pressure and Skin thickness
> - (c) Insulin and DiabetesPedigreeFunction, and
> - (d) BMI and Age 
> make sense?
>
> If not, how do you think we should deal with them? 
> Can you hypothesise what the consequences of this approach would be?
>
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> >
> > # possibly missing
> > diabetes[diabetes == 0].count()
> > 
> > # not missing
> > diabetes[diabetes != 0].count()
> > 
> > ~~~
> > 
> > It is clear that the values of several variables are zero when it is impossible for them to be so (i.e. this value could not be zero if it was measured). Hence, we are dealing with "hidden" missing data, and should recode it as NA.
> > 
> > The following variables have zero "values" that are actually likely to be missing:
> > 
> > 1. Glucose (a)
> > 2. BloodPressure (b)
> > 3. SkinThickness (b)
> > 4. Insulin (c)
> > 5. BMI (d)
> > 
> > {: .output}
> {: .solution}
{: .challenge}

### Let‚Äôs use visualisation to further explore the dataset:


```python
_ = sns.countplot(x="Pregnancies",
                hue="Outcome", 
                data=diabetes)
```


![png](../fig/50-Classification_19_0.png)



```python
diabetes['Outcome'].value_counts()
```




    0    500
    1    268
    Name: Outcome, dtype: int64



If we wanted to look at all possible scatterplot pairs we would do something like:


```python
numVars = diabetes.select_dtypes(exclude = ['object']).columns

_ = sns.pairplot(data=diabetes,
                 vars=numVars,
                 hue='Outcome',
                 palette={0:'k',1:'r'},
                 # use kernel density estimates for univariate plots
                 diag_kind='kde',
                 # make the shape of points circular and diamond, respectively
                 markers=["o", "d"])
```

    /opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:288: UserWarning: Data must have variance to compute a kernel density estimate.
      warnings.warn(msg, UserWarning)
    /opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:288: UserWarning: Data must have variance to compute a kernel density estimate.
      warnings.warn(msg, UserWarning)



![png](../fig/50-Classification_22_1.png)


Generate a boxplot by possible prediction variables. Which do you hypothesize would me it easiest for us to separate the data?


```python
# copy the original dataframe
diabetes2 = diabetes.copy()
# create a new patient id column
diabetes2['PatientID'] = range(1, len(diabetes2) + 1)
# melt that dataframe
diabetes2 = diabetes2.melt(id_vars=['PatientID','Outcome'])

grid = sns.axisgrid.FacetGrid(diabetes2[diabetes2.variable.isin(numVars[:5])], 
                              col='variable', 
                              # y axis scale different for each boxplot
                              sharey=False)
grid.map(sns.boxplot, 'Outcome', 'value', order=[0, 1]);
```


![png](../fig/50-Classification_24_0.png)



```python
grid = sns.axisgrid.FacetGrid(diabetes2[diabetes2.variable.isin(numVars[5:])], 
                              col='variable', 
                               # y axis scale different for each boxplot
                              sharey=False)
grid.map(sns.boxplot, 'Outcome','value', order=[0, 1]);
```


![png](../fig/50-Classification_25_0.png)


Make a correlation plot betwee all numeric variables


```python
corr = diabetes.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Generate a custom diverging colormap
# hue_negative, hue_positive
cmap = sns.diverging_palette(10,260, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.set(rc={'figure.figsize':(12,8)})
sns.set(font_scale = 1.2)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0., square=True, linewidths=.5);
```


![png](../fig/50-Classification_27_0.png)


## Prepare Data

Let‚Äôs replace the missing values in the diabetes data frame with NaN.


```python
# get the column names
diabetes.columns
```




    Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
          dtype='object')




```python
replaceVars = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']
```


```python
# mark zero values as missing or NaN
diabetes[replaceVars] = diabetes[replaceVars].replace(0, np.NaN)
```


```python
print(diabetes.isnull().sum())
```

    Pregnancies                   0
    Glucose                       5
    BloodPressure                35
    SkinThickness               227
    Insulin                     374
    BMI                          11
    DiabetesPedigreeFunction      0
    Age                           0
    Outcome                       0
    dtype: int64


Let‚Äôs generate a newcorrelation plot where the missing data has been properly recoded as NaN. Which correlations change?


```python
corr = diabetes.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Generate a custom diverging colormap
# hue_negative, hue_positive
cmap = sns.diverging_palette(10,260, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.set(rc={'figure.figsize':(12,8)})
sns.set(font_scale = 1.2)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0., square=True, linewidths=.5);
```


![png](../fig/50-Classification_34_0.png)


***
## Aim

To create a classifier for predicting whether a person has diabetes or not.

## Train-Test split

We‚Äôre going to split our data into 70% training and 30% testing sets.


```python
features_train, features_test, outcome_train, outcome_test = train_test_split(
    diabetes[diabetes.columns.difference(['Outcome'])],
    diabetes['Outcome'], 
    train_size=0.7, 
    test_size=0.3, 
    random_state = 42, 
    stratify = diabetes['Outcome']
)
```

How many examples do we have in the training and testing sets?


```python
features_train.shape
```




    (537, 8)




```python
features_test.shape
```




    (231, 8)



## Impute missing values using median values


```python
## Impute missing information
imp_median = SimpleImputer(strategy='median') 
imp_median.fit(features_train)
features_train_imp = pd.DataFrame(imp_median.transform(features_train))
features_test_imp = pd.DataFrame(imp_median.transform(features_test))
features_train_imp.columns = features_train.columns
features_test_imp.columns = features_test.columns
```

Confirm that we have imputed the values for BOTH training and testing datasets using the TRAINING data median!


```python
features_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>209</th>
      <td>41</td>
      <td>35.5</td>
      <td>84.0</td>
      <td>0.355</td>
      <td>184.0</td>
      <td>NaN</td>
      <td>7</td>
      <td>33.0</td>
    </tr>
    <tr>
      <th>176</th>
      <td>42</td>
      <td>31.2</td>
      <td>78.0</td>
      <td>0.382</td>
      <td>85.0</td>
      <td>NaN</td>
      <td>6</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>147</th>
      <td>34</td>
      <td>30.5</td>
      <td>64.0</td>
      <td>1.400</td>
      <td>106.0</td>
      <td>119.0</td>
      <td>2</td>
      <td>35.0</td>
    </tr>
    <tr>
      <th>454</th>
      <td>24</td>
      <td>37.8</td>
      <td>54.0</td>
      <td>0.498</td>
      <td>100.0</td>
      <td>105.0</td>
      <td>2</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>636</th>
      <td>48</td>
      <td>28.8</td>
      <td>74.0</td>
      <td>0.153</td>
      <td>104.0</td>
      <td>NaN</td>
      <td>5</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_train_imp.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>41.0</td>
      <td>35.5</td>
      <td>84.0</td>
      <td>0.355</td>
      <td>184.0</td>
      <td>126.0</td>
      <td>7.0</td>
      <td>33.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>42.0</td>
      <td>31.2</td>
      <td>78.0</td>
      <td>0.382</td>
      <td>85.0</td>
      <td>126.0</td>
      <td>6.0</td>
      <td>29.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>34.0</td>
      <td>30.5</td>
      <td>64.0</td>
      <td>1.400</td>
      <td>106.0</td>
      <td>119.0</td>
      <td>2.0</td>
      <td>35.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24.0</td>
      <td>37.8</td>
      <td>54.0</td>
      <td>0.498</td>
      <td>100.0</td>
      <td>105.0</td>
      <td>2.0</td>
      <td>28.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>48.0</td>
      <td>28.8</td>
      <td>74.0</td>
      <td>0.153</td>
      <td>104.0</td>
      <td>126.0</td>
      <td>5.0</td>
      <td>29.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_test.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>94</th>
      <td>21</td>
      <td>24.7</td>
      <td>82.0</td>
      <td>0.761</td>
      <td>142.0</td>
      <td>64.0</td>
      <td>2</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>437</th>
      <td>28</td>
      <td>29.9</td>
      <td>75.0</td>
      <td>0.434</td>
      <td>147.0</td>
      <td>NaN</td>
      <td>5</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>86</th>
      <td>45</td>
      <td>36.6</td>
      <td>72.0</td>
      <td>0.178</td>
      <td>106.0</td>
      <td>NaN</td>
      <td>13</td>
      <td>54.0</td>
    </tr>
    <tr>
      <th>221</th>
      <td>66</td>
      <td>31.6</td>
      <td>90.0</td>
      <td>0.805</td>
      <td>158.0</td>
      <td>NaN</td>
      <td>2</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>19</th>
      <td>32</td>
      <td>34.6</td>
      <td>70.0</td>
      <td>0.529</td>
      <td>115.0</td>
      <td>96.0</td>
      <td>1</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_test_imp.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>226</th>
      <td>21.0</td>
      <td>24.7</td>
      <td>82.0</td>
      <td>0.761</td>
      <td>142.0</td>
      <td>64.0</td>
      <td>2.0</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>227</th>
      <td>28.0</td>
      <td>29.9</td>
      <td>75.0</td>
      <td>0.434</td>
      <td>147.0</td>
      <td>126.0</td>
      <td>5.0</td>
      <td>29.0</td>
    </tr>
    <tr>
      <th>228</th>
      <td>45.0</td>
      <td>36.6</td>
      <td>72.0</td>
      <td>0.178</td>
      <td>106.0</td>
      <td>126.0</td>
      <td>13.0</td>
      <td>54.0</td>
    </tr>
    <tr>
      <th>229</th>
      <td>66.0</td>
      <td>31.6</td>
      <td>90.0</td>
      <td>0.805</td>
      <td>158.0</td>
      <td>126.0</td>
      <td>2.0</td>
      <td>29.0</td>
    </tr>
    <tr>
      <th>230</th>
      <td>32.0</td>
      <td>34.6</td>
      <td>70.0</td>
      <td>0.529</td>
      <td>115.0</td>
      <td>96.0</td>
      <td>1.0</td>
      <td>30.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_train.median()
```




    Age                          30.000
    BMI                          32.400
    BloodPressure                72.000
    DiabetesPedigreeFunction      0.385
    Glucose                     117.000
    Insulin                     126.000
    Pregnancies                   3.000
    SkinThickness                29.000
    dtype: float64




```python
features_test.median()
```




    Age                          28.000
    BMI                          31.600
    BloodPressure                72.000
    DiabetesPedigreeFunction      0.342
    Glucose                     118.000
    Insulin                     120.000
    Pregnancies                   3.000
    SkinThickness                30.000
    dtype: float64



### Standardize data ranges


```python
StSc = StandardScaler()
StSc.fit(features_train_imp)
features_train_sc = StSc.transform(features_train_imp)
features_test_sc  = StSc.transform(features_test_imp)
```

### Check outcome


```python
# 0 is normal 
# 1 is diabetes
print((outcome_test == 0).sum())
print((outcome_test != 0).sum())

# 
print((outcome_train == 0).sum())
print((outcome_train != 0).sum())

# is our train/test balanced?
print((outcome_test != 0).sum() / (outcome_test == 0).sum())
print((outcome_train != 0).sum() / (outcome_train == 0).sum())
```

    150
    81
    350
    187
    0.54
    0.5342857142857143


# Classifiers

## k-Nearest Neighbours Classifier

This takes the nearest k things and and says what is the majority vote? E.g. in the example below we look at the seven nearest neighbours, 4 of which are cats so we say that the new example is probably a cat as well.

![A way to classify a new example as a cat or not...take the average of the nearest k=7 examples. It's a cat!](../fig/50-CatKNN.jpg)


### Let's Classify!

Train KNN classifier.

Link to [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) for students, if you'd like to look at the formulas to understand how one function can generalise two distance metrics.


```python
# the default settings are metric='minkowski' and p = 2
# , which is the same as the standard Euclidean metric
# p = 1 gets us manhattan distance

cf_knn= KNeighborsClassifier(metric='minkowski')

# use GridSeachCV to test how many neighbors are optimal
cf_knn_gscv = GridSearchCV(cf_knn, 
                           # test from 1 to 50 neighbors 
                           {'n_neighbors': np.arange(1, 51)},
                           # use 5xfold cross-validation
                           cv=5,
                           # use f1 as error metric
                           scoring = 'f1',
                           return_train_score=True)

fit_knn = cf_knn_gscv.fit(features_train_sc, outcome_train)
```

Explore the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for other error metrics we could have used.


```python
# Look at the cross-validation results
knn_results = pd.DataFrame.from_dict(fit_knn.cv_results_)
knn_results.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean_fit_time</th>
      <th>std_fit_time</th>
      <th>mean_score_time</th>
      <th>std_score_time</th>
      <th>param_n_neighbors</th>
      <th>params</th>
      <th>split0_test_score</th>
      <th>split1_test_score</th>
      <th>split2_test_score</th>
      <th>split3_test_score</th>
      <th>...</th>
      <th>mean_test_score</th>
      <th>std_test_score</th>
      <th>rank_test_score</th>
      <th>split0_train_score</th>
      <th>split1_train_score</th>
      <th>split2_train_score</th>
      <th>split3_train_score</th>
      <th>split4_train_score</th>
      <th>mean_train_score</th>
      <th>std_train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000777</td>
      <td>0.000132</td>
      <td>0.003920</td>
      <td>0.000191</td>
      <td>1</td>
      <td>{'n_neighbors': 1}</td>
      <td>0.571429</td>
      <td>0.640000</td>
      <td>0.623377</td>
      <td>0.500000</td>
      <td>...</td>
      <td>0.556513</td>
      <td>0.073053</td>
      <td>40</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000601</td>
      <td>0.000023</td>
      <td>0.004057</td>
      <td>0.000054</td>
      <td>2</td>
      <td>{'n_neighbors': 2}</td>
      <td>0.550725</td>
      <td>0.474576</td>
      <td>0.557377</td>
      <td>0.436364</td>
      <td>...</td>
      <td>0.466031</td>
      <td>0.089967</td>
      <td>50</td>
      <td>0.692982</td>
      <td>0.747899</td>
      <td>0.717949</td>
      <td>0.739496</td>
      <td>0.744770</td>
      <td>0.728619</td>
      <td>0.020663</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000567</td>
      <td>0.000021</td>
      <td>0.003661</td>
      <td>0.000254</td>
      <td>3</td>
      <td>{'n_neighbors': 3}</td>
      <td>0.627907</td>
      <td>0.641026</td>
      <td>0.640000</td>
      <td>0.606061</td>
      <td>...</td>
      <td>0.616332</td>
      <td>0.027843</td>
      <td>3</td>
      <td>0.793220</td>
      <td>0.831081</td>
      <td>0.784566</td>
      <td>0.800000</td>
      <td>0.804054</td>
      <td>0.802584</td>
      <td>0.015705</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000583</td>
      <td>0.000043</td>
      <td>0.003551</td>
      <td>0.000060</td>
      <td>4</td>
      <td>{'n_neighbors': 4}</td>
      <td>0.563380</td>
      <td>0.523077</td>
      <td>0.593750</td>
      <td>0.456140</td>
      <td>...</td>
      <td>0.534412</td>
      <td>0.046080</td>
      <td>49</td>
      <td>0.688259</td>
      <td>0.711462</td>
      <td>0.671875</td>
      <td>0.692015</td>
      <td>0.734375</td>
      <td>0.699597</td>
      <td>0.021470</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000555</td>
      <td>0.000004</td>
      <td>0.003617</td>
      <td>0.000069</td>
      <td>5</td>
      <td>{'n_neighbors': 5}</td>
      <td>0.617284</td>
      <td>0.619718</td>
      <td>0.605263</td>
      <td>0.558824</td>
      <td>...</td>
      <td>0.591693</td>
      <td>0.027866</td>
      <td>12</td>
      <td>0.744681</td>
      <td>0.759582</td>
      <td>0.724138</td>
      <td>0.760000</td>
      <td>0.765517</td>
      <td>0.750784</td>
      <td>0.015011</td>
    </tr>
  </tbody>
</table>
<p>5 rows √ó 21 columns</p>
</div>




```python
print(fit_knn.best_estimator_.n_neighbors)
```

    9



```python
split_cols = knn_results.filter(regex='split.*test_score').columns
knn_cv_scores = knn_results.melt(id_vars='param_n_neighbors', 
                                  value_vars=split_cols)

sns.pointplot(x="param_n_neighbors",
              y="value",
              data = knn_cv_scores)

best_n = fit_knn.best_estimator_.n_neighbors
plt.scatter(x = best_n - 1,
            y = knn_cv_scores.loc[knn_cv_scores['param_n_neighbors'] == best_n, 'value'].mean(),
            color = 'r',
            # plots point on top of other plot
            zorder = 10);
```


![png](../fig/50-Classification_61_0.png)


> ## Challenge
>
> Do you think this is the optimal solution?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > Probably not, since the performance of the classifier after a certain number of neighbors (~9 - 30) doesn't really seem to improve that much
> > 
> > {: .output}
> {: .solution}
{: .challenge}


```python
# Use trained classifier to predict outcome for training and test
knn_outcome_pred_class_train = fit_knn.predict(features_train_sc)
knn_outcome_pred_class_test = fit_knn.predict(features_test_sc)


# get probabilities out
def model_probabilities(model = fit_knn, dataset = features_train_sc):
    probs1 = model.predict_proba(dataset)
    probs2 = [p[1] for p in probs1] 
    # hopefully close to 1 for true 1's
    return(probs2)

knn_outcome_pred_prob_train = model_probabilities(model = fit_knn, dataset = features_train_sc)
knn_outcome_pred_prob_test = model_probabilities(model = fit_knn, dataset = features_test_sc)
```

### Classifier Diagnostics/evaluation
So how well did the classifier do? Let's define a function to generate a confusion matrix:


```python
# define custom confusion matrix function
def confmatrix(truth = outcome_train, 
               prediction = knn_outcome_pred_class_train):
    df = pd.DataFrame(confusion_matrix(truth, prediction))
    #Total sum per row: 
    df.loc['Predictions',:]= df.sum(axis=0)
    #Total sum per column: 
    df.loc[:,'Truth'] = df.sum(axis=1)
    return(df)
```

Assess performance on the training set:


```python
confmatrix(truth = outcome_train, prediction = knn_outcome_pred_class_train)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>307.0</td>
      <td>43.0</td>
      <td>350.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>61.0</td>
      <td>126.0</td>
      <td>187.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>368.0</td>
      <td>169.0</td>
      <td>537.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
confmatrix(truth = outcome_test, prediction = knn_outcome_pred_class_test)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>127.0</td>
      <td>23.0</td>
      <td>150.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.0</td>
      <td>45.0</td>
      <td>81.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>163.0</td>
      <td>68.0</td>
      <td>231.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# define a function to plot an ROC curve
def plot_ROC(truth = outcome_train, prediction1 = knn_outcome_pred_prob_train, 
             col = 'b'):
    fpr, tpr, _ = roc_curve(truth, prediction1)
    AUC = roc_auc_score(truth, prediction1)
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label='AUC = %4.2f'%AUC, color = col)
    _ = plt.legend(loc="lower right")


# plot the roc curves
# training in blue
plot_ROC(truth = outcome_train, prediction1 = knn_outcome_pred_prob_train, col = 'b')
# test in red
plot_ROC(truth = outcome_test, prediction1 = knn_outcome_pred_prob_test, col = 'r')
```


![png](../fig/50-Classification_69_0.png)


> ## Challenge
>
> How well do you think this model generalised?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > While the overall performance of the model is not very good, it does generalise well, as the difference between the 
> > 
> > 
> > {: .output}
> {: .solution}
{: .challenge}

## Naive Bayes Classifier

A Na√Øve Bayes classifier assumes that each of your columns are independent (uncorrelated with each other). It works out a probability that your example is a cat by counting the fraction of cats that had that value in each column, multiplying the values together and then multiplying again by what fraction of your training examples were cats. This is just writing out bayes rule of conditional probability and simplifying it for independent columns.

$$p(\text{Cat}| x_\text{new})=\frac{p(\text{Cat})p(x_\text{new}|\text{Cat})}{p(\text{Cat})p(x_\text{new}|\text{Cat})+p(\text{Not Cat})p(x_\text{new}|\text{Not Cat})}$$

In practice your columns are probably not independent, but we still use it anyway and it's usually ok, providing we only care about the label and not the probability it spits out.  

Continuous variables have to be somehow turned into discrete variables before you can use this technique, but most algorithms do this for you automatically.


### Let's Classify!
Train Naive Bayes classifier


```python
from sklearn.naive_bayes import GaussianNB
cf_gnb = GaussianNB()


cf_gnb_gscv = GridSearchCV(cf_gnb, 
                           {'var_smoothing' : [1e-09]},
                           # use 5xfold cross-validation
                           cv=5,
                           # use f1 as error metric
                           scoring = 'f1')

fit_gnb = cf_gnb_gscv.fit(features_train_sc, outcome_train)
```

Use trained classifier to predict outcome for test-set


```python
gnb_train_pred_class = cf_gnb_gscv.predict(features_train_sc)
gnb_test_pred_class = cf_gnb_gscv.predict(features_test_sc)

gnb_pred_prob_train = model_probabilities(model = cf_gnb_gscv, dataset=features_train_sc)
gnb_pred_prob_test = model_probabilities(model = cf_gnb_gscv, dataset=features_test_sc)
```

### Classifier Diagnostics
So how well did it go?


```python
# print matrix for predictions on training and testing
print("Training set")
confmatrix(truth = outcome_train, prediction = gnb_train_pred_class)
```

    Training set





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>297.0</td>
      <td>53.0</td>
      <td>350.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>77.0</td>
      <td>110.0</td>
      <td>187.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>374.0</td>
      <td>163.0</td>
      <td>537.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
print("\nTesting set")
confmatrix(truth = outcome_test, prediction = gnb_test_pred_class)
```

    
    Testing set





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>121.0</td>
      <td>29.0</td>
      <td>150.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>34.0</td>
      <td>47.0</td>
      <td>81.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>155.0</td>
      <td>76.0</td>
      <td>231.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = gnb_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = gnb_pred_prob_test, col = 'r')
```


![png](../fig/50-Classification_79_0.png)


## Regularized Logistic Classifier

This fits a logistic regression to the probability of receiving a class label of 1 or 0. Regularisation (hopefully) stops it from overfitting. 

### Let's Classify!
Train Regularized Logistic classifier. 

There are several algorithms to do this accessible in scikit-learn, and we will use the one that uses the saga solver. 


```python
np.linspace(start=0, stop=1, num=8)
```




    array([0.        , 0.14285714, 0.28571429, 0.42857143, 0.57142857,
           0.71428571, 0.85714286, 1.        ])




```python
from sklearn.linear_model import LogisticRegression
# class_weight balanced will use the values of y to automatically adjust weights inversely 
# proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)
# multi_class='ovr' - a binary problem is fit for each label
cf_rlc = LogisticRegression(random_state=42, solver='saga',
                            multi_class='ovr')

# test different penalty and class weight parameters - define this as a dictionary to make subsequent plotting easier
rlc_dict = {'penalty': ['l1', 'l2'],
            # for proper ENET
            # 'penalty': ['elasticnet'], 
            'class_weight': [None, 'balanced']}
            # l1_ratio with saga implements a proper EN, but doesn't work ATM b/c pf a l1_ratio not a recognised parameter issue
            # 'l1_ratio': np.linspace(start=0.01, stop=0.99, num=8)}

# use GridSeachCV to which form of regularisation is optimal
cf_rlc_gscv = GridSearchCV(cf_rlc,
                           rlc_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')
# fit model on training data
fit_rlc = cf_rlc_gscv.fit(features_train_sc, outcome_train)
```


```python
# what is the best estimator?
fit_rlc.best_estimator_
```




    LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                       fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                       max_iter=100, multi_class='ovr', n_jobs=None, penalty='l2',
                       random_state=42, solver='saga', tol=0.0001, verbose=0,
                       warm_start=False)



Use trained classifier to predict outcome for training and test sets


```python
rlc_train_pred_class = fit_rlc.predict(features_train_sc)
rlc_test_pred_class = fit_rlc.predict(features_test_sc)

rlc_pred_prob_train = model_probabilities(model = cf_rlc_gscv, dataset=features_train_sc)
rlc_pred_prob_test = model_probabilities(model = cf_rlc_gscv, dataset=features_test_sc)
```

### Classifier evaluation
So how well did the classifier do?


```python
# print matrix for predictions on training and testing
print("Training set")
confmatrix(truth = outcome_train, prediction = rlc_train_pred_class)
```

    Training set





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>279.0</td>
      <td>71.0</td>
      <td>350.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>51.0</td>
      <td>136.0</td>
      <td>187.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>330.0</td>
      <td>207.0</td>
      <td>537.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
print("\nTesting set")
confmatrix(truth = outcome_test, prediction = rlc_test_pred_class)
```

    
    Testing set





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>Truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>119.0</td>
      <td>31.0</td>
      <td>150.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>24.0</td>
      <td>57.0</td>
      <td>81.0</td>
    </tr>
    <tr>
      <th>Predictions</th>
      <td>143.0</td>
      <td>88.0</td>
      <td>231.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = rlc_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = rlc_pred_prob_test, col = 'r')
```


![png](../fig/50-Classification_89_0.png)


## Decision Trees

A decision tree üå≥ picks the best split in the data greedily for each feature and basically makes a flowchart to follow with a new data point to say what you should classify it as. This makes them easy to understand, but also usually not very accurate. 

### Let's Classify!
Train Decision Tree classifier.

To see how the gini coefficient vs entropy are calculated see [here](https://scikit-learn.org/stable/modules/tree.html#classification-criteria). 


```python
from sklearn.tree import DecisionTreeClassifier
cf_dtc = DecisionTreeClassifier(random_state= 42)
dtc_dict = {
    # function to measure quality of split
    'criterion': ['gini', 'entropy'],
    # The minimum number of samples required to split an internal node
    'min_samples_split': np.arange(start = 2, stop = 20, step = 3),
    # The minimum number of samples required to be at a leaf node
    'min_samples_leaf': np.arange(start = 1, stop = 10, step = 2)}


cf_dtc_gscv = GridSearchCV(cf_dtc,
                           dtc_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')
    
# fit the model on the training data
fit_dtc = cf_dtc_gscv.fit(features_train_sc, outcome_train)

# what were the best parameters?
fit_dtc.best_estimator_
```




    DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                           max_depth=None, max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=9, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, presort='deprecated',
                           random_state=42, splitter='best')



Use trained classifier to predict outcome for test-set


```python
dtc_train_pred_class = fit_dtc.predict(features_train_sc)
dtc_test_pred_class = fit_dtc.predict(features_test_sc)

dtc_pred_prob_train = model_probabilities(model = cf_dtc_gscv, dataset=features_train_sc)
dtc_pred_prob_test = model_probabilities(model = cf_dtc_gscv, dataset=features_test_sc)


# # print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = dtc_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = dtc_test_pred_class))



# # plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = dtc_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = dtc_pred_prob_test, col = 'r')
```

    Training set
                     0      1  Truth
    0            320.0   30.0  350.0
    1             54.0  133.0  187.0
    Predictions  374.0  163.0  537.0
    
    Testing set
                     0     1  Truth
    0            131.0  19.0  150.0
    1             42.0  39.0   81.0
    Predictions  173.0  58.0  231.0



![png](../fig/50-Classification_93_1.png)


We can also plot the decision tree


```python
from sklearn import tree
from sklearn.tree import plot_tree
tree.plot_tree(fit_dtc.best_estimator_, max_depth=2,
               label='root',
               filled=True); 
```


![png](../fig/50-Classification_95_0.png)


> ## Challenge
>
> Do you think this model is fit well?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > No, the tree has overfit.
> > 
> > {: .output}
> {: .solution}
{: .challenge}

## Random Forest Classifier
A random decision tree is where you make a decision tree but only train it on either:

- (a) a random sample of the available data or 
- (b) a random sample of the available features or 
- (c) both.

A random forest is a whole bunch of these averaged together. 

Turns out these do pretty good and are used all over the place. But because they're the average of so many different models it's hard to get an understanding about it. It's basically a black box that predicts well.


```python
from sklearn.ensemble import RandomForestClassifier
cf_rfc = RandomForestClassifier(random_state=42)
 
rf_dict = {
    # The number of trees in the forest
    'n_estimators': np.arange(10,150,25), 
    # quality of a split
    'criterion': ['entropy'],
    # The minimum number of samples required to split an internal node
    'min_samples_split': np.arange(start = 2, stop = 20, step = 5),
    # The minimum number of samples required to be at a leaf node
    'min_samples_leaf': np.arange(start = 1, stop = 3, step = 1),
    # the number of features to consider when looking for the best split
    # max_features=sqrt(n_features)/max_features=log2(n_features)
    'max_features': ['sqrt'],
    'class_weight': ['balanced_subsample', None]
}

# use GridSeachCV to which form of regularisation is optimal
cf_rf_gscv = GridSearchCV(cf_rfc,
                           rf_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')

# fit model on training data
fit_rf = cf_rf_gscv.fit(features_train_sc, outcome_train)

# what were the best parameters?
fit_rf.best_estimator_
```




    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                           class_weight='balanced_subsample', criterion='entropy',
                           max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                           max_samples=None, min_impurity_decrease=0.0,
                           min_impurity_split=None, min_samples_leaf=2,
                           min_samples_split=12, min_weight_fraction_leaf=0.0,
                           n_estimators=110, n_jobs=None, oob_score=False,
                           random_state=42, verbose=0, warm_start=False)



Use trained classifier to predict outcome for test-set


```python
rf_train_pred_class = cf_rf_gscv.predict(features_train_sc)
rf_test_pred_class = cf_rf_gscv.predict(features_test_sc)

rf_pred_prob_train = model_probabilities(model = cf_rf_gscv, dataset=features_train_sc)
rf_pred_prob_test = model_probabilities(model = cf_rf_gscv, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = rf_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = rf_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = rf_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = rf_pred_prob_test, col = 'r');
```

    Training set
                     0      1  Truth
    0            328.0   22.0  350.0
    1             14.0  173.0  187.0
    Predictions  342.0  195.0  537.0
    
    Testing set
                     0     1  Truth
    0            122.0  28.0  150.0
    1             26.0  55.0   81.0
    Predictions  148.0  83.0  231.0



![png](../fig/50-Classification_100_1.png)


## Support Vector Machine
A support vector machine tries to find the data points right on the boundary between the two classes (the "support vectors") and then uses them to define a maximum margin boundary.

![A linear Support Vector Machine for Cats](../fig/50-CatSVM.jpg)

### Let's Classify!
Train SVM


```python
from sklearn import svm
cf_svm = svm.SVC(random_state=42, probability=True, 
                 # Kernel coefficient for ‚Äòrbf‚Äô, ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô.
                 # 1 / (n_features * X.var()) 
                gamma = 'scale')

cf_svm_gscv_lin = GridSearchCV(cf_svm,
                           {'kernel': ['linear']},
                           # use 5xfold cross-validation
                           cv=5,
                           # use F1 score as error metric
                           scoring = 'f1')

fit_svm_lin = cf_svm_gscv_lin.fit(features_train_sc, outcome_train)

# use GridSeachCV to which form of regularisation is optimal
cf_svm_gscv_rad = GridSearchCV(cf_svm,
                           {'kernel': ['rbf']},
                           # use 5xfold cross-validation
                           cv=5,
                           # use F1 as error metric
                           scoring = 'f1')

fit_svm_rbf = cf_svm_gscv_rad.fit(features_train_sc, outcome_train)
```

Use trained classifier to predict outcome for test-set


```python
fit_svm_lin.best_estimator_
```




    SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',
        max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,
        verbose=False)




```python
fit_svm_rbf.best_estimator_
```




    SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
        max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,
        verbose=False)




```python
svm_l_train_pred_class = cf_svm_gscv_lin.predict(features_train_sc)
svm_l_test_pred_class = cf_svm_gscv_lin.predict(features_test_sc)

svm_l_pred_prob_train = model_probabilities(model = cf_svm_gscv_lin, dataset=features_train_sc)
svm_l_pred_prob_test = model_probabilities(model = cf_svm_gscv_lin, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = svm_l_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = svm_l_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = svm_l_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = svm_l_pred_prob_test, col = 'r')
```

    Training set
                     0      1  Truth
    0            317.0   33.0  350.0
    1             81.0  106.0  187.0
    Predictions  398.0  139.0  537.0
    
    Testing set
                     0     1  Truth
    0            130.0  20.0  150.0
    1             43.0  38.0   81.0
    Predictions  173.0  58.0  231.0



![png](../fig/50-Classification_106_1.png)



```python
svm_r_train_pred_class = cf_svm_gscv_rad.predict(features_train_sc)
svm_r_test_pred_class = cf_svm_gscv_rad.predict(features_test_sc)

svm_r_pred_prob_train = model_probabilities(model = cf_svm_gscv_rad, dataset=features_train_sc)
svm_r_pred_prob_test = model_probabilities(model = cf_svm_gscv_rad, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = svm_r_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = svm_r_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = svm_r_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = svm_r_pred_prob_test, col = 'r')
```

    Training set
                     0      1  Truth
    0            327.0   23.0  350.0
    1             62.0  125.0  187.0
    Predictions  389.0  148.0  537.0
    
    Testing set
                     0     1  Truth
    0            131.0  19.0  150.0
    1             40.0  41.0   81.0
    Predictions  171.0  60.0  231.0



![png](../fig/50-Classification_107_1.png)


# Compare all the classifiers


```python
evaluations = ['Misclassification rate', 'Sensitivity', 'Specificity', 'AUC']
pretrained_models = {'k Nearest Neighbours':cf_knn_gscv, 
                     'Naive Bayes':cf_gnb_gscv, 
                     'Regularised Logistic Classifier':cf_rlc_gscv, 
                     'Decision Tree':cf_dtc_gscv, 
                     'Random Forest':cf_rf_gscv, 
                     'Linear SVM':cf_svm_gscv_lin,
                     'Radial SVM':cf_svm_gscv_rad}
comparison_stats = pd.DataFrame(index = pretrained_models.keys(), columns=evaluations)
for method, model in pretrained_models.items():
    outcome_pred_class = model.predict(features_test_sc)
    outcome_pred_prob = model.predict_proba(features_test_sc)
    outcome_pred_prob1 = [p[1] for p in outcome_pred_prob]
    AUC = roc_auc_score(outcome_test, outcome_pred_prob1)
    conf_mat = confusion_matrix(outcome_test, outcome_pred_class)
    # in this case 0-0 is negatives
    # 1-1 is diabetes
    TP = conf_mat[1,1]
    FP = conf_mat[0,1]
    TN = conf_mat[0,0]
    FN = conf_mat[1,0]
    
    comparison_stats.loc[method,'Misclassification rate']  = 1. - accuracy_score(outcome_test, outcome_pred_class)
    # sensitivity == recall
    comparison_stats.loc[method,'Sensitivity'] = TP/(TP + FN)
    comparison_stats.loc[method,'Specificity'] = TN/(TN + FP)
    comparison_stats.loc[method,'Precision'] = TP/(TP + FP)
    comparison_stats.loc[method,'Accuracy'] = (TP + TN)/(TP + FP + TN + FN)
    comparison_stats.loc[method,'FDR'] = FP/(FP + TP)
    comparison_stats.loc[method,'F1'] = 2 * TP/(2 * TP + FP + FN)
    comparison_stats.loc[method,'AUC'] = AUC
```


```python
comparison_stats.round(decimals=3).sort_values(by = 'AUC', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Random Forest</th>
      <td>0.233766</td>
      <td>0.679012</td>
      <td>0.813333</td>
      <td>0.841646</td>
      <td>0.663</td>
      <td>0.766</td>
      <td>0.337</td>
      <td>0.671</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.238095</td>
      <td>0.703704</td>
      <td>0.793333</td>
      <td>0.836543</td>
      <td>0.648</td>
      <td>0.762</td>
      <td>0.352</td>
      <td>0.675</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.272727</td>
      <td>0.469136</td>
      <td>0.866667</td>
      <td>0.832757</td>
      <td>0.655</td>
      <td>0.727</td>
      <td>0.345</td>
      <td>0.547</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.506173</td>
      <td>0.873333</td>
      <td>0.817449</td>
      <td>0.683</td>
      <td>0.745</td>
      <td>0.317</td>
      <td>0.582</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.255411</td>
      <td>0.555556</td>
      <td>0.846667</td>
      <td>0.813827</td>
      <td>0.662</td>
      <td>0.745</td>
      <td>0.338</td>
      <td>0.604</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.272727</td>
      <td>0.580247</td>
      <td>0.806667</td>
      <td>0.803704</td>
      <td>0.618</td>
      <td>0.727</td>
      <td>0.382</td>
      <td>0.599</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.264069</td>
      <td>0.481481</td>
      <td>0.873333</td>
      <td>0.767202</td>
      <td>0.672</td>
      <td>0.736</td>
      <td>0.328</td>
      <td>0.561</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Misclassification rate', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Naive Bayes</th>
      <td>0.272727</td>
      <td>0.580247</td>
      <td>0.806667</td>
      <td>0.803704</td>
      <td>0.618</td>
      <td>0.727</td>
      <td>0.382</td>
      <td>0.599</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.272727</td>
      <td>0.469136</td>
      <td>0.866667</td>
      <td>0.832757</td>
      <td>0.655</td>
      <td>0.727</td>
      <td>0.345</td>
      <td>0.547</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.264069</td>
      <td>0.481481</td>
      <td>0.873333</td>
      <td>0.767202</td>
      <td>0.672</td>
      <td>0.736</td>
      <td>0.328</td>
      <td>0.561</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.255411</td>
      <td>0.555556</td>
      <td>0.846667</td>
      <td>0.813827</td>
      <td>0.662</td>
      <td>0.745</td>
      <td>0.338</td>
      <td>0.604</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.506173</td>
      <td>0.873333</td>
      <td>0.817449</td>
      <td>0.683</td>
      <td>0.745</td>
      <td>0.317</td>
      <td>0.582</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.238095</td>
      <td>0.703704</td>
      <td>0.793333</td>
      <td>0.836543</td>
      <td>0.648</td>
      <td>0.762</td>
      <td>0.352</td>
      <td>0.675</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.233766</td>
      <td>0.679012</td>
      <td>0.813333</td>
      <td>0.841646</td>
      <td>0.663</td>
      <td>0.766</td>
      <td>0.337</td>
      <td>0.671</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Sensitivity', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.238095</td>
      <td>0.703704</td>
      <td>0.793333</td>
      <td>0.836543</td>
      <td>0.648</td>
      <td>0.762</td>
      <td>0.352</td>
      <td>0.675</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.233766</td>
      <td>0.679012</td>
      <td>0.813333</td>
      <td>0.841646</td>
      <td>0.663</td>
      <td>0.766</td>
      <td>0.337</td>
      <td>0.671</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.272727</td>
      <td>0.580247</td>
      <td>0.806667</td>
      <td>0.803704</td>
      <td>0.618</td>
      <td>0.727</td>
      <td>0.382</td>
      <td>0.599</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.255411</td>
      <td>0.555556</td>
      <td>0.846667</td>
      <td>0.813827</td>
      <td>0.662</td>
      <td>0.745</td>
      <td>0.338</td>
      <td>0.604</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.506173</td>
      <td>0.873333</td>
      <td>0.817449</td>
      <td>0.683</td>
      <td>0.745</td>
      <td>0.317</td>
      <td>0.582</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.264069</td>
      <td>0.481481</td>
      <td>0.873333</td>
      <td>0.767202</td>
      <td>0.672</td>
      <td>0.736</td>
      <td>0.328</td>
      <td>0.561</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.272727</td>
      <td>0.469136</td>
      <td>0.866667</td>
      <td>0.832757</td>
      <td>0.655</td>
      <td>0.727</td>
      <td>0.345</td>
      <td>0.547</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Specificity', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Decision Tree</th>
      <td>0.264069</td>
      <td>0.481481</td>
      <td>0.873333</td>
      <td>0.767202</td>
      <td>0.672</td>
      <td>0.736</td>
      <td>0.328</td>
      <td>0.561</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.506173</td>
      <td>0.873333</td>
      <td>0.817449</td>
      <td>0.683</td>
      <td>0.745</td>
      <td>0.317</td>
      <td>0.582</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.272727</td>
      <td>0.469136</td>
      <td>0.866667</td>
      <td>0.832757</td>
      <td>0.655</td>
      <td>0.727</td>
      <td>0.345</td>
      <td>0.547</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.255411</td>
      <td>0.555556</td>
      <td>0.846667</td>
      <td>0.813827</td>
      <td>0.662</td>
      <td>0.745</td>
      <td>0.338</td>
      <td>0.604</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.233766</td>
      <td>0.679012</td>
      <td>0.813333</td>
      <td>0.841646</td>
      <td>0.663</td>
      <td>0.766</td>
      <td>0.337</td>
      <td>0.671</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.272727</td>
      <td>0.580247</td>
      <td>0.806667</td>
      <td>0.803704</td>
      <td>0.618</td>
      <td>0.727</td>
      <td>0.382</td>
      <td>0.599</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.238095</td>
      <td>0.703704</td>
      <td>0.793333</td>
      <td>0.836543</td>
      <td>0.648</td>
      <td>0.762</td>
      <td>0.352</td>
      <td>0.675</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'F1', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.238095</td>
      <td>0.703704</td>
      <td>0.793333</td>
      <td>0.836543</td>
      <td>0.648</td>
      <td>0.762</td>
      <td>0.352</td>
      <td>0.675</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.233766</td>
      <td>0.679012</td>
      <td>0.813333</td>
      <td>0.841646</td>
      <td>0.663</td>
      <td>0.766</td>
      <td>0.337</td>
      <td>0.671</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.255411</td>
      <td>0.555556</td>
      <td>0.846667</td>
      <td>0.813827</td>
      <td>0.662</td>
      <td>0.745</td>
      <td>0.338</td>
      <td>0.604</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.272727</td>
      <td>0.580247</td>
      <td>0.806667</td>
      <td>0.803704</td>
      <td>0.618</td>
      <td>0.727</td>
      <td>0.382</td>
      <td>0.599</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.506173</td>
      <td>0.873333</td>
      <td>0.817449</td>
      <td>0.683</td>
      <td>0.745</td>
      <td>0.317</td>
      <td>0.582</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.264069</td>
      <td>0.481481</td>
      <td>0.873333</td>
      <td>0.767202</td>
      <td>0.672</td>
      <td>0.736</td>
      <td>0.328</td>
      <td>0.561</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.272727</td>
      <td>0.469136</td>
      <td>0.866667</td>
      <td>0.832757</td>
      <td>0.655</td>
      <td>0.727</td>
      <td>0.345</td>
      <td>0.547</td>
    </tr>
  </tbody>
</table>
</div>



What do you think?
