---
title: "Classification"
author: "Madhura Killedar, Darya Vanichkina"
keypoints:
- Classification attempts to predict the class to which a particular observation belongs
- Scikit-learn has tons of classifier options
- There are many different metrics for assessing performance for a classification problem
- Which metric you choose and optimise for should be considered carefully, and will be different depending on the problem
- Exporatory data analysis is a time consuming but critical process that needs to be carried out prior to any modeling
- Support vector machines are a class of ML algorithms that construct a boundary in n-dimensional space to separate different classes
- They represent flexible methods that can handle a wide range of problems, including those impossible to address using conventional linear approaches
-  Different models had different best performance depending on the error metric we chose to evaluate


objectives: 
- Explain what a confusion matrix is, and how it relates to common classification error metrics
- Understand how missing data can be hidden within normal-looking datasets
- Use the scikit-learn library to work with data
- Learn how to build and evaluate classifiers
- Use the online documentation to figure out what the hyperparameters are for a specific method
- Build classification models using KNN, naive bayes, regularised and boosted logistic regression, decision trees and a random forest


questions: 
- What metrics are used to for evaluation in a classification problem?
- What dataset will we be working with today?
- What exploratory data analysis do we need to carry out on datasets we plan to work with for classification?
- How do we split our data into training and testing sets?
- How can we use scikit-learn to classify data in python?
- How can we find out which hyperparameters are best for classification?

source: ipynb
teaching: 150
start: 1
exercises: 120
---


## Classification

## What is a classifier?

A classifier is some kind of rule / black box / widget that you can feed a new observation/data/record and it will decide whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.

![A classifier for cats and not cats](50-CatNotCat.jpg)

You can have classifiers for anything you can have a yes/no answer to, e.g.

- Is this a cat? üê±
- Do these test results indicate cancer? üöë
- Is this email spam or not spam? üìß

You can also have classifiers that categorise things into multiple (more than two) categories e.g.

- Which animal is this, out of the 12 animals I have trained my model on? üê±
- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? üöë
- Is this email important, not important but not spam, or spam? üìß

It is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.

## Model evaluation (classification)

For now, let's imagine we have a classifier already. How can we test it to see how good it is?
A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.

![An demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.]("50-CatConfusion.jpg")

### Confusion Matrix

When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is simply a matrix that compares "the truth" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and of course if we correctly label something which is not a cat, as not a cat, then this is a true negative.

### Some common metrics

![Error metrics](50-ErrorMetrics.png)

#### AUC: Area under the curve

A good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.

To capture this balance, we often use a Receiver Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.

![A Receiver Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](50-CatArea.jpg)


# Pima Indians Diabetes
Today, we are going to be working with the [Pima Indians Diabetes Dataset](). This is a classic dataset from the UCI machine learning repository, which is now hosted on kaggle. We have downloaded the .csv file of this dataset from kaggle. It contains the following variables:



1) Pregnancies - Number of times pregnant
2) Glucose - Plasma glucose concentration a 2 hours in an [oral glucose tolerance test](https://en.wikipedia.org/wiki/Glucose_tolerance_test)
3) BloodPressure - Diastolic blood pressure (mm Hg)
4) SkinThickness - Triceps skin fold thickness (mm) - [a measure correlated with body fat](https://en.wikipedia.org/wiki/Anthropometry_of_the_upper_arm)
5) Insulin - 2-Hour serum insulin (mu U/ml)
6) BMI - Body mass index (weight in kg/(height in m)^2)
7) DiabetesPedigreeFunction - Diabetes pedigree function
8) Age - Age (years)
9) Outcome - diabetes status (1 - diabetic; 0 - non-diabetic)


The diabetes pedigree function was developed by [Smith 1988](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2245318/) to provide a synthesis ofthe diabetes mellitus history in relatives and the genetic relationship of those relatives to the subject. It uses information from parents, grandparents, siblings, aunts and uncles, and first cousin to provide a measure of the expected genetic influence of affected and unaffected relatives on the subject‚Äôs eventual diabetes risk:

![DiabetesPedigree](dpf.png)


- i - ranges across all relatives who HAD developed diabetes by subject's examination date
- j - ranges across all relatives who HAD NOT developed diabetes by subject's examination date
- Kx - percentage of genes shared by relative and subject. Equal to:
  - 0.5 when relative is parent or full sibling
  - 0.25 when relative is half-sibling, grandparent, aunt or uncle
  - 0.125 when relative is half aunt, half uncle or first cousin
- ADMi - age when diabetes was diagnosed
- ALC - age of relative when last "non-diabetic" assessment was made
- 88 and 14 - constants - maximum and minimum age at which patients in study were diagnosed with diabetes
- Constants 20 and 50 chosen so that:
  - A subject with no relatives has DPF slighly lower than average
  - DPF value decreases as young relatives free of diabetes join the database
  - DPF increases quickly as known relatives develop diabetes
  
DPF increases as:

- the number of relatives with diabetes increases
- the age at which those relatives develop diabetes decreases
- percentage of genes these relatives share with subject increase

DPF decreases as:

- the number of relatives who never develop diabetes increases
- their ages at last examination increase
- percentage of genes these relatives share with subject increase


## Let's Explore our data


```python
import numpy as np
import pandas as pd

import statsmodels.api as sm

# plotting libraries
import matplotlib.pyplot as plt
import seaborn as sns

# sklearn libraries
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, accuracy_score, recall_score, precision_score 
from sklearn.model_selection import GridSearchCV
from sklearn.utils.multiclass import unique_labels
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier

%matplotlib inline
sns.set(font_scale = 1.5)
```

### Load Data


```python
diabetes = pd.read_csv('../data/diabetes.csv') # read csv
```

Explore the variables:


```python
print(diabetes.columns)
```

    Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
          dtype='object')


Recode Outcome into integers


```python
print(diabetes.info())
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 768 entries, 0 to 767
    Data columns (total 9 columns):
    Pregnancies                 768 non-null int64
    Glucose                     768 non-null int64
    BloodPressure               768 non-null int64
    SkinThickness               768 non-null int64
    Insulin                     768 non-null int64
    BMI                         768 non-null float64
    DiabetesPedigreeFunction    768 non-null float64
    Age                         768 non-null int64
    Outcome                     768 non-null int64
    dtypes: float64(2), int64(7)
    memory usage: 54.1 KB
    None


### Summary info
Shape of data frame


```python
diabetes.shape
```




    (768, 9)



Look for missing data:


```python
diabetes.count()
```




    Pregnancies                 768
    Glucose                     768
    BloodPressure               768
    SkinThickness               768
    Insulin                     768
    BMI                         768
    DiabetesPedigreeFunction    768
    Age                         768
    Outcome                     768
    dtype: int64



It seems like there is no missing data.
Get a summary of the data frame:


```python
diabetes.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.845052</td>
      <td>120.894531</td>
      <td>69.105469</td>
      <td>20.536458</td>
      <td>79.799479</td>
      <td>31.992578</td>
      <td>0.471876</td>
      <td>33.240885</td>
      <td>0.348958</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.369578</td>
      <td>31.972618</td>
      <td>19.355807</td>
      <td>15.952218</td>
      <td>115.244002</td>
      <td>7.884160</td>
      <td>0.331329</td>
      <td>11.760232</td>
      <td>0.476951</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.078000</td>
      <td>21.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>99.000000</td>
      <td>62.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>27.300000</td>
      <td>0.243750</td>
      <td>24.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>117.000000</td>
      <td>72.000000</td>
      <td>23.000000</td>
      <td>30.500000</td>
      <td>32.000000</td>
      <td>0.372500</td>
      <td>29.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.000000</td>
      <td>140.250000</td>
      <td>80.000000</td>
      <td>32.000000</td>
      <td>127.250000</td>
      <td>36.600000</td>
      <td>0.626250</td>
      <td>41.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>17.000000</td>
      <td>199.000000</td>
      <td>122.000000</td>
      <td>99.000000</td>
      <td>846.000000</td>
      <td>67.100000</td>
      <td>2.420000</td>
      <td>81.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>



> ## Challenge
>
> Look at the output of summary above and the table that explains what each of the variables are. Do the 
> values for all 
> - (a) Pregnancies and Glucose
> - (b) Blood pressure and Skin thickness
> - (c) Insulin and DiabetesPedigreeFunction, and
> - (d) BMI and Age 
> make sense?
>
> If not, how do you think we should deal with them? 
> Can you hypothesise what the consequences of this approach would be?
>
> {: .source}
>
> > ## Solution
> > 
> > ~~~
> >
> > # possibly missing
> > diabetes[diabetes == 0].count()
> > 
> > # not missing
> > diabetes[diabetes != 0].count()
> > 
> > ~~~
> > 
> > It is clear that the values of several variables are zero when it is impossible for them to be so (i.e. this value could not be zero if it was measured). Hence, we are dealing with "hidden" missing data, and should recode it as NA.
> > 
> > The following variables have zero "values" that are actually likely to be missing:
> > 
> > 1. Glucose (a)
> > 2. BloodPressure (b)
> > 3. SkinThickness (b)
> > 4. Insulin (c)
> > 5. BMI (d)
> > 
> > {: .output}
> {: .solution}
{: .challenge}

### Let‚Äôs use visualisation to further explore the dataset:


```python
_ = sns.countplot(x="Pregnancies",
                hue="Outcome", 
                data=diabetes)

```


![png](../fig/50-Classification_19_0.png)


If we wanted to look at all possible scatterplot pairs we would do something like:


```python
#catVars = diabetes.select_dtypes(include = ['object']).columns
numVars = diabetes.select_dtypes(exclude = ['object']).columns
```


```python
diabetes['Outcome'].value_counts()
```




    0    500
    1    268
    Name: Outcome, dtype: int64



If we wanted to look at all possible scatterplot pairs we would do something like:


```python
_ = sns.pairplot(data=diabetes,
                 vars=numVars,
                 hue='Outcome',
                 palette={0:'k',1:'r'},
                 # use kernel density estimates for univariate plots
                 diag_kind='kde',
                 # make the shape of points circular and diamond, respectively
                 markers=["o", "d"])
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kde.py:487: RuntimeWarning: invalid value encountered in true_divide
      binned = fast_linbin(X, a, b, gridsize) / (delta * nobs)
    /Users/darya/anaconda3/lib/python3.7/site-packages/statsmodels/nonparametric/kdetools.py:34: RuntimeWarning: invalid value encountered in double_scalars
      FAC1 = 2*(np.pi*bw/RANGE)**2
    /Users/darya/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce
      return ufunc.reduce(obj, axis, dtype, out, **passkwargs)



![png](../fig/50-Classification_24_1.png)


Generate a boxplot by possible prediction variables. Which do you hypothesize would me it easiest for us to separate the data?


```python
# copy the original dataframe
diabetes2 = diabetes.copy()
# create a new patient id column
diabetes2['PatientID'] = range(1, len(diabetes2) + 1)
# melt that dataframe
diabetes2 = diabetes2.melt(id_vars=['PatientID','Outcome'])

grid = sns.axisgrid.FacetGrid(diabetes2[diabetes2.variable.isin(numVars[1:5])], 
                              col='variable', 
                              # y axis scale different for each boxplot
                              sharey=False)
grid.map(sns.boxplot, 'Outcome','value')
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/seaborn/axisgrid.py:715: UserWarning: Using the boxplot function without specifying `order` is likely to produce an incorrect plot.
      warnings.warn(warning)





    <seaborn.axisgrid.FacetGrid at 0x12478c7b8>




![png](../fig/50-Classification_26_2.png)



```python
grid = sns.axisgrid.FacetGrid(diabetes2[diabetes2.variable.isin(numVars[5:])], 
                              col='variable', 
                               # y axis scale different for each boxplot
                              sharey=False)
grid.map(sns.boxplot, 'Outcome','value')
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/seaborn/axisgrid.py:715: UserWarning: Using the boxplot function without specifying `order` is likely to produce an incorrect plot.
      warnings.warn(warning)





    <seaborn.axisgrid.FacetGrid at 0x1249dea58>




![png](../fig/50-Classification_27_2.png)


Make a correlation plot betwee all numeric variables


```python
corr = diabetes.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Generate a custom diverging colormap
# hue_negative, hue_positive
cmap = sns.diverging_palette(10,260, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.set(rc={'figure.figsize':(12,8)})
sns.set(font_scale = 1.2)
_ = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0., square=True, linewidths=.5)
```


![png](../fig/50-Classification_29_0.png)


## Prepare Data

Let‚Äôs replace the missing values in the diabetes data frame with NaN.


```python
# get the column names
diabetes.columns
```




    Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
           'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
          dtype='object')




```python
replaceVars = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']
```


```python
# mark zero values as missing or NaN
diabetes[replaceVars] = diabetes[replaceVars].replace(0, np.NaN)
```


```python
print(diabetes.isnull().sum())
```

    Pregnancies                   0
    Glucose                       5
    BloodPressure                35
    SkinThickness               227
    Insulin                     374
    BMI                          11
    DiabetesPedigreeFunction      0
    Age                           0
    Outcome                       0
    dtype: int64


Let‚Äôs generate a newcorrelation plot where the missing data has been properly recoded as NaN. Which correlations change?


```python
corr = diabetes.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Generate a custom diverging colormap
# hue_negative, hue_positive
cmap = sns.diverging_palette(10,260, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.set(rc={'figure.figsize':(12,8)})
sns.set(font_scale = 1.2)
_ = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0., square=True, linewidths=.5)
```


![png](../fig/50-Classification_36_0.png)


***
## Aim

To create a classifier for predicting whether a person has diabetes or not.

## Train-Test split

We‚Äôre going to split our data into 70% training and 30% testing sets.


```python
rs = np.random.RandomState(42)
features_train, features_test, outcome_train, outcome_test = train_test_split(diabetes[diabetes.columns.difference(['Outcome'])],diabetes['Outcome'], train_size=0.7, test_size=0.3)
```

How many examples do we have in the training and testing sets?


```python
features_train.shape
```




    (537, 8)




```python
features_test.shape
```




    (231, 8)



## Impute missing values using median values


```python
## Impute missing information
imp_median = SimpleImputer(strategy='median') 
imp_median.fit(features_train)
features_train_imp = pd.DataFrame(imp_median.transform(features_train))
features_test_imp = pd.DataFrame(imp_median.transform(features_test))
features_train_imp.columns = features_train.columns
features_test_imp.columns = features_test.columns
```

Confirm that we have imputed the values for BOTH training and testing datasets using the TRAINING data median!


```python
features_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>638</th>
      <td>32</td>
      <td>40.9</td>
      <td>76.0</td>
      <td>0.871</td>
      <td>97.0</td>
      <td>91.0</td>
      <td>7</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>371</th>
      <td>21</td>
      <td>NaN</td>
      <td>64.0</td>
      <td>1.731</td>
      <td>118.0</td>
      <td>89.0</td>
      <td>0</td>
      <td>23.0</td>
    </tr>
    <tr>
      <th>574</th>
      <td>23</td>
      <td>30.1</td>
      <td>86.0</td>
      <td>0.892</td>
      <td>143.0</td>
      <td>330.0</td>
      <td>1</td>
      <td>30.0</td>
    </tr>
    <tr>
      <th>659</th>
      <td>27</td>
      <td>34.2</td>
      <td>82.0</td>
      <td>1.292</td>
      <td>80.0</td>
      <td>70.0</td>
      <td>3</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>282</th>
      <td>37</td>
      <td>32.4</td>
      <td>88.0</td>
      <td>0.262</td>
      <td>133.0</td>
      <td>155.0</td>
      <td>7</td>
      <td>15.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_train_imp.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>32.0</td>
      <td>40.9</td>
      <td>76.0</td>
      <td>0.871</td>
      <td>97.0</td>
      <td>91.0</td>
      <td>7.0</td>
      <td>32.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>21.0</td>
      <td>32.4</td>
      <td>64.0</td>
      <td>1.731</td>
      <td>118.0</td>
      <td>89.0</td>
      <td>0.0</td>
      <td>23.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>23.0</td>
      <td>30.1</td>
      <td>86.0</td>
      <td>0.892</td>
      <td>143.0</td>
      <td>330.0</td>
      <td>1.0</td>
      <td>30.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>27.0</td>
      <td>34.2</td>
      <td>82.0</td>
      <td>1.292</td>
      <td>80.0</td>
      <td>70.0</td>
      <td>3.0</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>37.0</td>
      <td>32.4</td>
      <td>88.0</td>
      <td>0.262</td>
      <td>133.0</td>
      <td>155.0</td>
      <td>7.0</td>
      <td>15.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_test.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>210</th>
      <td>25</td>
      <td>27.7</td>
      <td>60.0</td>
      <td>0.290</td>
      <td>81.0</td>
      <td>NaN</td>
      <td>2</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>289</th>
      <td>33</td>
      <td>36.1</td>
      <td>72.0</td>
      <td>0.263</td>
      <td>108.0</td>
      <td>75.0</td>
      <td>5</td>
      <td>43.0</td>
    </tr>
    <tr>
      <th>402</th>
      <td>35</td>
      <td>35.0</td>
      <td>84.0</td>
      <td>0.286</td>
      <td>136.0</td>
      <td>88.0</td>
      <td>5</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>658</th>
      <td>51</td>
      <td>39.0</td>
      <td>106.0</td>
      <td>0.190</td>
      <td>127.0</td>
      <td>NaN</td>
      <td>11</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>727</th>
      <td>22</td>
      <td>32.4</td>
      <td>84.0</td>
      <td>0.433</td>
      <td>141.0</td>
      <td>NaN</td>
      <td>0</td>
      <td>26.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_test_imp.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>BMI</th>
      <th>BloodPressure</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Glucose</th>
      <th>Insulin</th>
      <th>Pregnancies</th>
      <th>SkinThickness</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>226</th>
      <td>25.0</td>
      <td>27.7</td>
      <td>60.0</td>
      <td>0.290</td>
      <td>81.0</td>
      <td>130.0</td>
      <td>2.0</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>227</th>
      <td>33.0</td>
      <td>36.1</td>
      <td>72.0</td>
      <td>0.263</td>
      <td>108.0</td>
      <td>75.0</td>
      <td>5.0</td>
      <td>43.0</td>
    </tr>
    <tr>
      <th>228</th>
      <td>35.0</td>
      <td>35.0</td>
      <td>84.0</td>
      <td>0.286</td>
      <td>136.0</td>
      <td>88.0</td>
      <td>5.0</td>
      <td>41.0</td>
    </tr>
    <tr>
      <th>229</th>
      <td>51.0</td>
      <td>39.0</td>
      <td>106.0</td>
      <td>0.190</td>
      <td>127.0</td>
      <td>130.0</td>
      <td>11.0</td>
      <td>30.0</td>
    </tr>
    <tr>
      <th>230</th>
      <td>22.0</td>
      <td>32.4</td>
      <td>84.0</td>
      <td>0.433</td>
      <td>141.0</td>
      <td>130.0</td>
      <td>0.0</td>
      <td>26.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
features_train.median()
```




    Age                          29.000
    BMI                          32.400
    BloodPressure                72.000
    DiabetesPedigreeFunction      0.364
    Glucose                     116.000
    Insulin                     130.000
    Pregnancies                   3.000
    SkinThickness                30.000
    dtype: float64




```python
features_test.median()
```




    Age                          31.000
    BMI                          32.000
    BloodPressure                72.000
    DiabetesPedigreeFunction      0.395
    Glucose                     119.000
    Insulin                     115.000
    Pregnancies                   3.000
    SkinThickness                28.000
    dtype: float64



### Standardize data ranges


```python
StSc = StandardScaler()
features_train_sc = StSc.fit_transform(features_train_imp)
features_test_sc  = StSc.transform(features_test_imp)
```

### Recode outcome


```python
training_true = outcome_train.astype("category").cat.codes.values
outcome_true = outcome_test.astype("category").cat.codes.values 
# code into 0's and 1's
# 0 is normal 
# 1 is diabetes
print((outcome_true == 0).sum())
print((outcome_true != 0).sum())

# 
print((training_true == 0).sum())
print((training_true != 0).sum())

# is our train/test balanced?
print((outcome_true != 0).sum()/(outcome_true == 0).sum())
print((training_true != 0).sum()/(training_true == 0).sum())
```

    151
    80
    349
    188
    0.5298013245033113
    0.5386819484240688


# Classifiers

## k-Nearest Neighbours Classifier

This takes the nearest k things and and says what is the majority vote? E.g. in the example below we look at the seven nearest neighbours, 4 of which are cats so we say that the new example is probably a cat as well.

![A way to classify a new example as a cat or not...take the average of the nearest k=7 examples. It's a cat!](../fig/50-CatKNN.jpg)


### Let's Classify!

Train KNN classifier.

Link to [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) for students, if you'd like to look at the formulas to understand how one function can generalise two distance metrics.


```python
# the default settings are metric='minkowski' and p = 2
# , which is the same as the standard Euclidean metric
# p = 1 gets us manhattan distance

cf_knn= KNeighborsClassifier(metric='minkowski')

# use GridSeachCV to test how many neighbors are optimal
cf_knn_gscv = GridSearchCV(cf_knn, 
                           # test from 1 to 50 neighbors 
                           {'n_neighbors': np.arange(1, 51)},
                           # use 5xfold cross-validation
                           cv=5,
                           # use f1 as error metric
                           scoring = 'f1')

fit_knn = cf_knn_gscv.fit(features_train_sc, outcome_train)
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)


Explore the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for other error metrics we could have used.


```python
# Look at the cross-validation results
fit_knn.cv_results_
```




    {'mean_fit_time': array([0.00131526, 0.00053339, 0.00061474, 0.00059137, 0.00060363,
            0.00059562, 0.00055022, 0.00056925, 0.00054188, 0.00055232,
            0.00055485, 0.00059195, 0.00060849, 0.00060363, 0.00055113,
            0.00056748, 0.00054765, 0.00054178, 0.00054202, 0.00065794,
            0.00070696, 0.00056477, 0.00053415, 0.00051184, 0.00059195,
            0.00053916, 0.00051107, 0.00053611, 0.00054369, 0.00055013,
            0.0005383 , 0.00052009, 0.0005271 , 0.00052938, 0.00053453,
            0.00053105, 0.00051894, 0.00052009, 0.00051432, 0.00052228,
            0.00052381, 0.000527  , 0.00056176, 0.00056171, 0.00052795,
            0.00051298, 0.00053682, 0.00054545, 0.00051279, 0.00053248]),
     'std_fit_time': array([1.47984677e-03, 9.64390627e-06, 1.63533429e-04, 3.30584706e-05,
            6.21919605e-05, 5.64242439e-05, 1.70667740e-05, 5.69359541e-05,
            4.82383892e-06, 1.43848423e-05, 1.46989646e-05, 4.92615481e-05,
            3.52635183e-05, 2.10330054e-05, 1.85198298e-05, 5.68579876e-05,
            8.02589324e-06, 2.85943305e-06, 3.77455090e-06, 1.90504567e-04,
            2.03939713e-04, 1.89439427e-05, 9.45605476e-06, 2.89813450e-06,
            7.00717680e-05, 1.44172091e-05, 3.39057709e-06, 2.47815900e-05,
            1.32450909e-05, 1.68517247e-05, 2.59459888e-05, 1.28556925e-05,
            1.40561048e-05, 1.33118724e-05, 2.66496093e-05, 1.24148105e-05,
            1.25351048e-05, 1.71969720e-05, 2.30860108e-06, 1.04791407e-05,
            1.31342456e-05, 1.97715570e-05, 1.48803757e-05, 7.37980028e-05,
            1.22711242e-05, 5.13923400e-06, 2.25905095e-05, 6.48758640e-05,
            2.80240731e-06, 1.41698482e-05]),
     'mean_score_time': array([0.00353613, 0.00320072, 0.00338616, 0.00359097, 0.0037324 ,
            0.00355902, 0.00341434, 0.00339055, 0.00349822, 0.00374851,
            0.00365577, 0.00367141, 0.0037261 , 0.00374112, 0.00356064,
            0.00353603, 0.00365057, 0.00359902, 0.00382543, 0.0038919 ,
            0.00439315, 0.00375414, 0.00359859, 0.00358801, 0.00388017,
            0.00376191, 0.00354557, 0.00370541, 0.00383592, 0.00389142,
            0.00364447, 0.00371799, 0.00380239, 0.00377059, 0.00365715,
            0.00390701, 0.00375385, 0.00369501, 0.00382123, 0.00379882,
            0.00390201, 0.00385685, 0.00404296, 0.00401297, 0.00383458,
            0.00389981, 0.00391784, 0.00381627, 0.00393705, 0.00390501]),
     'std_score_time': array([7.10722187e-04, 1.14971105e-04, 1.36781684e-04, 3.19123410e-04,
            6.33152405e-04, 2.92973795e-04, 1.07058958e-04, 1.10313368e-05,
            1.16729951e-04, 4.66718716e-04, 1.06513361e-04, 1.10865306e-04,
            1.46728427e-04, 4.61374388e-05, 6.64587844e-05, 1.61642873e-05,
            1.16484262e-04, 2.36645783e-05, 4.18389786e-04, 3.83873590e-04,
            7.38495957e-04, 1.05142107e-04, 6.08524553e-05, 1.41230312e-04,
            2.88267667e-04, 1.90424046e-04, 4.72163663e-05, 3.04172573e-04,
            2.31614588e-04, 2.06810735e-04, 1.23382479e-04, 1.51797917e-04,
            1.68684311e-04, 9.85783991e-05, 5.18215602e-05, 2.00667637e-04,
            9.75454762e-05, 6.07021381e-05, 1.20418387e-04, 9.01932916e-05,
            8.98305270e-05, 1.32475404e-04, 2.27793380e-04, 2.27141299e-04,
            1.08061085e-04, 1.22748573e-04, 1.89309793e-04, 2.97680085e-05,
            1.79902603e-04, 9.29192054e-05]),
     'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,
                        31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,
                        45, 46, 47, 48, 49, 50],
                  mask=[False, False, False, False, False, False, False, False,
                        False, False, False, False, False, False, False, False,
                        False, False, False, False, False, False, False, False,
                        False, False, False, False, False, False, False, False,
                        False, False, False, False, False, False, False, False,
                        False, False, False, False, False, False, False, False,
                        False, False],
            fill_value='?',
                 dtype=object),
     'params': [{'n_neighbors': 1},
      {'n_neighbors': 2},
      {'n_neighbors': 3},
      {'n_neighbors': 4},
      {'n_neighbors': 5},
      {'n_neighbors': 6},
      {'n_neighbors': 7},
      {'n_neighbors': 8},
      {'n_neighbors': 9},
      {'n_neighbors': 10},
      {'n_neighbors': 11},
      {'n_neighbors': 12},
      {'n_neighbors': 13},
      {'n_neighbors': 14},
      {'n_neighbors': 15},
      {'n_neighbors': 16},
      {'n_neighbors': 17},
      {'n_neighbors': 18},
      {'n_neighbors': 19},
      {'n_neighbors': 20},
      {'n_neighbors': 21},
      {'n_neighbors': 22},
      {'n_neighbors': 23},
      {'n_neighbors': 24},
      {'n_neighbors': 25},
      {'n_neighbors': 26},
      {'n_neighbors': 27},
      {'n_neighbors': 28},
      {'n_neighbors': 29},
      {'n_neighbors': 30},
      {'n_neighbors': 31},
      {'n_neighbors': 32},
      {'n_neighbors': 33},
      {'n_neighbors': 34},
      {'n_neighbors': 35},
      {'n_neighbors': 36},
      {'n_neighbors': 37},
      {'n_neighbors': 38},
      {'n_neighbors': 39},
      {'n_neighbors': 40},
      {'n_neighbors': 41},
      {'n_neighbors': 42},
      {'n_neighbors': 43},
      {'n_neighbors': 44},
      {'n_neighbors': 45},
      {'n_neighbors': 46},
      {'n_neighbors': 47},
      {'n_neighbors': 48},
      {'n_neighbors': 49},
      {'n_neighbors': 50}],
     'split0_test_score': array([0.54545455, 0.42307692, 0.65671642, 0.56666667, 0.66666667,
            0.55172414, 0.58064516, 0.49122807, 0.56666667, 0.55172414,
            0.59016393, 0.47272727, 0.59016393, 0.51724138, 0.55737705,
            0.5862069 , 0.57627119, 0.5       , 0.55172414, 0.55172414,
            0.55172414, 0.50909091, 0.55172414, 0.52631579, 0.57627119,
            0.5       , 0.55172414, 0.54545455, 0.52631579, 0.44444444,
            0.46428571, 0.44444444, 0.49122807, 0.45283019, 0.5       ,
            0.48148148, 0.47272727, 0.47272727, 0.47272727, 0.47272727,
            0.47272727, 0.44444444, 0.44444444, 0.44444444, 0.44444444,
            0.44444444, 0.5       , 0.47272727, 0.5       , 0.45283019]),
     'split1_test_score': array([0.48571429, 0.37037037, 0.60273973, 0.46875   , 0.56756757,
            0.56716418, 0.61111111, 0.57575758, 0.60869565, 0.60606061,
            0.6       , 0.56716418, 0.60869565, 0.57142857, 0.60606061,
            0.57142857, 0.59701493, 0.57142857, 0.59375   , 0.57142857,
            0.59375   , 0.58064516, 0.58461538, 0.57142857, 0.55384615,
            0.57142857, 0.59701493, 0.5       , 0.5625    , 0.52459016,
            0.57575758, 0.52459016, 0.55384615, 0.49180328, 0.53125   ,
            0.50793651, 0.55384615, 0.51612903, 0.53968254, 0.51612903,
            0.50793651, 0.46666667, 0.51612903, 0.46666667, 0.46666667,
            0.46666667, 0.46666667, 0.44067797, 0.46666667, 0.38596491]),
     'split2_test_score': array([0.58823529, 0.50746269, 0.70588235, 0.72222222, 0.6835443 ,
            0.68493151, 0.6746988 , 0.66666667, 0.6835443 , 0.70422535,
            0.73684211, 0.73972603, 0.72      , 0.70422535, 0.71052632,
            0.71428571, 0.71232877, 0.74285714, 0.71232877, 0.72222222,
            0.71232877, 0.69565217, 0.70422535, 0.73529412, 0.71232877,
            0.68571429, 0.71232877, 0.70422535, 0.72222222, 0.70588235,
            0.68571429, 0.69565217, 0.68571429, 0.68656716, 0.67605634,
            0.70588235, 0.68571429, 0.70588235, 0.69565217, 0.6969697 ,
            0.69565217, 0.6969697 , 0.69565217, 0.6969697 , 0.67647059,
            0.66666667, 0.67647059, 0.70769231, 0.70588235, 0.65625   ]),
     'split3_test_score': array([0.58666667, 0.55737705, 0.63888889, 0.5483871 , 0.62857143,
            0.57142857, 0.57971014, 0.55384615, 0.55072464, 0.51612903,
            0.53731343, 0.53125   , 0.52941176, 0.55384615, 0.59701493,
            0.57575758, 0.6       , 0.58461538, 0.58823529, 0.6031746 ,
            0.60869565, 0.5625    , 0.63768116, 0.61290323, 0.61538462,
            0.6031746 , 0.61538462, 0.6031746 , 0.59375   , 0.6031746 ,
            0.59375   , 0.58064516, 0.58064516, 0.59016393, 0.59016393,
            0.59016393, 0.61290323, 0.61290323, 0.61290323, 0.61290323,
            0.61290323, 0.59016393, 0.59016393, 0.59016393, 0.59016393,
            0.57627119, 0.56666667, 0.54237288, 0.56666667, 0.55172414]),
     'split4_test_score': array([0.4       , 0.32142857, 0.51428571, 0.46875   , 0.57142857,
            0.54545455, 0.58823529, 0.58461538, 0.62686567, 0.60606061,
            0.5915493 , 0.59701493, 0.69444444, 0.61538462, 0.64788732,
            0.62686567, 0.61971831, 0.58461538, 0.58823529, 0.61538462,
            0.57971014, 0.59375   , 0.65671642, 0.64615385, 0.63636364,
            0.63636364, 0.61764706, 0.59375   , 0.60606061, 0.6031746 ,
            0.59375   , 0.6031746 , 0.59375   , 0.61290323, 0.63492063,
            0.61290323, 0.63492063, 0.59016393, 0.63492063, 0.54237288,
            0.61290323, 0.61290323, 0.61290323, 0.59016393, 0.6031746 ,
            0.53333333, 0.58064516, 0.53333333, 0.53333333, 0.53333333]),
     'mean_test_score': array([0.52154372, 0.43614348, 0.62408185, 0.55528849, 0.62374051,
            0.58430834, 0.60700014, 0.57442313, 0.60733187, 0.59695591,
            0.61138439, 0.5816127 , 0.62848232, 0.59241155, 0.62373326,
            0.61493726, 0.62111089, 0.59677083, 0.60695872, 0.61279505,
            0.60935274, 0.58835555, 0.62686188, 0.61832609, 0.61878004,
            0.59919117, 0.61883067, 0.58927861, 0.60217091, 0.57610283,
            0.58258951, 0.56955626, 0.58099011, 0.56663864, 0.5862909 ,
            0.5795302 , 0.59182366, 0.57945959, 0.59097379, 0.56823348,
            0.58024304, 0.56198885, 0.57167161, 0.55750027, 0.55594576,
            0.53741965, 0.55798984, 0.53937759, 0.55456603, 0.51588955]),
     'std_test_score': array([0.07091398, 0.08633379, 0.06383444, 0.09286789, 0.04763731,
            0.05138097, 0.03581528, 0.05664388, 0.04706292, 0.06372156,
            0.06673095, 0.08947936, 0.06980714, 0.06434232, 0.0521612 ,
            0.0535325 , 0.0477887 , 0.07976991, 0.05496086, 0.05934857,
            0.0549825 , 0.06107272, 0.05382712, 0.07110049, 0.05508894,
            0.06257249, 0.05255659, 0.0684765 , 0.06625275, 0.08773761,
            0.0707984 , 0.08366676, 0.06329673, 0.0845972 , 0.06478574,
            0.0801093 , 0.07328583, 0.08097901, 0.07760139, 0.07901398,
            0.08048706, 0.09455362, 0.08585456, 0.09254858, 0.0877413 ,
            0.08005629, 0.07275412, 0.09251619, 0.08292035, 0.09204802]),
     'rank_test_score': array([48, 50,  3, 44,  4, 27, 15, 35, 14, 19, 12, 29,  1, 21,  5, 10,  6,
            20, 16, 11, 13, 25,  2,  9,  8, 18,  7, 24, 17, 34, 28, 37, 30, 39,
            26, 32, 22, 33, 23, 38, 31, 40, 36, 42, 43, 47, 41, 46, 45, 49],
           dtype=int32)}




```python
print(fit_knn.best_estimator_.n_neighbors)
```

    13



```python
# lets make a pandas dataframe to enable plotting
cvresults = pd.DataFrame.from_dict({k: v for k, v in fit_knn.cv_results_.items() if k.startswith('split')})
cvparams = pd.DataFrame.from_dict(fit_knn.cv_results_['params'])
knn_cv_results = pd.concat([cvresults, cvparams], axis=1).melt(id_vars=['n_neighbors'])
```

Let's make a plot to see performance of the algorithm with various numbers of neighbors


```python
_ = sns.pointplot(x="n_neighbors",
                  y="value",
                 data = knn_cv_results)
# need to subtract 1 for x due to zero-based indexing
plt.scatter(x = fit_knn.best_estimator_.n_neighbors-1,
            y = knn_cv_results[knn_cv_results['n_neighbors'] == fit_knn.best_estimator_.n_neighbors]['value'].mean(),
            color = 'r',
            # plots point on top of other plot
            zorder = 10)
```




    <matplotlib.collections.PathCollection at 0x124b19b38>




![png](../fig/50-Classification_65_1.png)


> ## Challenge
>
> Do you think this is the optimal solution?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > Probably not, since the performance of the classifier after a certain number of neighbors (~9 - 30) doesn't really seem to improve that much
> > 
> > {: .output}
> {: .solution}
{: .challenge}


```python
# Use trained classifier to predict outcome for training and test
knn_outcome_pred_class_train = fit_knn.predict(features_train_sc)
knn_outcome_pred_class_test = fit_knn.predict(features_test_sc)


# get probabilities out
def model_probabilities(model = fit_knn, dataset = features_train_sc):
    probs1 = model.predict_proba(dataset)
    probs2 = [p[1] for p in probs1] 
    # hopefully close to 1 for true 1's
    return(probs2)

knn_outcome_pred_prob_train = model_probabilities(model = fit_knn, dataset = features_train_sc)
knn_outcome_pred_prob_test = model_probabilities(model = fit_knn, dataset = features_test_sc)

```

### Classifier Diagnostics/evaluation
So how well did the classifier do? Let's define a function to generate a confusion matrix:


```python
# define custom confusion matrix function
def confmatrix(truth = outcome_train, prediction = knn_outcome_pred_class_train):
    return(pd.crosstab(pd.Series(truth, name='True'), pd.Series(prediction, name='Predicted'), margins=True))
```

Assess performance on the training set:


```python
confmatrix(truth = outcome_train, prediction = knn_outcome_pred_class_train)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>True</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>160</td>
      <td>79</td>
      <td>239</td>
    </tr>
    <tr>
      <th>1</th>
      <td>98</td>
      <td>31</td>
      <td>129</td>
    </tr>
    <tr>
      <th>All</th>
      <td>258</td>
      <td>110</td>
      <td>368</td>
    </tr>
  </tbody>
</table>
</div>




```python
confmatrix(truth = outcome_test, prediction = knn_outcome_pred_class_test)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>True</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>28</td>
      <td>14</td>
      <td>42</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20</td>
      <td>12</td>
      <td>32</td>
    </tr>
    <tr>
      <th>All</th>
      <td>48</td>
      <td>26</td>
      <td>74</td>
    </tr>
  </tbody>
</table>
</div>




```python
# define a function to plot an ROC curve
def plot_ROC(truth = outcome_train, prediction1 = knn_outcome_pred_prob_train, col = 'b'):
    fpr, tpr, _ = roc_curve(truth, prediction1)
    AUC = roc_auc_score(truth, prediction1)
    plt.xlim([-0.05, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.plot(fpr, tpr, label='AUC = %4.2f'%AUC, color = col)
    _ = plt.legend(loc="lower right")

    
plot_ROC(truth = outcome_train, prediction1 = knn_outcome_pred_prob_train)
```


![png](../fig/50-Classification_73_0.png)



```python
# assess the same on the test set
plot_ROC(truth = outcome_test, prediction1 = knn_outcome_pred_prob_test)
```


![png](../fig/50-Classification_74_0.png)


> ## Challenge
>
> How well do you think this model generalised?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > While the overall performance of the model is not very good, it does generalise well, as the difference between the 
> > 
> > 
> > {: .output}
> {: .solution}
{: .challenge}

## Naive Bayes Classifier

A Na√Øve Bayes classifier assumes that each of your columns are independent (uncorrelated with each other). It works out a probability that your example is a cat by counting the fraction of cats that had that value in each column, multiplying the values together and then multiplying again by what fraction of your training examples were cats. This is just writing out bayes rule of conditional probability and simplifying it for independent columns.

$$p(\text{Cat}| x_\text{new})=\frac{p(\text{Cat})p(x_\text{new}|\text{Cat})}{p(\text{Cat})p(x_\text{new}|\text{Cat})+p(\text{Not Cat})p(x_\text{new}|\text{Not Cat})}$$

In practice your columns are probably not independent, but we still use it anyway and it's usually ok, providing we only care about the label and not the probability it spits out.  

Continuous variables have to be somehow turned into discrete variables before you can use this technique, but most algorithms do this for you automatically.


### Let's Classify!
Train Naive Bayes classifier


```python
from sklearn.naive_bayes import GaussianNB
cf_gnb = GaussianNB()


# use GridSeachCV to test how many neighbors are optimal
cf_gnb_gscv = GridSearchCV(cf_gnb, 
                           {'var_smoothing' : [1e-09]},
                           # use 5xfold cross-validation
                           cv=5,
                           # use f1 as error metric
                           scoring = 'f1')

fit_gnb = cf_gnb_gscv.fit(features_train_sc, outcome_train)
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)


Use trained classifier to predict outcome for test-set


```python
gnb_train_pred_class = cf_knn_gscv.predict(features_train_sc)
gnb_test_pred_class = cf_knn_gscv.predict(features_test_sc)

gnb_pred_prob_train = model_probabilities(model = cf_gnb_gscv, dataset=features_train_sc)
gnb_pred_prob_test = model_probabilities(model = cf_gnb_gscv, dataset=features_test_sc)
```

### Classifier Diagnostics
So how well did it go?


```python
# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = gnb_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = gnb_test_pred_class))
```

    Training set
    Predicted    0    1  All
    True                    
    0          160   79  239
    1           98   31  129
    All        258  110  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          28  14   42
    1          20  12   32
    All        48  26   74



```python
# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = gnb_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = gnb_pred_prob_test, col = 'r')
```


![png](../fig/50-Classification_83_0.png)


## Regularized Logistic Classifier

This fits a logistic regression to the probability of receiving a class label of 1 or 0. Regularisation (hopefully) stops it from overfitting. 

### Let's Classify!
Train Regularized Logistic classifier. 

There are several algorithms to do this accessible in scikit-learn, and we will use the one that uses the saga solver. 


```python
from sklearn.linear_model import LogisticRegression
# class_weight balanced will uses the values of y to automatically adjust weights inversely 
# proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)
# multi_class='ovr' - a binary problem is fit for each label
cf_rlc = LogisticRegression(random_state=42, solver='saga', multi_class='ovr')

# test different penalty and class weight parameters - define this as a dictionary to make subsequent plotting easier
rlc_dict = {'penalty': ['elasticnet'], 'class_weight': ['None', 'balanced'], 'l1_ratio': np.linspace(start=0, stop=1, num=8)
                           }
# use GridSeachCV to which form of regularisation is optimal
cf_rlc_gscv = GridSearchCV(cf_rlc,
                           rlc_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')
# fit model on training data
fit_rlc = cf_rlc_gscv.fit(features_train_sc, outcome_train)
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)



```python
# what is the best estimator?
fit_rlc.best_estimator_
```




    LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                       fit_intercept=True, intercept_scaling=1,
                       l1_ratio=0.8571428571428571, max_iter=100, multi_class='ovr',
                       n_jobs=None, penalty='elasticnet', random_state=42,
                       solver='saga', tol=0.0001, verbose=0, warm_start=False)



Use trained classifier to predict outcome for training and test sets


```python
rlc_train_pred_class = fit_rlc.predict(features_train_sc)
rlc_test_pred_class = fit_rlc.predict(features_test_sc)

rlc_pred_prob_train = model_probabilities(model = cf_rlc_gscv, dataset=features_train_sc)
rlc_pred_prob_test = model_probabilities(model = cf_rlc_gscv, dataset=features_test_sc)
```

### Classifier evaluation
So how well did the classifier do?


```python
# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = rlc_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = rlc_test_pred_class))

# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = rlc_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = rlc_pred_prob_test, col = 'r')
```

    Training set
    Predicted    0    1  All
    True                    
    0          140   99  239
    1           78   51  129
    All        218  150  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          26  16   42
    1          20  12   32
    All        46  28   74



![png](../fig/50-Classification_90_1.png)


## Decision Trees

A decision tree üå≥ picks the best split in the data greedily for each feature and basically makes a flowchart to follow with a new data point to say what you should classify it as. This makes them easy to understsand, but also usually not very accurate. 

### Let's Classify!
Train Decision Tree classifier.

To see how the gini coefficient vs entropy are calculated see [here](https://scikit-learn.org/stable/modules/tree.html#classification-criteria). 


```python
from sklearn.tree import DecisionTreeClassifier
cf_dtc = DecisionTreeClassifier(random_state= 42)
dtc_dict = {
    # function to measure quality of split
    'criterion': ['gini', 'entropy'],
    # The minimum number of samples required to split an internal node
    'min_samples_split': np.arange(start = 2, stop = 20, step = 3),
    # The minimum number of samples required to be at a leaf node
    'min_samples_leaf': np.arange(start = 1, stop = 10, step = 2)}


cf_dtc_gscv = GridSearchCV(cf_dtc,
                           dtc_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')
    
# fit the model on the training data
fit_dtc = cf_dtc_gscv.fit(features_train_sc, outcome_train)

# what were the best parameters?
fit_dtc.best_estimator_
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)





    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=5, min_samples_split=14,
                           min_weight_fraction_leaf=0.0, presort=False,
                           random_state=42, splitter='best')



Use trained classifier to predict outcome for test-set


```python
dtc_train_pred_class = fit_dtc.predict(features_train_sc)
dtc_test_pred_class = fit_dtc.predict(features_test_sc)

dtc_pred_prob_train = model_probabilities(model = cf_dtc_gscv, dataset=features_train_sc)
dtc_pred_prob_test = model_probabilities(model = cf_dtc_gscv, dataset=features_test_sc)


# # print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = dtc_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = dtc_test_pred_class))



# # plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = dtc_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = dtc_pred_prob_test, col = 'r')
```

    Training set
    Predicted    0    1  All
    True                    
    0          152   87  239
    1           83   46  129
    All        235  133  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          29  13   42
    1          22  10   32
    All        51  23   74



![png](../fig/50-Classification_94_1.png)


We can also plot the decision tree


```python
from sklearn import tree
from sklearn.tree import plot_tree
tree.plot_tree(fit_dtc.best_estimator_) 
```




    [Text(305.19878048780487, 415.1127272727273, 'X[4] <= 0.195\nentropy = 0.934\nsamples = 537\nvalue = [349, 188]'),
     Text(99.0109756097561, 375.5781818181818, 'X[1] <= -0.871\nentropy = 0.721\nsamples = 341\nvalue = [273, 68]'),
     Text(32.66341463414634, 336.0436363636364, 'X[5] <= -1.146\nentropy = 0.089\nsamples = 89\nvalue = [88, 1]'),
     Text(16.33170731707317, 296.5090909090909, 'entropy = 0.722\nsamples = 5\nvalue = [4, 1]'),
     Text(48.99512195121952, 296.5090909090909, 'entropy = 0.0\nsamples = 84\nvalue = [84, 0]'),
     Text(165.35853658536587, 336.0436363636364, 'X[0] <= -0.46\nentropy = 0.835\nsamples = 252\nvalue = [185, 67]'),
     Text(81.65853658536585, 296.5090909090909, 'X[7] <= 0.007\nentropy = 0.538\nsamples = 122\nvalue = [107, 15]'),
     Text(48.99512195121952, 256.9745454545455, 'X[5] <= -1.27\nentropy = 0.133\nsamples = 54\nvalue = [53, 1]'),
     Text(32.66341463414634, 217.44, 'entropy = 0.722\nsamples = 5\nvalue = [4, 1]'),
     Text(65.32682926829268, 217.44, 'entropy = 0.0\nsamples = 49\nvalue = [49, 0]'),
     Text(114.3219512195122, 256.9745454545455, 'X[1] <= 0.922\nentropy = 0.734\nsamples = 68\nvalue = [54, 14]'),
     Text(97.99024390243903, 217.44, 'X[3] <= 0.016\nentropy = 0.833\nsamples = 53\nvalue = [39, 14]'),
     Text(65.32682926829268, 177.90545454545457, 'X[3] <= -0.617\nentropy = 0.602\nsamples = 34\nvalue = [29, 5]'),
     Text(48.99512195121952, 138.3709090909091, 'X[6] <= -0.368\nentropy = 0.918\nsamples = 15\nvalue = [10, 5]'),
     Text(32.66341463414634, 98.83636363636367, 'entropy = 0.469\nsamples = 10\nvalue = [9, 1]'),
     Text(65.32682926829268, 98.83636363636367, 'entropy = 0.722\nsamples = 5\nvalue = [1, 4]'),
     Text(81.65853658536585, 138.3709090909091, 'entropy = 0.0\nsamples = 19\nvalue = [19, 0]'),
     Text(130.65365853658537, 177.90545454545457, 'X[1] <= 0.145\nentropy = 0.998\nsamples = 19\nvalue = [10, 9]'),
     Text(114.3219512195122, 138.3709090909091, 'entropy = 0.592\nsamples = 7\nvalue = [6, 1]'),
     Text(146.98536585365855, 138.3709090909091, 'entropy = 0.918\nsamples = 12\nvalue = [4, 8]'),
     Text(130.65365853658537, 217.44, 'entropy = 0.0\nsamples = 15\nvalue = [15, 0]'),
     Text(249.05853658536586, 296.5090909090909, 'X[3] <= 0.459\nentropy = 0.971\nsamples = 130\nvalue = [78, 52]'),
     Text(187.81463414634146, 256.9745454545455, 'X[4] <= -0.935\nentropy = 0.861\nsamples = 95\nvalue = [68, 27]'),
     Text(171.4829268292683, 217.44, 'entropy = 0.0\nsamples = 19\nvalue = [19, 0]'),
     Text(204.14634146341464, 217.44, 'X[1] <= -0.69\nentropy = 0.939\nsamples = 76\nvalue = [49, 27]'),
     Text(187.81463414634146, 177.90545454545457, 'entropy = 0.0\nsamples = 5\nvalue = [0, 5]'),
     Text(220.4780487804878, 177.90545454545457, 'X[0] <= -0.21\nentropy = 0.893\nsamples = 71\nvalue = [49, 22]'),
     Text(179.64878048780488, 138.3709090909091, 'X[1] <= 0.734\nentropy = 0.323\nsamples = 17\nvalue = [16, 1]'),
     Text(163.3170731707317, 98.83636363636367, 'entropy = 0.0\nsamples = 12\nvalue = [12, 0]'),
     Text(195.98048780487807, 98.83636363636367, 'entropy = 0.722\nsamples = 5\nvalue = [4, 1]'),
     Text(261.30731707317074, 138.3709090909091, 'X[4] <= -0.337\nentropy = 0.964\nsamples = 54\nvalue = [33, 21]'),
     Text(228.6439024390244, 98.83636363636367, 'X[7] <= 0.18\nentropy = 0.784\nsamples = 30\nvalue = [23, 7]'),
     Text(212.31219512195122, 59.30181818181819, 'X[4] <= -0.569\nentropy = 0.964\nsamples = 18\nvalue = [11, 7]'),
     Text(195.98048780487807, 19.76727272727277, 'entropy = 1.0\nsamples = 12\nvalue = [6, 6]'),
     Text(228.6439024390244, 19.76727272727277, 'entropy = 0.65\nsamples = 6\nvalue = [5, 1]'),
     Text(244.97560975609755, 59.30181818181819, 'entropy = 0.0\nsamples = 12\nvalue = [12, 0]'),
     Text(293.9707317073171, 98.83636363636367, 'X[6] <= -0.663\nentropy = 0.98\nsamples = 24\nvalue = [10, 14]'),
     Text(277.6390243902439, 59.30181818181819, 'entropy = 0.0\nsamples = 7\nvalue = [0, 7]'),
     Text(310.30243902439025, 59.30181818181819, 'X[4] <= -0.171\nentropy = 0.977\nsamples = 17\nvalue = [10, 7]'),
     Text(293.9707317073171, 19.76727272727277, 'entropy = 0.863\nsamples = 7\nvalue = [2, 5]'),
     Text(326.6341463414634, 19.76727272727277, 'entropy = 0.722\nsamples = 10\nvalue = [8, 2]'),
     Text(310.30243902439025, 256.9745454545455, 'X[2] <= 1.19\nentropy = 0.863\nsamples = 35\nvalue = [10, 25]'),
     Text(293.9707317073171, 217.44, 'X[5] <= -0.29\nentropy = 0.722\nsamples = 30\nvalue = [6, 24]'),
     Text(277.6390243902439, 177.90545454545457, 'entropy = 0.985\nsamples = 7\nvalue = [4, 3]'),
     Text(310.30243902439025, 177.90545454545457, 'X[7] <= 1.163\nentropy = 0.426\nsamples = 23\nvalue = [2, 21]'),
     Text(293.9707317073171, 138.3709090909091, 'entropy = 0.0\nsamples = 16\nvalue = [0, 16]'),
     Text(326.6341463414634, 138.3709090909091, 'entropy = 0.863\nsamples = 7\nvalue = [2, 5]'),
     Text(326.6341463414634, 217.44, 'entropy = 0.722\nsamples = 5\nvalue = [4, 1]'),
     Text(511.38658536585365, 375.5781818181818, 'X[4] <= 1.092\nentropy = 0.963\nsamples = 196\nvalue = [76, 120]'),
     Text(418.5, 336.0436363636364, 'X[1] <= -0.682\nentropy = 0.997\nsamples = 113\nvalue = [60, 53]'),
     Text(367.4634146341464, 296.5090909090909, 'X[2] <= 0.041\nentropy = 0.544\nsamples = 24\nvalue = [21, 3]'),
     Text(351.13170731707316, 256.9745454545455, 'entropy = 0.845\nsamples = 11\nvalue = [8, 3]'),
     Text(383.7951219512195, 256.9745454545455, 'entropy = 0.0\nsamples = 13\nvalue = [13, 0]'),
     Text(469.5365853658537, 296.5090909090909, 'X[6] <= 1.11\nentropy = 0.989\nsamples = 89\nvalue = [39, 50]'),
     Text(416.45853658536583, 256.9745454545455, 'X[2] <= 0.041\nentropy = 0.999\nsamples = 72\nvalue = [37, 35]'),
     Text(359.29756097560977, 217.44, 'X[6] <= -0.663\nentropy = 0.929\nsamples = 29\nvalue = [10, 19]'),
     Text(342.9658536585366, 177.90545454545457, 'entropy = 0.391\nsamples = 13\nvalue = [1, 12]'),
     Text(375.6292682926829, 177.90545454545457, 'X[2] <= -0.451\nentropy = 0.989\nsamples = 16\nvalue = [9, 7]'),
     Text(359.29756097560977, 138.3709090909091, 'entropy = 0.811\nsamples = 8\nvalue = [6, 2]'),
     Text(391.96097560975613, 138.3709090909091, 'entropy = 0.954\nsamples = 8\nvalue = [3, 5]'),
     Text(473.61951219512196, 217.44, 'X[0] <= -0.168\nentropy = 0.952\nsamples = 43\nvalue = [27, 16]'),
     Text(440.9560975609756, 177.90545454545457, 'X[3] <= -0.283\nentropy = 0.592\nsamples = 21\nvalue = [18, 3]'),
     Text(424.62439024390244, 138.3709090909091, 'entropy = 0.881\nsamples = 10\nvalue = [7, 3]'),
     Text(457.2878048780488, 138.3709090909091, 'entropy = 0.0\nsamples = 11\nvalue = [11, 0]'),
     Text(506.2829268292683, 177.90545454545457, 'X[0] <= 1.126\nentropy = 0.976\nsamples = 22\nvalue = [9, 13]'),
     Text(489.9512195121951, 138.3709090909091, 'X[4] <= 0.361\nentropy = 0.75\nsamples = 14\nvalue = [3, 11]'),
     Text(473.61951219512196, 98.83636363636367, 'entropy = 0.0\nsamples = 5\nvalue = [0, 5]'),
     Text(506.2829268292683, 98.83636363636367, 'entropy = 0.918\nsamples = 9\nvalue = [3, 6]'),
     Text(522.6146341463415, 138.3709090909091, 'entropy = 0.811\nsamples = 8\nvalue = [6, 2]'),
     Text(522.6146341463415, 256.9745454545455, 'X[4] <= 0.876\nentropy = 0.523\nsamples = 17\nvalue = [2, 15]'),
     Text(506.2829268292683, 217.44, 'entropy = 0.0\nsamples = 12\nvalue = [0, 12]'),
     Text(538.9463414634147, 217.44, 'entropy = 0.971\nsamples = 5\nvalue = [2, 3]'),
     Text(604.2731707317073, 336.0436363636364, 'X[3] <= -0.764\nentropy = 0.707\nsamples = 83\nvalue = [16, 67]'),
     Text(587.9414634146342, 296.5090909090909, 'entropy = 0.996\nsamples = 13\nvalue = [6, 7]'),
     Text(620.6048780487805, 296.5090909090909, 'X[6] <= 0.814\nentropy = 0.592\nsamples = 70\nvalue = [10, 60]'),
     Text(604.2731707317073, 256.9745454545455, 'X[1] <= 0.058\nentropy = 0.747\nsamples = 47\nvalue = [10, 37]'),
     Text(571.609756097561, 217.44, 'X[3] <= -0.293\nentropy = 0.985\nsamples = 14\nvalue = [6, 8]'),
     Text(555.2780487804878, 177.90545454545457, 'entropy = 0.918\nsamples = 6\nvalue = [4, 2]'),
     Text(587.9414634146342, 177.90545454545457, 'entropy = 0.811\nsamples = 8\nvalue = [2, 6]'),
     Text(636.9365853658537, 217.44, 'X[3] <= 2.025\nentropy = 0.533\nsamples = 33\nvalue = [4, 29]'),
     Text(620.6048780487805, 177.90545454545457, 'X[1] <= 1.837\nentropy = 0.235\nsamples = 26\nvalue = [1, 25]'),
     Text(604.2731707317073, 138.3709090909091, 'entropy = 0.0\nsamples = 21\nvalue = [0, 21]'),
     Text(636.9365853658537, 138.3709090909091, 'entropy = 0.722\nsamples = 5\nvalue = [1, 4]'),
     Text(653.2682926829268, 177.90545454545457, 'entropy = 0.985\nsamples = 7\nvalue = [3, 4]'),
     Text(636.9365853658537, 256.9745454545455, 'entropy = 0.0\nsamples = 23\nvalue = [0, 23]')]




![png](../fig/50-Classification_96_1.png)


> ## Challenge
>
> Do you think this model is fit well?
>
> 
> {: .source}
>
> > ## Solution
> > 
> > No, the tree has overfit.
> > 
> > {: .output}
> {: .solution}
{: .challenge}

## Random Forest Classifier
A random decision tree is where you make a decision tree but only train it on either:

- (a) a random sample of the available data or 
- (b) a random sample of the available features or 
- (c) both.

A random forest is a whole bunch of these averaged together. 

Turns out these do pretty good and are used all over the place. But because they're the average of so many different models it's hard to get an understanding about it. It's basically a black box that predicts well.


```python
from sklearn.ensemble import RandomForestClassifier
cf_rfc = RandomForestClassifier(random_state=42)
# 
rf_dict = {
    # The number of trees in the forest
    'n_estimators': np.arange(10,150,25), 
    # quality of a split
    'criterion': ['entropy'],
    # The minimum number of samples required to split an internal node
    'min_samples_split': np.arange(start = 2, stop = 20, step = 5),
    # The minimum number of samples required to be at a leaf node
    'min_samples_leaf': np.arange(start = 1, stop = 3, step = 1),
    # the number of features to consider when looking for the best split
    # max_features=sqrt(n_features)/max_features=log2(n_features)
    'max_features': ['sqrt'],
    'class_weight': ['balanced_subsample', None]
}

# use GridSeachCV to which form of regularisation is optimal
cf_rf_gscv = GridSearchCV(cf_rfc,
                           rf_dict,
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')

# fit model on training data
fit_rf = cf_rf_gscv.fit(features_train_sc, outcome_train)

# what were the best parameters?
fit_rf.best_estimator_
```

    /Users/darya/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)





    RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',
                           criterion='entropy', max_depth=None, max_features='sqrt',
                           max_leaf_nodes=None, min_impurity_decrease=0.0,
                           min_impurity_split=None, min_samples_leaf=2,
                           min_samples_split=17, min_weight_fraction_leaf=0.0,
                           n_estimators=85, n_jobs=None, oob_score=False,
                           random_state=42, verbose=0, warm_start=False)



Use trained classifier to predict outcome for test-set


```python
rf_train_pred_class = cf_rf_gscv.predict(features_train_sc)
rf_test_pred_class = cf_rf_gscv.predict(features_test_sc)

rf_pred_prob_train = model_probabilities(model = cf_rf_gscv, dataset=features_train_sc)
rf_pred_prob_test = model_probabilities(model = cf_rf_gscv, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = rf_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = rf_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = rf_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = rf_pred_prob_test, col = 'r')
```

    Training set
    Predicted    0    1  All
    True                    
    0          144   95  239
    1           85   44  129
    All        229  139  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          26  16   42
    1          20  12   32
    All        46  28   74



![png](../fig/50-Classification_101_1.png)


## Support Vector Machine
A support vector machine tries to find the data points right on the boundary between the two classes (the "support vectors") and then uses them to define a maximum margin boundary.

![A linear Support Vector Machine for Cats](../fig/50-CatSVM.jpg)

### Let's Classify!
Train SVM


```python
from sklearn import svm
cf_svm = svm.SVC(random_state=42, probability=True, 
                 # Kernel coefficient for ‚Äòrbf‚Äô, ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô.
                 # 1 / (n_features * X.var()) 
                gamma = 'scale')

cf_svm_gscv_lin = GridSearchCV(cf_svm,
                           {'kernel': ['linear']},
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')

fit_svm_lin = cf_svm_gscv_lin.fit(features_train_sc, outcome_train)

# use GridSeachCV to which form of regularisation is optimal
cf_svm_gscv_rad = GridSearchCV(cf_svm,
                           {'kernel': ['rbf']},
                           # use 5xfold cross-validation
                           cv=5,
                           # use AUC as error metric
                           scoring = 'f1')

fit_svm_rbf = cf_svm_gscv_rad.fit(features_train_sc, outcome_train)
```

Use trained classifier to predict outcome for test-set


```python
fit_svm_lin.best_estimator_
```




    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',
        max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,
        verbose=False)




```python
fit_svm_rbf.best_estimator_
```




    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
        decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
        max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,
        verbose=False)




```python
svm_l_train_pred_class = cf_svm_gscv_lin.predict(features_train_sc)
svm_l_test_pred_class = cf_svm_gscv_lin.predict(features_test_sc)

svm_l_pred_prob_train = model_probabilities(model = cf_svm_gscv_lin, dataset=features_train_sc)
svm_l_pred_prob_test = model_probabilities(model = cf_svm_gscv_lin, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = svm_l_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = svm_l_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = svm_l_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = svm_l_pred_prob_test, col = 'r')
```

    Training set
    Predicted    0   1  All
    True                   
    0          177  62  239
    1           94  35  129
    All        271  97  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          32  10   42
    1          22  10   32
    All        54  20   74



![png](../fig/50-Classification_107_1.png)



```python
svm_r_train_pred_class = cf_svm_gscv_rad.predict(features_train_sc)
svm_r_test_pred_class = cf_svm_gscv_rad.predict(features_test_sc)

svm_r_pred_prob_train = model_probabilities(model = cf_svm_gscv_rad, dataset=features_train_sc)
svm_r_pred_prob_test = model_probabilities(model = cf_svm_gscv_rad, dataset=features_test_sc)


# print matrix for predictions on training and testing
print("Training set")
print(confmatrix(truth = outcome_train, prediction = svm_r_train_pred_class))
print("\nTesting set")
print(confmatrix(truth = outcome_test, prediction = svm_r_test_pred_class))



# plot the roc curves
plot_ROC(truth = outcome_train, prediction1 = svm_r_pred_prob_train, col = 'b')
plot_ROC(truth = outcome_test, prediction1 = svm_r_pred_prob_test, col = 'r')
```

    Training set
    Predicted    0    1  All
    True                    
    0          168   71  239
    1           96   33  129
    All        264  104  368
    
    Testing set
    Predicted   0   1  All
    True                  
    0          30  12   42
    1          20  12   32
    All        50  24   74



![png](../fig/50-Classification_108_1.png)


# Compare all the classifiers


```python
evaluations = ['Misclassification rate', 'Sensitivity', 'Specificity', 'AUC']
pretrained_models = {'k Nearest Neighbours':cf_knn_gscv, 
                     'Naive Bayes':cf_gnb_gscv, 
                     'Regularised Logistic Classifier':cf_rlc_gscv, 
                     'Decision Tree':cf_dtc_gscv, 
                     'Random Forest':cf_rf_gscv, 
                     'Linear SVM':cf_svm_gscv_lin,
                     'Radial SVM':cf_svm_gscv_rad}
comparison_stats = pd.DataFrame(index = pretrained_models.keys(), columns=evaluations)
for method, model in pretrained_models.items():
    outcome_pred_class = model.predict(features_test_sc)
    if method in []: # Support Vector Machine'
        AUC = float('nan')
    else:
        outcome_pred_prob = model.predict_proba(features_test_sc)
        outcome_pred_prob1 = [p[1] for p in outcome_pred_prob]
        AUC = roc_auc_score(outcome_true, outcome_pred_prob1)
    conf_mat = confusion_matrix(outcome_test, outcome_pred_class)
    # in this case 0-0 is negatives
    # 1-1 is diabetes
    TP = conf_mat[1,1]
    FP = conf_mat[0,1]
    TN = conf_mat[0,0]
    FN = conf_mat[1,0]
    
    comparison_stats.loc[method,'Misclassification rate']  = 1. - accuracy_score(outcome_test, outcome_pred_class)
    # sensitivity == recall
    comparison_stats.loc[method,'Sensitivity'] = TP/(TP + FN)
    comparison_stats.loc[method,'Specificity'] = TN/(TN + FP)
    comparison_stats.loc[method,'Precision'] = TP/(TP + FP)
    comparison_stats.loc[method,'Accuracy'] = (TP + TN)/(TP + FP + TN + FN)
    comparison_stats.loc[method,'FDR'] = FP/(FP + TP)
    comparison_stats.loc[method,'F1'] = 2 * TP/(2 * TP + FP + FN)
    comparison_stats.loc[method,'AUC'] = AUC
```


```python
comparison_stats.round(decimals=3).sort_values(by = 'AUC', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.225108</td>
      <td>0.7625</td>
      <td>0.781457</td>
      <td>0.86101</td>
      <td>0.649</td>
      <td>0.775</td>
      <td>0.351</td>
      <td>0.701</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.229437</td>
      <td>0.575</td>
      <td>0.874172</td>
      <td>0.860844</td>
      <td>0.708</td>
      <td>0.771</td>
      <td>0.292</td>
      <td>0.634</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.229437</td>
      <td>0.75</td>
      <td>0.781457</td>
      <td>0.851821</td>
      <td>0.645</td>
      <td>0.771</td>
      <td>0.355</td>
      <td>0.694</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.251082</td>
      <td>0.6125</td>
      <td>0.821192</td>
      <td>0.835679</td>
      <td>0.645</td>
      <td>0.749</td>
      <td>0.355</td>
      <td>0.628</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.6</td>
      <td>0.821192</td>
      <td>0.822103</td>
      <td>0.640</td>
      <td>0.745</td>
      <td>0.360</td>
      <td>0.619</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.264069</td>
      <td>0.5875</td>
      <td>0.81457</td>
      <td>0.797227</td>
      <td>0.627</td>
      <td>0.736</td>
      <td>0.373</td>
      <td>0.606</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.268398</td>
      <td>0.6125</td>
      <td>0.794702</td>
      <td>0.75</td>
      <td>0.612</td>
      <td>0.732</td>
      <td>0.388</td>
      <td>0.612</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Misclassification rate', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Decision Tree</th>
      <td>0.268398</td>
      <td>0.6125</td>
      <td>0.794702</td>
      <td>0.75</td>
      <td>0.612</td>
      <td>0.732</td>
      <td>0.388</td>
      <td>0.612</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.264069</td>
      <td>0.5875</td>
      <td>0.81457</td>
      <td>0.797227</td>
      <td>0.627</td>
      <td>0.736</td>
      <td>0.373</td>
      <td>0.606</td>
    </tr>
    <tr>
      <th>Radial SVM</th>
      <td>0.255411</td>
      <td>0.6</td>
      <td>0.821192</td>
      <td>0.822103</td>
      <td>0.640</td>
      <td>0.745</td>
      <td>0.360</td>
      <td>0.619</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.251082</td>
      <td>0.6125</td>
      <td>0.821192</td>
      <td>0.835679</td>
      <td>0.645</td>
      <td>0.749</td>
      <td>0.355</td>
      <td>0.628</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.229437</td>
      <td>0.75</td>
      <td>0.781457</td>
      <td>0.851821</td>
      <td>0.645</td>
      <td>0.771</td>
      <td>0.355</td>
      <td>0.694</td>
    </tr>
    <tr>
      <th>Linear SVM</th>
      <td>0.229437</td>
      <td>0.575</td>
      <td>0.874172</td>
      <td>0.860844</td>
      <td>0.708</td>
      <td>0.771</td>
      <td>0.292</td>
      <td>0.634</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.225108</td>
      <td>0.7625</td>
      <td>0.781457</td>
      <td>0.86101</td>
      <td>0.649</td>
      <td>0.775</td>
      <td>0.351</td>
      <td>0.701</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Sensitivity', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.268398</td>
      <td>0.639535</td>
      <td>0.786207</td>
      <td>0.825341</td>
      <td>0.640</td>
      <td>0.732</td>
      <td>0.360</td>
      <td>0.640</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.238095</td>
      <td>0.627907</td>
      <td>0.841379</td>
      <td>0.843865</td>
      <td>0.701</td>
      <td>0.762</td>
      <td>0.299</td>
      <td>0.663</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.30303</td>
      <td>0.534884</td>
      <td>0.793103</td>
      <td>0.717081</td>
      <td>0.605</td>
      <td>0.697</td>
      <td>0.395</td>
      <td>0.568</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.281385</td>
      <td>0.523256</td>
      <td>0.834483</td>
      <td>0.799358</td>
      <td>0.652</td>
      <td>0.719</td>
      <td>0.348</td>
      <td>0.581</td>
    </tr>
    <tr>
      <th>Support Vector Machine</th>
      <td>0.25974</td>
      <td>0.476744</td>
      <td>0.896552</td>
      <td>0.827987</td>
      <td>0.732</td>
      <td>0.740</td>
      <td>0.268</td>
      <td>0.577</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.25974</td>
      <td>0.453488</td>
      <td>0.910345</td>
      <td>0.830152</td>
      <td>0.750</td>
      <td>0.740</td>
      <td>0.250</td>
      <td>0.565</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'Specificity', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.25974</td>
      <td>0.453488</td>
      <td>0.910345</td>
      <td>0.830152</td>
      <td>0.750</td>
      <td>0.740</td>
      <td>0.250</td>
      <td>0.565</td>
    </tr>
    <tr>
      <th>Support Vector Machine</th>
      <td>0.25974</td>
      <td>0.476744</td>
      <td>0.896552</td>
      <td>0.827987</td>
      <td>0.732</td>
      <td>0.740</td>
      <td>0.268</td>
      <td>0.577</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.238095</td>
      <td>0.627907</td>
      <td>0.841379</td>
      <td>0.843865</td>
      <td>0.701</td>
      <td>0.762</td>
      <td>0.299</td>
      <td>0.663</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.281385</td>
      <td>0.523256</td>
      <td>0.834483</td>
      <td>0.799358</td>
      <td>0.652</td>
      <td>0.719</td>
      <td>0.348</td>
      <td>0.581</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.30303</td>
      <td>0.534884</td>
      <td>0.793103</td>
      <td>0.717081</td>
      <td>0.605</td>
      <td>0.697</td>
      <td>0.395</td>
      <td>0.568</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.268398</td>
      <td>0.639535</td>
      <td>0.786207</td>
      <td>0.825341</td>
      <td>0.640</td>
      <td>0.732</td>
      <td>0.360</td>
      <td>0.640</td>
    </tr>
  </tbody>
</table>
</div>




```python
comparison_stats.round(decimals=3).sort_values(by = 'F1', ascending=False)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Misclassification rate</th>
      <th>Sensitivity</th>
      <th>Specificity</th>
      <th>AUC</th>
      <th>Precision</th>
      <th>Accuracy</th>
      <th>FDR</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Random Forest</th>
      <td>0.238095</td>
      <td>0.627907</td>
      <td>0.841379</td>
      <td>0.843865</td>
      <td>0.701</td>
      <td>0.762</td>
      <td>0.299</td>
      <td>0.663</td>
    </tr>
    <tr>
      <th>Regularised Logistic Classifier</th>
      <td>0.268398</td>
      <td>0.639535</td>
      <td>0.786207</td>
      <td>0.825341</td>
      <td>0.640</td>
      <td>0.732</td>
      <td>0.360</td>
      <td>0.640</td>
    </tr>
    <tr>
      <th>Naive Bayes</th>
      <td>0.281385</td>
      <td>0.523256</td>
      <td>0.834483</td>
      <td>0.799358</td>
      <td>0.652</td>
      <td>0.719</td>
      <td>0.348</td>
      <td>0.581</td>
    </tr>
    <tr>
      <th>Support Vector Machine</th>
      <td>0.25974</td>
      <td>0.476744</td>
      <td>0.896552</td>
      <td>0.827987</td>
      <td>0.732</td>
      <td>0.740</td>
      <td>0.268</td>
      <td>0.577</td>
    </tr>
    <tr>
      <th>Decision Tree</th>
      <td>0.30303</td>
      <td>0.534884</td>
      <td>0.793103</td>
      <td>0.717081</td>
      <td>0.605</td>
      <td>0.697</td>
      <td>0.395</td>
      <td>0.568</td>
    </tr>
    <tr>
      <th>k Nearest Neighbours</th>
      <td>0.25974</td>
      <td>0.453488</td>
      <td>0.910345</td>
      <td>0.830152</td>
      <td>0.750</td>
      <td>0.740</td>
      <td>0.250</td>
      <td>0.565</td>
    </tr>
  </tbody>
</table>
</div>



What do you think?
